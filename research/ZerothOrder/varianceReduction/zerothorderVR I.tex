\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage{mathptmx}  
\usepackage[scaled=.92]{helvet}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage[numbers]{natbib}  
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}  %%check
\usepackage[dvipsnames]{xcolor}
%%%Algorithms
\usepackage[noend]{algpseudocode}
\usepackage{algorithm,amsmath}
\usepackage{mathtools}
\usepackage{newtxtext,newtxmath}

\newcommand*{\R}{\mathbb{R}}
\newcommand*{\G}{\mathcal{G}}
\newcommand*{\Po}{\text{Prox}}
\newcommand*{\Am}{\text{argmin}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\VRG}{\,\tilde{\nabla}_k^s}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\Iprod}[2]{\left\langle #1,#2\right\rangle}
\newcommand\myeq[2]{\mathrel{\stackrel{\makebox[0pt]{\mbox{#1}}}{#2}}}

\renewcommand{\algorithmicrequire}{
\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\Initialize}{\textbf{Initialize:}{\,}}
\newcommand{\Input}{\textbf{Input:}{\,}}
\newcommand{\Output}{\textbf{Output:}{\,}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{question}[theorem]{Question}
\newtheorem{corollary}[theorem]{Corollary} 
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{statement}[theorem]{Statement}
\newtheorem{notation}[theorem]{Notation} 
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}


\title{Proximal Gradient Algorithm for Nonconvex Problems}
\date{March 2018}

\begin{document}

\maketitle

\section{Introduction}


\section{Accelerated Proximal Gradient Method}

\begin{algorithm}\label{APGnonconvex-Algo}
\caption{Nonconvex ProxSVRG+}
\begin{algorithmic}[1]
\State\Input initial point $x_0$, batch size $B$, minibatch size $b$, epoch length $m$, step size $\eta$
\State\Initialize $\tilde{x}^0 = x_0$
\For{ $s=1,2,\ldots, S$ }
\State $x_0^s = \widetilde{x}^{s-1}$
\State $\hat{g}^s = \frac{1}{B} \sum_{j\in I_B} \nabla f_j (\widetilde{x}^{s-1})$
\For{ $t=1,2,\ldots, m$ }
\State ${\hat{v}}_{t-1}^s = \frac{1}{b} \sum_{i\in I_b}\left(\hat{\nabla} f_{i}(x_{t-1}^s)-\hat{\nabla} f_{i}(\tilde{x}^{s-1})\right)+\hat{g}^s$
\State $x_{t}^s= \Po_{\eta h}(x_{t-1}^s - \eta \hat{v}_{t-1}^s)$
\EndFor
\State $\widetilde{x}^{s} = x_m^s$
 \EndFor
 \State\Output $\hat{x}$ chosen uniformly from $\{x_{t-1}^s\}_{t\in [m], s\in [S]}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}\label{APGnonconvex-Algo}
\caption{ZO-PROXSVRG for convex Optimization}\begin{algorithmic}[1]
\State\Input mini-batch size $b$, $S$, $m$ and step size $\eta > 0$, parameter $\theta$
\State\Initialize $\tilde{x}^0 = x_0^1 = x_0 \in \R^d$
\For{ $s=1,2,\ldots, S$ }
\State ${\color{red}\mu_s = }\hat{\nabla}f(\tilde{x}^s) = \frac{1}{n}\sum_{i=1}^n \hat{\nabla}f_i(\tilde{x}^s), \theta=\frac{2}{s+4}, \eta = \frac{1}{4L\theta}$
\For{ $j=1,\ldots, m$ }
\State Randomly pick up an $i_j$ from
$\{1,\ldots,n\}$
\State $y_{j-1} = \theta x_{j-1}^s+(1-\theta)\tilde{x}_{s-1}$
\State $\hat{v}_j^s = \nabla f_{i_j}(y_{j-1})-\nabla f_{i_j}(\tilde{x}_{s-1})+\mu_s$
\State $x_{j}^s=\text{argmin}_{x}\left\{\frac{1}{2\eta}\norm{x-x_{j-1}^s}^2+\Iprod{\hat{v}_j^s}{x}+g(x)\right\}$
\EndFor
\State $\tilde{x}^s=\frac{\theta}{m}\sum_{j=1}^{m}x_j^s + (1-\theta)\tilde{x}^{s-1}$ 
\State $x_0^{s+1} = x_m^s$
 \EndFor
 \State\Output ${\tilde{x}}_{S}$
\end{algorithmic}
\end{algorithm}
\begin{theorem}
If function $f$ is convex, then by choosing $m=\mathcal{O}(n)$, ZO-PROXSVRG achieves the following oracle complexity in expectation
\[
\mathcal{O}\left(n\sqrt{\frac{F(x^0)-F(x^*)}{\epsilon}}+\sqrt{\frac{nL\norm{x^0-x^*}^2}{\epsilon}}\right).
\]
This result implies that ZO-PROXSVRG attains the optimal convergence rate $\mathcal{O}(1/T^2)$, where $T=S(m+n)$ is the total number of stochastic iterations.
\end{theorem}
\begin{proof}
We first impose the following constraint on $\eta$ and $\theta$
\begin{equation}\label{fast-eq12}
L\theta+\frac{L\theta}{1-\theta} \leq \frac{1}{\eta}, \qquad \text{or equivalently}\,\, \eta \leq \frac{1-\theta}{L\theta(2-\theta)}.
\end{equation}
We start with the convexity of $f$ at $y_{j-1}$. By definition for any vector $u\in\R^d$, we have 
\begin{equation}\label{eq14-fast}
\begin{split}
f(y_{j-1}) - f(u) &\leq \Iprod{\nabla f(y_{j-1})}{y_{j-1}-i}\\
\frac{1-\theta}{\theta}\Iprod{\nabla f(y_{j-1})}{\tilde{x}^{s-1}-y_{j-1}}+\Iprod{\nabla f(y_{j-1})}{x_{j-1}-u},
\end{split}
\end{equation}
where (a) follows from the fact that $y_{j-1} = \theta x_{j-1}+(1-\theta)\tilde{x}^{s-1}$. Then we further expand $\Iprod{\nabla f(y_{j-1})}{x_{j-1}-u}$ as 
\begin{equation}\label{eq15-fast}
\Iprod{\nabla f(y_{j-1})}{x_{j-1}-u} = \Iprod{\nabla f(y_{j-1}) - \hat{v}_j^s}{x_{j-1}-u}+\Iprod{\hat{v}_j^s}{x_{j-1}-x_j}+\Iprod{\hat{v}_j^s}{x_j-u}.
\end{equation}
Using L-smooth of $f$ at $(y_j,y_{j-1})$, we get 
\begin{equation}
\begin{split}
f(y_j) - f(y_{j-1}) &\leq \Iprod{\nabla f(y_{j-1})}{y_j-y_{j-1}}+\frac{L}{2}\norm{y_j-y_{j-1}}^2\\
&= \left[ \Iprod{\nabla f(y_{j-1})-\hat{v}_j^s}{x_j-x_{j-1}} + \Iprod{\hat{v}_j^s}{x_j-x_{j-1}}+\frac{L\theta^2}{2}\norm{x_j-x_{j-1}}^2\right] 
\end{split}
\end{equation}
Equivalently we obtain
\begin{equation}
\Iprod{\hat{v}_j^s}{x_j-x_{j-1}} \leq \frac{1}{\theta}(f(y_{j-1}) - f(y_{j})) + \Iprod{\nabla f(y_{j-1})-\hat{v}_j^s}{x_j-x_{j-1}} + \frac{L\theta}{2}\norm{x_j-x_{j-1}}^2.
\end{equation}
Using the constraint \eqref{fast-eq12} we have
\begin{equation}\label{eq16-fast}
\Iprod{\hat{v}_j^s}{x_{j-1}-x_{j}} \leq \frac{1}{\theta}(f(y_{j-1}) - f(y_{j})) + \Iprod{\nabla f(y_{j-1})-\hat{v}_j^s}{x_j-x_{j-1}} + \frac{1}{2\eta}\norm{x_j-x_{j-1}}^2 - \frac{L\theta}{2(1-\theta)}\norm{x_j-x_{j-1}}^2.
\end{equation}
Then we can combine \eqref{eq14-fast}, \eqref{eq15-fast}, \eqref{eq16-fast}, as well as {\color{red} Lemma 3}, which leads to
\begin{equation}
\begin{split}
f(y_{j-1}) - f(u) &\leq \frac{1-\theta}{\theta} \Iprod{\nabla f(y_{j-1})}{\tilde{x}^{s-1}-y_{j-1}} + \Iprod{\nabla f(y_{j-1})-\hat{v}_j^s}{x_{j}-u} + \frac{1}{\theta}(f(y_{j-1}) - f(y_{j}))\\
&  - \frac{L\theta}{2(1-\theta)}\norm{x_j-x_{j-1}}^2+ \frac{1}{2\eta}\norm{x_{j-1}-u}^2 - \frac{1}{2\eta}\norm{x_j-u}^2+g(u) - g(x_j)
\end{split}
\end{equation}
After taking expectation with respect to the sample $i_j$, we get
\begin{equation}
\begin{split}
f(y_{j-1}) - f(u) &\leq \frac{1-\theta}{\theta} \Iprod{\nabla f(y_{j-1})}{\tilde{x}^{s-1}-y_{j-1}} + \frac{1}{2\beta}\E_{i_j}[\norm{\nabla f(y_{j-1})-\hat{v}_j^s}^2]+ \frac{\beta}{2}\E_{i_j}[\norm{x_{j}-u}^2]\\
& + \frac{1}{\theta}(f(y_{j-1}) - f(y_{j}))\\
&  - \frac{L\theta}{2(1-\theta)}\norm{x_j-x_{j-1}}^2+ \frac{1}{2\eta}\norm{x_{j-1}-u}^2 - \frac{1}{2\eta}\norm{x_j-u}^2+g(u) - g(x_j)
\end{split}
\end{equation}
\end{proof}
% \begin{algorithm}\label{VR-Algo}
% \caption{AFSVRG for smooth component functions}\label{alg:euclid}
% \begin{algorithmic}[1]
% \State\Input The number of epochs $S$ and the step size $\eta$.
% \State\Initialize $\widetilde{x}^0$, $m_1$, $\theta_1$, and $\rho > 1$.
% \For{ $s=1,2,\ldots, S$ }
% \State $\widetilde{\mu} = \frac{1}{n}\sum_{i=1}^n\nabla f_i(\widetilde{x}^{s-1})$, $x_0^s = y_0^s = \widetilde{x}^{s-1}$;
%  \For{$k=0,1,2,\ldots,m_s-1$}
%  \State Pick $i_k^s$ uniformly at random from $\{1,\ldots,n\}$;
%  \State $\widetilde{\nabla} f_{i_k^s}(x_{k}^s) = \nabla f_{i_k^s}(x_{k}^s) - \nabla f_{i_k^s}(\widetilde{x}^{s-1}) + \widetilde{\mu}$;
%  \State $y_{k+1}^s = y_{k}^s - \eta\left[\widetilde{\nabla} f_{i_k^s}(x_{k}^s)+ \nabla g(x_{k}^s)\right]$;
%  \State $x_{k+1}^s = \widetilde{x}^{s-1}+\theta_s(y_{k+1}^s-\widetilde{x}^{s-1})$;
%  \EndFor
% $\widetilde{x}^s = \frac{1}{m_s}\sum_{k=0}^{m_s-1} x_{k}^s$, $m_{s+1} = \lceil{\rho^s\cdot m_1}\rceil$
%  \EndFor
%  \State\Output $\widetilde{x}^S$
% \end{algorithmic}
% \end{algorithm}



\section{Convergence Analysis}
\begin{lemma}\label{CooSGE}
Assume that the function $f(x)$ is $L$-smooth. Let $\hat{\nabla} f(x)$ denote the estimated gradient defined by {\bf CooSGE}. Define $f_{\mu_j} = \E_{u\sim U[\mu_j, \mu_j]} f(x+ue_j)$, where $U[-\mu_j,\mu_j]$ denotes the uniform distribution at the interval $[\mu_j, \mu_j]$. Then we have 
1) $f_{\mu_j}$ is $L$-smooth, and 
\begin{equation}
\hat{\nabla} f(x) = \sum_{j=1}^d \frac{\partial f_{\mu_j}}{\partial x_j}e_j
\end{equation} 
where $\partial f/\partial x_j$ denotes the partial derivative with respect to $j$th coordinate.

2) For $j\in [d]$, 
\begin{align}
\abs{f_{\mu_j}(x) - f(x)} &\leq \frac{L\mu_j^2}{2}\\
\abs{\frac{\partial f_{\mu_j}(x)}{\partial x_j}} \leq \frac{L\mu_j^2}{2}
\end{align}
 
 3) If $\mu = \mu_j$ for $j\in [d]$, then 
 \begin{equation}
 \norm{\hat{\nabla} f(x) - {\nabla} f(x)} ^2 \leq \frac{L^2 d^2 \mu^2}{4}
\end{equation}  
\end{lemma}
\begin{lemma}\label{GauSGE}
Assume that the function $f(x)$ is $L$-smooth. Let $\hat{\nabla} f(x)$ denote the estimated gradient defined by {\bf GauSGE}. Define $f_{\mu} = \E_{u\sim \mathcal{N(0, I)}} [f(x+\mu u)]$. Then we have 
1) For any $x\in \R^d$, $\nabla f_{\mu}(x) = \E_u [\hat{\nabla} f(x)]$.

2) For any $x\in \R^d$, 
 \begin{align}
&\abs{f_{\mu}(x) - f(x)} \leq \frac{L d \mu^2}{2}\\
&\abs{\nabla f_{\mu}(x) - \nabla f(x)} \leq \frac{L\mu(d+3)^{\frac{3}{2}}}{2}\\
&\E_u \norm{\hat{\nabla} f(x)}^2 \leq 2(d+4)\norm{{\nabla} f(x)}^2+ \frac{\mu^2L^2 (d+6)^3}{2}
\end{align}
3) For any $x\in \R^d$, 
\begin{equation}
\E_u  \norm{\hat{\nabla} f(x) - {\nabla} f(x)}^2 \leq 2(2d+9)\norm{\nabla f(x)}^2 + \mu^2 L^2 (d+6)^3.
\end{equation}
\end{lemma}
\begin{lemma}\label{lemma1}
Let $x^+ = \Po_{\eta h}(x-\eta v)$, then the following inequality holds:
\begin{equation}\label{eq10}
\Phi(x^+) \leq \Phi(z) + \Iprod{\nabla f(x)-v}{x^+-z}-\frac{1}{\eta} \Iprod{x^+-x}{x^+-z}+\frac{L}{2}\norm{x^+-x}^2+\frac{L}{2}\norm{z-x}^2, \forall z\in \R^d. 
\end{equation}
\end{lemma}
\begin{proof}
First, we recall the proximal operator 
\begin{equation}\label{eq11}
\Po_{\eta h}(x-\eta v) := \text{arg}\,\,\min_{y\in\R^d}\left(h(y)+\frac{1}{2\eta}\norm{y-x}^2+\Iprod{v}{y}\right)
\end{equation}
For the nonsmooth function $h(x)$, we have 
\begin{equation}\label{eq12}
\begin{split}
h(x^+) &\leq h(z) + \Iprod{p}{x^+-z}\\
&= h(z) - \Iprod{v+\frac{1}{\eta}(x^+-x)}{x^+-z}
\end{split}
\end{equation}
where $p\in \partial h(x^+)$ such that $p+\frac{1}{\eta}(x^+-x)+v = 0$ according to the optimality condition of \eqref{eq11}, and \eqref{eq12} due to the convexity of $h$.
\begin{equation}\label{eq14}
f(x^+) \leq f(x)+\Iprod{\nabla f(x)}{x^+-x}+\frac{L}{2}\norm{x^+-x}^2
\end{equation}
\begin{equation}\label{eq15}
-f(z) \leq -f(x)+\Iprod{-\nabla f(x)}{z-x}+\frac{L}{2}\norm{z-x}^2
\end{equation}
where \eqref{eq14} holds since $f(x)$ has $L$-Lipschitz continuous gradient, and \eqref{eq15} holds since $-f(x)$ has the same $L$-Lipschitz continuous gradient as $f(x)$. 

 This lemma is proved by adding \eqref{eq12}, \eqref{eq14}, \eqref{eq15}, and recalling $\Phi(x) = f(x)+h(x)$. 
\end{proof}
\begin{lemma}
\end{lemma}
\begin{proof}
{\color{blue}
 \begin{align}
  \E&\left[\eta\norm{\nabla f(x_{t-1}^s)-{\color{blue}\hat{v}_{t-1}^s}}^2\right]\notag\\
   =&\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s)-\hat{g}^s\right)}^2\right]\notag\\
   =&\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s)-\frac{1}{B}\sum_{j\in I_B}\hat{\nabla} f_j(\widetilde{x}^{s-1})\right)}^2\right]\notag\\
   =&\notag\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)+ \left(\frac{1}{B}\sum_{j\in I_B}\hat{\nabla} f_j(\widetilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\\
   =&\notag\eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)+ \frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\widetilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\\
   =&\notag2\eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})\right) - \left(\hat{\nabla} f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)+ \frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\widetilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\\
   &\,\, + 2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
    = &\notag 2 \eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})\right) - \left(\hat{\nabla} f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)}^2\right]\\
   &\,\, +2 \eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\widetilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]+ 2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\label{eq26}\\
   = &\notag\frac{2\eta}{b^2}\E\left[\sum_{i\in I_b}\norm{\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})\right) - \left({\hat{\nabla}} f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)}^2\right]\\
   &\,\, \label{eq27}+ 2\eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\widetilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]+2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
   \leq  &\label{eq28}\frac{2\eta}{b^2}\E\left[\sum_{i\in I_b}\norm{\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\widetilde{x}^{s-1})}^2\right]+ 2\eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\widetilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\\
   &\,\,+2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
   \leq  &\label{eq29}\frac{2\eta L^2 d}{b}\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
   \leq  &\label{eq29}\frac{2\eta L^2 d}{b}\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
 \end{align}
 }
 where the last inequality holds by Lemma \ref{CooSGE}. Using Lemma \ref{CooSGE}, we have 
 \begin{equation}
 \begin{split}
 \E \norm{\hat{\nabla} f_i(x_{t}^s)-\hat{\nabla} f_i(\tilde{x}^s)}^2 &= \E \norm{ \sum_{j=1}^d\frac{f_{i,\mu_j}(x_{t}^s)}{\partial x_j}e_j-\frac{f_{i,\mu_j}(\tilde{x}^s)}{\partial x_j}e_j}^2\\
 &\leq d \sum_{j=1}^d \E \norm{ \frac{f_{i,\mu_j}(x_{t}^s)}{\partial x_j}-\frac{f_{i,\mu_j}(\tilde{x}^s)}{\partial x_j}}^2\\
 &\leq L^2 d \sum_{j=1}^d \E \norm{x_{t,j}^s-\tilde{x}_{j}^s}^2 = L^2 d \norm{x_{t}^s-\tilde{x}^s}^2\\
 \end{split}
 \end{equation}
 
\end{proof}
\begin{theorem}\label{theo1}
Let step size $\eta=\frac{1}{6L}$ and $b$ denote the minibatch size. The $\hat{x}$ returned by Algorithm 1  is an $\epsilon$- accurate solution for problem \ref{}. We distinguish the following two cases:

1) We let batch size $B=n$. The number of SFO calls is at most 
\[
36 L (\Phi(x_0)-\Phi(x^*))\left(\frac{B}{\epsilon\sqrt{b}}+\frac{b}{\epsilon}\right) = O\left(\frac{n}{\epsilon \sqrt{b}}+\frac{b}{\epsilon}\right).
\]
2) Under Assumption 1, we let batch size $B = \{6\sigma^2/\epsilon, n\}$. The number of SFO calls is at most 
\[
36 L (\Phi(x_0)-\Phi(x^*))\left(\frac{B}{\epsilon\sqrt{b}}+\frac{b}{\epsilon}\right) = O\left((n\wedge \frac{1}{\epsilon})\frac{1}{\epsilon \sqrt{b}}+\frac{b}{\epsilon}\right).
\]
where $\wedge$ denotes the minimum.
\end{theorem}
In both cases, the number of $PO$ calls equals to the total number of iterations $T$, which is at most
\[
\frac{36 L}{\epsilon}(\Phi(x_0)-\Phi(x^*)) = O\left(\frac{1}{\epsilon}\right).
\] 
\begin{proof}
Now, we are ready to use Lemma \ref{lemma1} to prove Theorem \ref{theo1}. Let $x_t^s = \Po_{\eta h} (x_{t-1}^s - \eta v_{t-1}^s)$ and $\overline{x}_t^s = \Po_{\eta h} (x_{t-1}^s - \eta \nabla f(x_{t-1}^s))$. By letting $x^+ = x_t^s$, $x = x_{t-1}^s$, $v = v_{t-1}^s$ and $z = \overline{x}_t^s$ in \eqref{eq10}, we have
\begin{equation}\label{eq16}
\Phi(x^s_t) \leq \Phi(\overline{x}_t^s) + \Iprod{\nabla f(x_{t-1}^s)-v_{t-1}^s}{x_t^s-\overline{x}_t^s}-\frac{1}{\eta} \Iprod{x_t^s-x_{t-1}^s}{x_t^s-\overline{x}_t^s}+\frac{L}{2}\norm{x_t^s-x_{t-1}^s}^2+\frac{L}{2}\norm{\overline{x}_t^s-x_{t-1}^s}^2. 
\end{equation}
Besides, by letting $x^+ = \overline{x}_t^s$, $x = x_{t-1}^s$, $v = \nabla f(x_{t-1}^s)$ and $z = x = {x}_{t-1}^s$ in \eqref{eq10}, we have
\begin{equation}\label{eq17}
\Phi(\overline{x}_t^s) \leq \Phi({x}_{t-1}^s) - \frac{1}{\eta}\Iprod{\overline{x}_t^s-x_{t-1}^s}{\overline{x}_t^s - x_{t-1}^s}+\frac{L}{2}\norm{\overline{x}_t^s-x_{t-1}^s}^2 = \Phi({x}_{t-1}^s) -(\frac{1}{\eta}-\frac{L}{2})\norm{\overline{x}_t^s-x_{t-1}^s}^2. 
\end{equation}
We add \eqref{eq16} and \eqref{eq17} to obtain the key inequality
 \begin{equation}\label{eq19}
 \begin{split}
 \Phi({x}_t^s) &\leq \Phi({x}_{t-1}^s) +\frac{L}{2}\norm{{x}_t^s-x_{t-1}^s}^2 - \left(\frac{1}{\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-v_{t-1}^s}{x_t^s - \overline{x}_t^s}\\
 &\,\,\,\, -\frac{1}{\eta} \Iprod{x_t^s-x_{t-1}^s}{x_t^s-\overline{x}_{t}^s}\\
 & = \Phi({x}_{t-1}^s)  +\frac{L}{2}\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-v_{t-1}^s}{x_t^s - \overline{x}_t^s}\\
 &\,\,\,\, -\frac{1}{2\eta} \left(\norm{x_t^s-x_{t-1}^s}^2+ \norm{x_t^s-\overline{x}_{t}^s}^2-\norm{\overline{x}_{t}^s-x_{t-1}^s}^2\right)\\
  & = \Phi({x}_{t-1}^s)  -(\frac{1}{2\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{2\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-v_{t-1}^s}{x_t^s - \overline{x}_t^s}\\
 &\,\,\,\, -\frac{1}{2\eta} \norm{x_t^s-\overline{x}_{t}^s}^2\\
 & \leq \Phi({x}_{t-1}^s)  -(\frac{1}{2\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{2\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-v_{t-1}^s}{x_t^s - \overline{x}_t^s}\\
 &\,\,\,\, -\frac{1}{8\eta} \norm{x_t^s-{x}_{t-1}^s}^2 + \frac{1}{6\eta} \norm{\overline{x}_{t}^s-{x}_{t-1}^s}^2\\
 & = \Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-v_{t-1}^s}{x_t^s - \overline{x}_t^s}\\
  & \leq \Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\eta \norm{\nabla f(x_{t-1}^s)-v_{t-1}^s}^2\\
 \end{split}
 \end{equation}
 where the second inequality Young's inequality and the last inequality holds due to the Lemma \ref{lemma2}.
 
Note that $x_t^s = \Po_{\eta h}(x_{t-1}^s - \eta v_{t-1}^s)$ is the iterated from in our algorithm. Now, we take expectations with all history for \eqref{eq19}.
 \begin{equation}\label{eq25}
 \begin{split} 
\E[\Phi({x}_{t}^s)] \leq \E\left[\Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\eta \norm{\nabla f(x_{t-1}^s)-v_{t-1}^s}^2\right]\\
 \end{split}
 \end{equation}
 Then, we bound the variance term in \eqref{eq25} as follows:
 \begin{align}
  \E&\left[\eta\norm{\nabla f(x_{t-1}^s)-v_{t-1}^s}^2\right]\notag\\
   =&\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\nabla f_i(x_{t-1}^s)-\nabla f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s)-g^s\right)}^2\right]\notag\\
   =&\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\nabla f_i(x_{t-1}^s)-\nabla f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s)-\frac{1}{B}\sum_{j\in I_B}\nabla f_j(\widetilde{x}^{s-1})\right)}^2\right]\notag\\
   =&\notag\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\nabla f_i(x_{t-1}^s)-\nabla f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \nabla f(\tilde{x}^{s-1}) \right)+ \left(\frac{1}{B}\sum_{j\in I_B}\nabla f_j(\widetilde{x}^{s-1}) - \nabla f(\tilde{x}^{s-1})\right)}^2\right]\\
   =&\notag\eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\nabla f_i(x_{t-1}^s)-\nabla f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \nabla f(\tilde{x}^{s-1}) \right)\right)+ \frac{1}{B}\sum_{j\in I_B}\left(\nabla f_j(\widetilde{x}^{s-1}) - \nabla f(\tilde{x}^{s-1})\right)}^2\right]\\
    = &\notag\eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\nabla f_i(x_{t-1}^s)-\nabla f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \nabla f(\tilde{x}^{s-1}) \right)\right)}^2\right]\\
   &\,\, + \eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\nabla f_j(\widetilde{x}^{s-1}) - \nabla f(\tilde{x}^{s-1})\right)}^2\right]\label{eq26}\\
   = &\notag\frac{\eta}{b^2}\E\left[\sum_{i\in I_b}\norm{\left(\left(\nabla f_i(x_{t-1}^s)-\nabla f_i(\widetilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \nabla f(\tilde{x}^{s-1}) \right)\right)}^2\right]\\
   &\,\, \label{eq27}+ \eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\nabla f_j(\widetilde{x}^{s-1}) - \nabla f(\tilde{x}^{s-1})\right)}^2\right]\\
   \leq  &\label{eq28}\frac{\eta}{b^2}\E\left[\sum_{i\in I_b}\norm{\nabla f_i(x_{t-1}^s)-\nabla f_i(\widetilde{x}^{s-1})}^2\right]+ \eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\nabla f_j(\widetilde{x}^{s-1}) - \nabla f(\tilde{x}^{s-1})\right)}^2\right]\\
   \leq  &\label{eq29}\frac{\eta L^2}{b}\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ \frac{I\{B < n\}\eta \sigma ^2}{B}
 \end{align}

 where the expectations are taking with $I_b$ and $I_B$, \eqref{eq26} and \eqref{eq27} holds $\E[\norm{x_1+x_2+\ldots+x_k}^2] = \sum_{i=1}^k \E[\norm{x_i}^2]$ if $x_1,x_2,\ldots,x_k$ are independent and of mean zero (note that $I_b$ and $I_B$ are also independent). \eqref{eq28} uses the fact that $\E[\norm{x-\E[x]}^2] \leq \E[\norm{x}^2]$, for any random variable $x$. \eqref{eq29} holds due to \eqref{eq2} and Assumption \ref{assump1}. 
 
 Now we plug \eqref{eq29} into \eqref{eq25} to obtain

 \begin{align} 
\E&[\Phi({x}_{t}^s)] \notag
\\ \leq& \E\left[\Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\frac{\eta L^2}{b} \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq30}\\
=& \E\left[\Phi({x}_{t-1}^s)  -\frac{13L}{4}\norm{{x}_t^s-x_{t-1}^s}^2- L\norm{\overline{x}_t^s-x_{t-1}^s}^2+\frac{L}{6b} \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq31}\\
=& \E\left[\Phi({x}_{t-1}^s)  -\frac{13L}{4}\norm{{x}_t^s-x_{t-1}^s}^2- \frac{1}{36 L}\norm{\G_{\eta}(x_{t-1}^s)}^2+\frac{L}{6b} \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq32}\\
=& \E\left[\Phi({x}_{t-1}^s)  -\frac{13L}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \frac{1}{36 L}\norm{\G_{\eta}(x_{t-1}^s)}^2+(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq33}
 \end{align}
{\color{blue}
 \begin{align} 
\E&[\Phi({x}_{t}^s)] \notag
\\ \leq& \E\left[\Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\frac{2\eta L^2 d}{b}\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]\\
&+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{eq30}\\
= & \E\left[\Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{\eta}{3}-L\eta^2\right)\norm{\G_{\eta}(x_{t-1}^s)}^2+\frac{2\eta L^2 d}{b}\E\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]
\\&+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{eq31}\\
\leq& \E\left[\Phi({x}_{t-1}^s)  -\frac{13Ld}{4}\norm{{x}_t^s-x_{t-1}^s}^2- \frac{1}{36 Ld}\norm{\G_{\eta}(x_{t-1}^s)}^2+\frac{L d}{3b}\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{L d \mu^2}{12}\right]\label{eq32}
 \end{align}
 {\color{Green}
 Next, we define an useful Lyapunov function as follows:
 \begin{equation}
 R_t^s = \E[\Phi(x_t^s) + c_t\norm{x_t^s - \tilde{x}^s}^2]
 \end{equation}
 where $\{c_t\}$ is a nonnegative sequence. Considering the upper bound of $\norm{x_{t}^s - \tilde{x}^{s-1}}^2$, we have
 \begin{equation}
 \begin{split}
  \norm{x_{t}^s - \tilde{x}^{s-1}}^2 & = \norm{x_{t}^s - x_{t-1}^s + x_{t-1}^s - \tilde{x}^{s-1}}^2 \\
  & = (1+\frac{1}{\alpha}) \norm{x_{t}^s - x_{t-1}^s}^2 + (1+\alpha) \norm{x_{t-1}^s - \tilde{x}^{s-1}}^2
 \end{split}
 \end{equation}
 where $\alpha > 0$. Then we have 
 \begin{align} 
R_t^s = \E&[\Phi({x}_{t}^s) + c_t \norm{x_{t}^s - \tilde{x}^{s-1}}^2] \notag\\
\leq \E&[\Phi({x}_{t}^s) + c_t (1+{\alpha}) \norm{x_{t}^s - x_{t-1}^s}^2 + c_t (1+\frac{1}{\alpha}) \norm{x_{t-1}^s - \tilde{x}^{s-1}}^2]\\
=& \E\left[\Phi({x}_{t-1}^s)  + (c_t (1+{\alpha})-(\frac{5}{8\eta} - \frac{L}{2}))\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{\eta}{3}-L\eta^2\right)\norm{\G_{\eta}(x_{t-1}^s)}^2 \right. \\
&+ (\frac{2\eta L^2 d}{b}+c_t (1+\frac{1}{\alpha}))\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
 \end{align}
 where $\eta = \frac{\rho}{L}$, $c_{t-1} = \frac{2\rho L d}{b}+c_t (1+\frac{1}{\alpha})$. Let $c_m=0$, $\alpha = m$, recursing on $t$. We have 
 \begin{equation}
 \begin{split}
 c_t = \frac{2\rho L d}{b}\frac{(1+\frac{1}{\alpha})^{m-t}-1}{\frac{1}{\alpha}} &= \frac{2\rho Lm d}{b} \left((1+\frac{1}{m})^{m-t}-1\right)\\
 &\leq \frac{2\rho Lm d}{b} (e-1) \leq \frac{4\rho Lmd}{b}
 \end{split}
 \end{equation}
 It follows that 
 \begin{equation}
 \begin{split}
 c_t (1+{\alpha})  + \frac{L}{2} & \leq \frac{4\rho Lmd}{b} (1+m) + \frac{L}{2}\\
 & \leq \frac{8\rho L m^2d}{b} + \frac{L}{2} \\
 & = 2\frac{L}{2\rho} (\frac{8\rho^2 m^2 d}{b} + \frac{\rho}{2})\\
 & \leq \frac{1}{2\eta} \leq \frac{5}{ 8 \eta}
 \end{split}
 \end{equation}
 where $2 (\frac{8\rho^2 m^2 d}{b} + \frac{\rho}{2}) \leq 1$.
 \begin{equation}
  \begin{split} 
R_t^s = \E&[\Phi({x}_{t}^s) + c_t \norm{x_{t}^s - \tilde{x}^{s-1}}^2] \notag\\
\leq & \E\left[\Phi({x}_{t-1}^s)  + (c_t (1+{\alpha})-(\frac{5}{8\eta} - \frac{L}{2}))\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{\eta}{3}-L\eta^2\right)\norm{\G_{\eta}(x_{t-1}^s)}^2 \right] \\
 & + (\frac{2\eta L^2 d}{b}+c_t (1+\frac{1}{\alpha}))\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\\
 & = R_{t-1}^s- \left(\frac{\eta}{3}-L\eta^2\right)\norm{\G_{\eta}(x_{t-1}^s)}^2+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
 \end{split}
 \end{equation}
 Telescoping the above inequality over $t$ from $0$ to $m-1$, since $x_0^s = x_m^{s-1} = \tilde{x}^{s-1}$ and $x_m^s = \tilde{x}^s$, we have 
 \begin{equation}
 \frac{1}{m}\sum_{t=1}^m\norm{\G_{\eta}(x_{t}^s)}^2 \leq \frac{\E[\Phi(\tilde{x}^{s-1}) - \Phi(\tilde{x}^s)]}{m\gamma}+2\frac{I\{B < n\}\eta \sigma ^2}{B\gamma}+\eta \frac{L^2 d^2 \mu^2}{2\gamma}
 \end{equation}
 where $\gamma = \frac{\eta}{3}-L\eta^2$. Summing the above inequality from $1$ to $S$, we have 
  \begin{equation}
  \begin{split}
 \frac{1}{T}\sum_{s=1}^S\sum_{t=1}^m\norm{\G_{\eta}(x_{t}^s)}^2 & \leq \frac{\E[\Phi(\tilde{x}^{0}) - \Phi(\tilde{x}^S)]}{T\gamma}+2\frac{I\{B < n\}\eta \sigma ^2}{B\gamma}+\eta \frac{L^2 d^2 \mu^2}{2\gamma}\\
 & \leq \frac{\E[\Phi(\tilde{x}^{0}) - \Phi({x}^*)]}{T\gamma}+2\frac{I\{B < n\}\eta \sigma ^2}{B\gamma}+\eta \frac{L^2 d^2 \mu^2}{2\gamma}
 \end{split}
 \end{equation}
 where $x^*$ is an optimal solution of problem \eqref{}. 
 
 Given $m = [n^{\frac{1}{3}}]$, $b = [n^{\frac{2}{3}}]$ and 
 $\rho = \frac{1}{6}$, it is easily verified that $2 (\frac{8\rho^2 m^2}{b} + \frac{\rho}{2}) = \frac{11}{18} < 1$. Using $d \geq 1$, we have $\gamma = \frac{\eta}{3}-L\eta^2 = \frac{1}{18 L} - \frac{1}{36 L} = \frac{1}{36 L}$.
 }
 }
 
 where \eqref{eq31} uses $\eta = \frac{1}{6L}$ and \eqref{eq32} uses the definition of gradient mapping $\G_{\eta}(x_{t-1}^s)$ and recall $\overline{x}_t^s := \Po_{\eta h}(x_{t-1}^s - \eta \nabla f(x_{t-1}^s))$. \eqref{eq33} uses $\norm{{x}_t^s-\tilde{x}^{s-1}}^2 \leq (1+\frac{1}{\alpha})\norm{{x}_{t-1}^s-\tilde{x}^{s-1}}^2 + (1+\alpha) \norm{{x}_t^s-x_{t-1}^s}^2$ by choosing $\alpha = 2t-1$.
 
Now, adding \eqref{eq33} for all iterations $1 \leq t \leq m$ in epoch $s$ and recalling that $x_m^s = \tilde{x}^s$ and $x_0^s = \tilde{x}^{s-1}$, we get
\begin{align} 
\E&[\Phi(\tilde{x}^s)]\notag 
\\  
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \frac{1}{36 L}\sum_{t=1}^m\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m}\frac{13L}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\notag\\
&\left.+\sum_{t=1}^m(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \sum_{t=1}^m\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\notag\\
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \frac{1}{36 L}\sum_{t=1}^m\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m-1}\frac{13L}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\notag\\
&\left.+\sum_{t=2}^m(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \sum_{t=1}^m\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq34}\\
=& \E\left[\Phi(\tilde{x}^{s-1}) - \sum_{t=1}^m\frac{1}{36 L}\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m-1}(\frac{13L}{8t}- \frac{L}{6b} - \frac{13L}{8t+4})\norm{{x}_t^s-\tilde{x}^{s-1}}^2+ \sum_{t=1}^m\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\notag\\
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \sum_{t=1}^m\frac{1}{36 L}\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m-1}(\frac{L}{2t^2}- \frac{L}{6b})\norm{{x}_t^s-\tilde{x}^{s-1}}^2+ \sum_{t=1}^m\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\notag\\
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \sum_{t=1}^m\frac{1}{36 L}\norm{\G_{\eta}(x_{t-1}^s)}^2 + \sum_{t=1}^m\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq35}
 \end{align}
 {\color{blue}
  \begin{align} 
\E&[\Phi(\tilde{x}^s)]\notag 
\\  
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \frac{1}{36 Ld}\sum_{t=1}^m\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m}\frac{13Ld}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\notag\\
&\left.+\sum_{t=1}^m(\frac{L d}{3b}+\frac{13Ld}{8t-4})\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ \sum_{t=1}^m\frac{2I\{B < n\}\eta \sigma ^2}{B}+ \sum_{t=1}^m\frac{L d \mu^2}{12}\right]\notag\\
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \frac{1}{36 Ld}\sum_{t=1}^m\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m-1}\frac{13Ld}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\notag\\
&\left.+\sum_{t=2}^m(\frac{L d}{3b}+\frac{13Ld}{8t-4})\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ \sum_{t=1}^m\frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \frac{L d \mu^2}{12}\right]\label{eq34}\\
=& \E\left[\Phi(\tilde{x}^{s-1}) - \sum_{t=1}^m\frac{1}{36 Ld}\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m-1}(\frac{13Ld}{8t}- \frac{Ld}{3b} - \frac{13Ld}{8t+4})\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\\
&\left.+ \sum_{t=1}^m\frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \frac{L d \mu^2}{12}\right]\notag\\
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \sum_{t=1}^m\frac{1}{36 Ld}\norm{\G_{\eta}(x_{t-1}^s)}^2 -\sum_{t=1}^{m-1}(\frac{Ld}{2t^2}- \frac{Ld}{3b})\norm{{x}_t^s-\tilde{x}^{s-1}}^2+ \sum_{t=1}^m\frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \frac{L d \mu^2}{12}\right]\notag\\
\leq& \E\left[\Phi(\tilde{x}^{s-1}) - \sum_{t=1}^m\frac{1}{36 Ld}\norm{\G_{\eta}(x_{t-1}^s)}^2 + \sum_{t=1}^m\frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \frac{L d \mu^2}{12}\right]\label{eq35}
 \end{align}
 }
 where \eqref{eq34} holds since $\norm{.}^2$ always be non-negative and $x_0^s = \tilde{x}^{s-1}$, and \eqref{eq35} holds since $m = \sqrt{b}$. Thus, $\frac{L}{2t^2}- \frac{L}{6b}\geq 0$ {\color{blue} $\frac{Ld}{2t^2}- \frac{Ld}{6b}\geq 0$} for all $1\leq t < m$.
 
 Now we sum up \eqref{eq35} for all epochs $1\leq s \leq S$ to finish the proof as follows:
 \begin{equation*}
\begin{split} 
0 \leq \E&[\Phi(\tilde{x}^S) - \Phi({x}^*)] \leq \E\left[\Phi(\tilde{x}^{0}) - \Phi({x}^*) - \sum_{s=1}^S\sum_{t=1}^m\frac{1}{36 L}\norm{\G_{\eta}(x_{t-1}^s)}^2 + \sum_{s=1}^S\sum_{t=1}^m\frac{I\{B < n\}\eta \sigma ^2}{B}\right]
 \end{split}
 \end{equation*}
  \begin{align}
\E[\norm{\G_{\eta}(\hat{x})}^2] & \leq \frac{36L\left(\Phi(x_0) - \Phi({x}^*)\right)}{Sm} + \frac{I\{B < n\}36 L\eta \sigma ^2}{B}\label{eq36}\\
 & = \frac{36L\left(\Phi(x_0) - \Phi({x}^*)\right)}{Sm} + \frac{I\{B < n\}6 \sigma ^2}{B} = 2\epsilon\label{eq37}
 \end{align}
 {\color{blue}
 \begin{equation*}
\begin{split} 
0 \leq \E&[\Phi(\tilde{x}^S) - \Phi({x}^*)] \leq \E\left[\Phi(\tilde{x}^{0}) - \Phi({x}^*) - \sum_{s=1}^S\sum_{t=1}^m\frac{1}{36 L d}\norm{\G_{\eta}(x_{t-1}^s)}^2 + \sum_{s=1}^S\sum_{t=1}^m(\frac{I\{B < n\}\eta \sigma ^2}{B}+\frac{L d \mu^2}{12})\right]
 \end{split}
 \end{equation*}
  \begin{align}
\E[\norm{\G_{\eta}(\hat{x})}^2] & \leq \frac{36Ld\left(\Phi(x_0) - \Phi({x}^*)\right)}{Sm} + \frac{I\{B < n\}36 L d\eta \sigma ^2}{B}+3{L^2 d^2 \mu^2}\label{eq36}\\
 & = \frac{36Ld\left(\Phi(x_0) - \Phi({x}^*)\right)}{Sm} + \frac{I\{B < n\}6 \sigma ^2}{B}+3{L^2 d^2 \mu^2} = 3\epsilon\label{eq37}
 \end{align}
 
 }
 where \eqref{eq36} holds since $\hat{x}$ is chosen uniformly randomly from $\{x_{t-1}^s\}_{t\in [m], s\in [S]}$, and \eqref{eq37} uses $\eta = \frac{1}{6L}$. Now we obtain the total number of iterations $T = Sm = S\sqrt{b} = \frac{36L\left(\Phi(x_0) - \Phi({x}^*)\right)}{\epsilon}$. The proof is finished since the number of SFO call equals to $Sn+Smb = 36 L \left(\Phi(x_0) - \Phi({x}^*)\right) (\frac{n}{\epsilon\sqrt{b}}+\frac{b}{\epsilon})$ if $B=n$ (i.e., the second term in \eqref{eq37} is $0$ and thus assumption \ref{assump1} is not needed), or equals to $Sn+Smb = 36 L \left(\Phi(x_0) - \Phi({x}^*)\right) (\frac{B}{\epsilon\sqrt{b}}+\frac{b}{\epsilon})$ if $B < n$ (note that $\frac{I\{B < n\}6\sigma^2}{B} \leq \epsilon$ since $B \geq 5\sigma^2 /\epsilon$). 
\end{proof}

\section{Convergence Under PL Condition}
In this section, we provide the global linear convergence rate for nonconvex functions under the Polyak-Lojasiewicz
(PL) condition [Polyak, 1963]. The original form of PL condition is
\begin{equation}
\exists \mu >0, \text{such~that} \norm{\nabla f(x)}^2 \geq 2\mu (f(x) - f^*),\,\, \forall x,
\end{equation}
where $f^*$ denotes the (global) optimal function value. It is worth noting that $f$ satisfies PL condition when $f$ is $\mu$-strongly convex.

Due to the nonsmooth term $h(x)$ in problem \eqref{}, we use the gradient mapping to define a more general form of PL condition as follows
\begin{equation}
\exists \mu >0, \text{such~that} \norm{G_{\eta}(x)}^2 \geq 2\mu (\Phi(x) - \Phi^*),\,\, \forall x.
\end{equation}
Recall that if $h(x)$ is a constant function, the gradient mapping reduces to $G_{\eta}(x) = \nabla f(x)$.

We want to point out that \cite{} used the following form of PL condition
 \begin{equation}
\exists \mu >0, \text{such~that} D_h(x,\alpha) \geq 2\mu (\Phi(x) - \Phi^*),\,\, \forall x.
\end{equation} 
where $D_h(x,\alpha) := -2\alpha \min_y\{\Iprod{\nabla f(x)}{y-x} + \frac{\alpha}{2}\norm{y-x}^2+h(y)-h(x)\}$. Our PL condition is arguably more natural.
\begin{theorem}
Let step size $\eta = \frac{1}{6L}$ and $b$ denote the minibatch size. Then the final iteration point $\tilde{x}^S$ in Algorithm \ref{} satisfies $\E[\Phi(\tilde{x}^S) - \Phi^*]\leq \epsilon$ under PL condition. We distinguish the following two cases:

1) We let batch size $B = n$. The number of SFO calls is bounded by
\[
O\left(\frac{n}{\mu \sqrt{b}}\log \frac{1}{\epsilon}+\frac{b}{\mu}\log\frac{1}{\epsilon}\right).
\]

2) Under Assumption 1, we let batch size $B = \min\{\frac{6\sigma^2}{\mu\epsilon},n\}$. The number of SFO calls is bounded by
\[
O\left((n\wedge\frac{1}{\mu \epsilon})\frac{1}{\mu \sqrt{b}}\log \frac{1}{\epsilon}+\frac{b}{\mu}\log\frac{1}{\epsilon}\right).
\]
where $\wedge$ denotes the minimum.

3) In both cases, the number of PO calls equals to the total number of iterations $T$ which is bounded by
\[
O\left(\frac{1}{\mu}\log\frac{1}{\epsilon}\right).
\]
\end{theorem}
\begin{proof}
First, we recall a key inequality \eqref{eq33} from the proof of Theorem 1, i.e.,

\begin{equation}\label{eq44}
\begin{split}
\E\Phi({x}_{t}^s)&\\
\E&\left[\Phi({x}_{t-1}^s)  -\frac{13L}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \frac{1}{36 L}\norm{\G_{\eta}(x_{t-1}^s)}^2+(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{I\{B < n\}\eta \sigma ^2}{B}\right]
\end{split}
\end{equation}
{\color{blue}
\begin{equation}\label{eq44}
\begin{split}
\E&[\Phi({x}_{t}^s)]
\\  
\leq& \E\left[\Phi({x}_{t-1}^s) - \frac{1}{36 Ld}\norm{\G_{\eta}(x_{t-1}^s)}^2 -\frac{13L d}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\\
&\left.+(\frac{L d}{6b}+\frac{13Ld}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{L d \mu^2}{12}\right]\\
\end{split}
\end{equation}
}
Then, we plug the following PL inequality 
\begin{equation}
\norm{G_{\eta}(x)}^2 \geq 2\mu (\Phi(x) - \Phi^*)
\end{equation}
into \eqref{eq44} to get
\begin{equation}\label{eq45}
\begin{split}
\E&\Phi({x}_{t}^s)\\
\leq\E&\left[\Phi({x}_{t-1}^s)  -\frac{13L}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \frac{\mu}{18 L}(\Phi({x}_{t-1}^s) - \Phi^*)+(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{I\{B < n\}\eta \sigma ^2}{B}\right]
\end{split}
\end{equation}
{\color{blue}
\begin{equation}\label{eq45}
\begin{split}
\E&[\Phi({x}_{t}^s)]
\\  
\leq& \E\left[\Phi({x}_{t-1}^s) - \frac{\mu}{18 L d}(\Phi({x}_{t-1}^s) - \Phi^*) -\frac{13L d}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\\
&\left.+(\frac{L d}{6b}+\frac{13Ld}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{L d \mu^2}{12}\right]\\
\end{split}
\end{equation}
}
Then, we obtain
\begin{equation}\label{eq46}
\begin{split}
\E&[\Phi({x}_{t}^s)-\Phi^*]\\
\leq\E&\left[\left(1- \frac{\mu}{18 L}\right)(\Phi({x}_{t-1}^s) - \Phi^*)  -\frac{13L}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2+(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{I\{B < n\}\eta \sigma ^2}{B}\right]
\end{split}
\end{equation}
{\color{blue}
\begin{equation}\label{eq46}
\begin{split}
\E&[\Phi({x}_{t}^s)-\Phi^*] 
\\  
\leq& \E\left[\left(1- \frac{\mu}{18 Ld}\right)(\Phi({x}_{t-1}^s) - \Phi^*) -\frac{13L d}{8t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\\
&\left.+(\frac{L d}{6b}+\frac{13Ld}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{L d \mu^2}{12}\right]\\
\end{split}
\end{equation}
}
Let $\alpha := 1 - \frac{\mu}{18 L}$ {\color{blue}$\alpha := 1 - \frac{\mu}{18 Ld}$} and $\Psi_t^s := \frac{\E[\Phi({x}_{t}^s)-\Phi^*]}{\alpha^t}$. Plugging them into \eqref{eq46}, we have  

\begin{equation}\label{eq47}
\begin{split}
\Psi_t^s&\\
\leq\Psi_{t-1}^s-\E&\left[\frac{13L}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\frac{1}{\alpha^t}(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 - \frac{1}{\alpha^t}\frac{I\{B < n\}\eta \sigma ^2}{B}\right]
\end{split}
\end{equation}
{\color{blue}
\begin{equation}\label{eq47}
\begin{split}
\Psi_t^s
\\  
\leq& \Psi_{t-1}^s - \E\left[\frac{13L d}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right.\\
&\left.-\frac{1}{\alpha^t}(\frac{L d}{6b}+\frac{13Ld}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 - \frac{1}{\alpha^t}\frac{2I\{B < n\}\eta \sigma ^2}{B}- \frac{1}{\alpha^t}\frac{L d \mu^2}{12}\right]\\
\end{split}
\end{equation}
}
Now, adding \eqref{eq47} from all iterations $1\leq t \leq m$ in epoch $s$ and recalling that $x_m^s = \tilde{x}^s$ and $x_0^s = \tilde{x}^{s-1}$, we have 
\begin{align}
\E&[\Phi(\tilde{x}^s)-\Phi^*]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] +\alpha^m\sum_{t=1}^m \frac{1}{\alpha^t}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{13L}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=1}^m\frac{1}{\alpha^t}(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{13L}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=1}^m\frac{1}{\alpha^t}(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{13L}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=2}^m\frac{1}{\alpha^t}(\frac{L}{6b}+\frac{13L}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\label{eq48}\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{1}{\alpha^{t+1}}\left(\frac{13L\alpha}{8t}-\frac{L}{6b} - \frac{13L}{8t+4}\right) \norm{x_{t}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{1}{\alpha^{t+1}}\left(\frac{13L}{8t}(1-\frac{1}{18\sqrt{n}})-\frac{L}{6b} - \frac{13L}{8t+4}\right) \norm{x_{t}^s-\tilde{x}^{s-1}}^2 \right]\label{eq49}\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{L}{\alpha^{t+1}}\left(\frac{1}{2t^2}-\frac{1}{8\sqrt{n}t}-\frac{1}{6b} \right) \norm{x_{t}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{I\{B < n\}\eta \sigma ^2}{B}\label{eq50}
\end{align}
{\color{blue}
\begin{align}
\E&[\Phi(\tilde{x}^s)-\Phi^*]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] +\alpha^m\sum_{t=1}^m \frac{1}{\alpha^t}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\alpha^m\sum_{t=1}^m \frac{1}{\alpha^t}\frac{L d \mu^2}{12}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{13Ld}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=1}^m\frac{1}{\alpha^t}(\frac{Ld}{6b}+\frac{13Ld}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{L d \mu^2}{12}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{13Ld}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=1}^m\frac{1}{\alpha^t}(\frac{Ld}{6b}+\frac{13Ld}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{L d \mu^2}{12}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{13Ld}{8t\alpha^t}\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=2}^m\frac{1}{\alpha^t}(\frac{Ld}{6b}+\frac{13Ld}{8t-4}) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\label{eq48}\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{L d \mu^2}{12}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{1}{\alpha^{t+1}}\left(\frac{13Ld\alpha}{8t}-\frac{Ld}{6b} - \frac{13Ld}{8t+4}\right) \norm{x_{t}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{L d \mu^2}{12}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{1}{\alpha^{t+1}}\left(\frac{13Ld}{8t}(1-\frac{1}{18\sqrt{n}})-\frac{Ld}{6b} - \frac{13Ld}{8t+4}\right) \norm{x_{t}^s-\tilde{x}^{s-1}}^2 \right]\label{eq49}\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{L d \mu^2}{12}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{Ld}{\alpha^{t+1}}\left(\frac{1}{2t^2}-\frac{1}{8\sqrt{n}t}-\frac{1}{6b} \right) \norm{x_{t}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{L d \mu^2}{12}\label{eq50}
\end{align}
}
where \eqref{eq48} holds since $\norm{.}^2$ always be non-negative and $x_0^s=\tilde{x}^{s-1}$. \eqref{eq49} holds since $\alpha = 1 - \frac{\mu}{18L}$ and the assumption $L/\mu > \sqrt{n}$. \eqref{eq50} holds since it is sufficient to show that $\Gamma_t \leq 0$ for all $1\leq t < m$, where 
$\Gamma_t = \frac{1}{2t^2} - \frac{1}{8\sqrt{n} t}-\frac{1}{6b}$. Taking a derivative for $\Gamma_t$, we get $\Gamma'_t = -\frac{1}{t^3}+\frac{1}{8\sqrt{n}t^2} = -\frac{8\sqrt{n}-t}{8\sqrt{n}t^3} < 0$ since $t<m = \sqrt{b}\leq \sqrt{n}$. Thus, $\Gamma_t$ decreases in $t$. We only need to show that $\Gamma_m = \Gamma_{\sqrt{b}}\geq 0$, i.e., $\frac{1}{2b} - \frac{1}{8\sqrt{n}b} - \frac{1}{6b} = \frac{1}{3b} - \frac{1}{8\sqrt{n}b}\geq 0$. It is easy to see that this inequality holds since $b\leq n$. 

Similarly, let  $\tilde{\alpha} = \alpha^m$ and $\tilde{\Psi}^s = \frac{\E[\Phi(\tilde{x}^{s})-\Phi^*]}{\tilde{\alpha}^s}$. Plugging them into \eqref{eq50}, we have
\begin{equation}\label{eq51}
\widetilde{\Psi}^s \leq \widetilde{\Psi}^{s-1} + \frac{1}{\tilde{\alpha}^s} \frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}
\end{equation}
{\color{blue}
\begin{equation}\label{eq51}
\widetilde{\Psi}^s \leq \widetilde{\Psi}^{s-1} + \frac{1}{\tilde{\alpha}^s} \frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}+ \frac{1}{\tilde{\alpha}^s} \frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{L d \mu^2}{12}
\end{equation}
}
Now, we sum up \eqref{eq51} for all epochs $1 \leq s \leq S$ to finish the proof as follows:
\begin{align}
\E[\Phi(\widetilde{x}^S) - {\Phi}^*] & \leq \widetilde{\alpha}^{S} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \widetilde{\alpha}^{S} \sum_{s=1}^S \frac{1}{\widetilde{\alpha}^s}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag\\
& = {\alpha}^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{1-\widetilde{\alpha}^S}{1-\widetilde{\alpha}}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag\\
& \leq {\alpha}^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{1}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}\notag\\
& = \left(1-\frac{\mu}{18L}\right)^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{I\{B < n\}18L\eta \sigma ^2}{\mu B}\label{eq52}\\
& = \left(1-\frac{\mu}{18L}\right)^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{I\{B < n\}3 \sigma ^2}{\mu B} = 2 \epsilon\label{eq53}
\end{align}
{\color{blue}
\begin{align}
\E[\Phi(\widetilde{x}^S) - {\Phi}^*] & \leq \widetilde{\alpha}^{S} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \widetilde{\alpha}^{S} \sum_{s=1}^S \frac{1}{\widetilde{\alpha}^s}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B} + \widetilde{\alpha}^{S} \sum_{s=1}^S \frac{1}{\widetilde{\alpha}^s}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{L d \mu^2}{12}\notag\\
& = {\alpha}^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{1-\widetilde{\alpha}^S}{1-\widetilde{\alpha}}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}+ \frac{1-\widetilde{\alpha}^S}{1-\widetilde{\alpha}}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{L d \mu^2}{12}\notag\\
& \leq {\alpha}^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{1}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}+ \frac{1}{1-{\alpha}}\frac{L d \mu^2}{12}\notag\\
& = \left(1-\frac{\mu}{18Ld}\right)^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{I\{B < n\}18Ld\eta \sigma ^2}{\mu B}+ \frac{18 L^2 d^2 \mu}{12}\label{eq52}\\
& = \left(1-\frac{\mu}{18Ld}\right)^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{I\{B < n\}3 \sigma ^2}{\mu B} + \frac{3 L^2 d^2 \mu}{2}= 3 \epsilon\label{eq53}
\end{align}
}
where \eqref{eq52} holds since $\alpha = 1-\frac{\mu}{18L}$ {\color{blue}$\alpha = 1-\frac{\mu}{18Ld}$}, and \eqref{eq53} uses $\eta = \frac{1}{6L}${\color{blue}$\eta = \frac{1}{6Ld}$}.

From \eqref{eq53}, we obtain the total number of iterations $T = Sm = S\sqrt{b} = O(\frac{1}{\mu}\log\frac{1}{\epsilon})$. The number of PO calls equals to $T = Sm = O(\frac{1}{\mu}\log\frac{1}{\epsilon})$. The number of SFO calls equals to $Sn+Smb = O(\frac{n}{\mu \sqrt{b}}\log\frac{1}{\epsilon}+\frac{b}{\mu}\log\frac{1}{\epsilon})$ if $B = n$, or equals to  $Sn+Smb = O(\frac{B}{\mu \sqrt{b}}\log\frac{1}{\epsilon}+\frac{b}{\mu}\log\frac{1}{\epsilon})$ if $B < n$(note that $\frac{I\{B < n\}3 \sigma ^2}{\mu B} \leq \epsilon$ since $B \geq 6 {\sigma ^2}/{\mu \epsilon}$).
{\color{blue} $\mu \leq \frac{2\epsilon}{3 L^2 d^2}$}
\end{proof}
{\color{Brown}
 \begin{align} 
\E&[\Phi({x}_{t}^s)] \notag
\\ \leq& \E\left[\Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\frac{2\eta L^2 d}{b}\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]\\
&+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{eq30}\\
= & \E\left[\Phi({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{\eta}{3}-L\eta^2\right)\norm{\G_{\eta}(x_{t-1}^s)}^2+\frac{2\eta L^2 d}{b}\E\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]
\\&+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{eq31}\\
\leq& \E\left[\Phi({x}_{t-1}^s)  -\frac{13Ld}{4}\norm{{x}_t^s-x_{t-1}^s}^2- \frac{1}{36 Ld}\norm{\G_{\eta}(x_{t-1}^s)}^2+\frac{L d}{3b}\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{L d \mu^2}{12}\right]\label{eq32}
 \end{align}
 Thus, we have
  \begin{align} 
\E&[\Phi({x}_{t}^s)] \notag
\\ \leq& \E\left[\Phi({x}_{t-1}^s)  -\frac{13Ld}{4}\norm{{x}_t^s-x_{t-1}^s}^2- \frac{1}{36 Ld}\norm{\G_{\eta}(x_{t-1}^s)}^2+\frac{L d}{3b}\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]\right.\\
&\left.+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{L d \mu^2}{12}\right]\label{eq32}
 \end{align}
 Then, we plug the following PL inequality 
\begin{equation}
\norm{G_{\eta}(x)}^2 \geq 2\mu (\Phi(x) - \Phi^*)
\end{equation}
\begin{align} 
\E&[\Phi({x}_{t}^s)-\Phi^*] \notag
\\ \leq& \E\left[(1- \frac{\mu}{18 Ld})(\Phi({x}_{t-1}^s)-\Phi^*)  -\frac{13Ld}{4}\norm{{x}_t^s-x_{t-1}^s}^2+\frac{L d}{3b}\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]\right.\\
&\left.+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{L d \mu^2}{12}\right]\label{eq32}
 \end{align}
 Next, we define an useful Lyapunov function as follows:
 \begin{equation}
 R_t^s = \E[\Phi(x_t^s) -\Phi^*+ c_t\norm{x_t^s - \tilde{x}^s}^2]
 \end{equation}
 where $\{c_t\}$ is a nonnegative sequence. Considering the upper bound of $\norm{x_{t}^s - \tilde{x}^{s-1}}^2$, we have
 \begin{equation}
 \begin{split}
  \norm{x_{t}^s - \tilde{x}^{s-1}}^2 & = \norm{x_{t}^s - x_{t-1}^s + x_{t-1}^s - \tilde{x}^{s-1}}^2 \\
  & = (1+\frac{1}{\alpha}) \norm{x_{t}^s - x_{t-1}^s}^2 + (1+\alpha) \norm{x_{t-1}^s - \tilde{x}^{s-1}}^2
 \end{split}
 \end{equation}
 where $\alpha > 0$. Then we have 
 \begin{align} 
R_t^s = \E&[\Phi({x}_{t}^s) -\Phi^*+ c_t \norm{x_{t}^s - \tilde{x}^{s-1}}^2] \notag\\
\leq \E&[\Phi({x}_{t}^s)-\Phi^* + c_t (1+{\alpha}) \norm{x_{t}^s - x_{t-1}^s}^2 + c_t (1+\frac{1}{\alpha}) \norm{x_{t-1}^s - \tilde{x}^{s-1}}^2]\\
=& \E\left[(\Phi({x}_{t-1}^s)-\Phi^*) - \left(\frac{\eta}{3}-L\eta^2\right)\norm{\G_{\eta}(x_{t-1}^s)}^2  + (c_t (1+{\alpha})-(\frac{5}{8\eta} - \frac{L}{2}))\norm{{x}_t^s-x_{t-1}^s}^2 \right. \\
 & + (\frac{2\eta L^2 d}{b}+c_t (1+\frac{1}{\alpha}))\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\\
=& \E\left[(1-{\color{red}{2\mu}{\gamma}})(\Phi({x}_{t-1}^s)-\Phi^*)  + (c_t (1+{\alpha})-(\frac{5}{8\eta} - \frac{L}{2}))\norm{{x}_t^s-x_{t-1}^s}^2 \right. \\
 & + (\frac{2\eta L^2 d}{b}+c_t (1+\frac{1}{\alpha}))\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
 \end{align}
 where $\gamma = \frac{\eta}{3}-L\eta^2$, $\eta = \frac{\rho}{L}$, $c_{t-1} = \frac{2\rho L d}{b}+c_t (1+\frac{1}{\alpha})$. Let $c_m=0$, $\alpha = m$, recursing on $t$. We have 
 \begin{equation}
 \begin{split}
 c_t = \frac{2\rho L d}{b}\frac{(1+\frac{1}{\alpha})^{m-t}-1}{\frac{1}{\alpha}} &= \frac{2\rho Lm d}{b} \left((1+\frac{1}{m})^{m-t}-1\right)\\
 &\leq \frac{2\rho Lm d}{b} (e-1) \leq \frac{4\rho Lmd}{b}
 \end{split}
 \end{equation}
 It follows that 
 \begin{equation}
 \begin{split}
 c_t (1+{\alpha})  + \frac{L}{2} & \leq \frac{4\rho Lmd}{b} (1+m) + \frac{L}{2}\\
 & \leq \frac{8\rho L m^2d}{b} + \frac{L}{2} \\
 & = 2\frac{L}{2\rho} (\frac{8\rho^2 m^2 d}{b} + \frac{\rho}{2})\\
 & \leq \frac{1}{2\eta} \leq \frac{5}{ 8 \eta}
 \end{split}
 \end{equation}
 where $2 (\frac{8\rho^2 m^2 d}{b} + \frac{\rho}{2}) \leq 1$.
 {\color{red}
   where $\eta = \frac{\rho}{L}$, $\beta c_{t-1} = \frac{2\rho L d}{b}+c_t (1+\frac{1}{\alpha})$. Let $c_m=0$, $\alpha = {2}$, recursing on $t$. We have 
 \begin{equation}
 \begin{split}
 c_t = \frac{2\rho L d}{b\beta}\frac{(\frac{1}{\beta}+\frac{1}{\beta\alpha})^{m-t}-1}{\frac{1}{\alpha\beta}+\frac{1}{\beta}-1}=\frac{2\rho L d}{b\beta}\frac{(\frac{1}{\beta}+\frac{1}{\beta\alpha})^{m-t}-1}{\frac{1}{\alpha\beta}} &= \frac{2\rho L d}{b\beta} \left((1+\frac{1}{\beta})^{m-t}-1\right)\\
 &\leq \frac{2\rho L d}{b\beta} \left((1+\frac{1}{\beta})^{m-t}-1\right) \leq \frac{4\rho Ld 3^{m-t}}{b}
 \end{split}
 \end{equation}
 It follows that 
 \begin{equation}
 \begin{split}
 c_t (1+{\alpha})  + \frac{L}{2} & \leq \frac{4\rho Ld 3^{m+1-t}}{b}  + \frac{L}{2}\\
 & = 2\frac{L}{2\rho} (\frac{4\rho^2 3^{m+1-t} d}{b} + \frac{\rho}{2})\\
 & \leq \frac{1}{2\eta} \leq \frac{5}{ 8 \eta}
 \end{split}
 \end{equation}
 where $2 (\frac{4\rho^2 3^{m+1-t} d}{b} + \frac{\rho}{2}) \leq 1$. {\color{Green} Be carfull make $c_{m+1} = 0$, $0< \beta\leq \alpha$}
 
 }
 \begin{equation}
  \begin{split} 
R_t^s = \E&[\Phi({x}_{t}^s)-\Phi^* + c_t \norm{x_{t}^s - \tilde{x}^{s-1}}^2] \notag\\
\leq & \E\left[(1- \frac{\mu}{18 Ld})(\Phi({x}_{t-1}^s)-\Phi^*)  + (c_t (1+{\alpha})-(\frac{5}{8\eta} - \frac{L}{2}))\norm{{x}_t^s-x_{t-1}^s}^2 \right] \\
 & + (\frac{2\eta L^2 d}{b}+c_t (1+\frac{1}{\alpha}))\E\left[\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\\
 & = \beta(\Phi({x}_{t-1}^s)-\Phi^*)+\beta c_{t-1}\norm{x_{t-1}^s-\widetilde{x}^{s-1}}^2+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\\
 \\
 & = \beta R_{t-1}^s+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
 \end{split}
 \end{equation}
 Thus, we obtain,
 \begin{equation}\label{eq47}
\begin{split}
\Psi_t^s\leq& \Psi_{t-1}^s - \E\left[- \frac{1}{\alpha^t}\frac{2I\{B < n\}\eta \sigma ^2}{B}- \frac{1}{\alpha^t}\eta \frac{L^2 d^2 \mu^2}{2}\right]\\
\end{split}
\end{equation}
with $\Psi_t^s := \frac{\E[\Phi({x}_{t}^s)-\Phi^*]+ c_t \norm{x_{t}^s - \tilde{x}^{s-1}}^2}{{\alpha}^t}$. Telescoping the above inequality over $t$ from $0$ to $m-1$, since $x_0^s = x_m^{s-1} = \tilde{x}^{s-1}$ and $x_m^s = \tilde{x}^s$, we have 
\begin{align}
\E&[\Phi(\tilde{x}^s)-\Phi^*]+ c_m \norm{\tilde{x}^{s} - \tilde{x}^{s-1}}^2\notag\\
\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] +\alpha^m\sum_{t=1}^m \frac{1}{\alpha^t}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\alpha^m\sum_{t=1}^m \frac{1}{\alpha^t}\eta \frac{L^2 d^2 \mu^2}{2}\notag
\\\leq\alpha^m\E&\left[(\Phi(\tilde{x}^{s-1}) - \Phi^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\eta \frac{L^2 d^2 \mu^2}{2}\label{eq50}
\end{align}
 {\color{blue}
\begin{equation}\label{eq51}
\widetilde{\Psi}^s + \frac{c_m}{\tilde{\alpha}^s} \norm{\tilde{x}^{s} - \tilde{x}^{s-1}}^2\leq \widetilde{\Psi}^{s-1} + \frac{1}{\tilde{\alpha}^s} \frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}+ \frac{1}{\tilde{\alpha}^s} \frac{1-\tilde{\alpha}}{1-{\alpha}}\eta \frac{L^2 d^2 \mu^2}{2}
\end{equation}
}
with $\widetilde{\Psi}^s := \frac{\E[\Phi(\tilde{x}^s)-\Phi^*]}{{\alpha}^s}$. Now, we sum up \eqref{eq51} for all epochs $1 \leq s \leq S$ to finish the proof as follows:

{\color{blue}
\begin{align}
\E[\Phi(\widetilde{x}^S) - {\Phi}^*]+ \sum_{s=1}^S\frac{c_m}{\tilde{\alpha}^s}\norm{\tilde{x}^{s} - \tilde{x}^{s-1}}^2 & \leq \widetilde{\alpha}^{S} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \widetilde{\alpha}^{S} \sum_{s=1}^S \frac{1}{\widetilde{\alpha}^s}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B} + \widetilde{\alpha}^{S} \sum_{s=1}^S \frac{1}{\widetilde{\alpha}^s}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\eta \frac{L^2 d^2 \mu^2}{2}\notag\\
& = {\alpha}^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{1-\widetilde{\alpha}^S}{1-\widetilde{\alpha}}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}+ \frac{1-\widetilde{\alpha}^S}{1-\widetilde{\alpha}}\frac{1-\widetilde{\alpha}}{1-{\alpha}}\eta \frac{L^2 d^2 \mu^2}{2}\notag\\
& \leq {\alpha}^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{1}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}+ \frac{1}{1-{\alpha}}\eta \frac{L^2 d^2 \mu^2}{2}\notag\\
& = \left(1-{\color{red}{2\mu}{\gamma}}\right)^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{I\{B < n\}\eta \sigma ^2}{2\mu\gamma B}+ \eta \frac{L^2 d^2 \mu^2}{2\gamma\mu}\label{eq52}\\
& = \left(1-{\color{red}{2\mu}{\gamma}}\right)^{Sm} \E[\Phi(\widetilde{x}^0) - {\Phi}^*] + \frac{I\{B < n\}\eta \sigma ^2}{2\mu\gamma B} + \eta \frac{L^2 d^2 \mu^2}{2\gamma\mu}= 3 \epsilon\label{eq53}
\end{align}
}
where \eqref{eq52} holds since $\alpha = 1-\frac{\mu}{18L}$ {\color{blue}$\alpha = 1-\frac{\mu}{18Ld}$}, and \eqref{eq53} uses $\eta = \frac{1}{6L}${\color{blue}$\eta = \frac{1}{6Ld}$}.
 
 }
\section{Proof Under Form 8}
First, similar to [Reddi et al., 2016b], we need the following inequality:
\begin{align}
\Phi(\overline{x}^s_t) &= f(\overline{x}^s_t)+h(\overline{x}^s_t) + h({x}^s_{t-1}) - h({x}^s_{t-1})\notag \\
&\leq f({x}^s_{t-1})+\Iprod{\nabla f({x}^s_{t-1})}{\overline{x}^s_t-{x}^s_{t-1}}+\frac{L}{2}\norm{\overline{x}^s_t - {x}^s_{t-1}}^2+h(\overline{x}^s_t) + h({x}^s_{t-1}) - h({x}^s_{t-1})\label{eq54}\\
&= \Phi({x}^s_{t-1})+\Iprod{\nabla f({x}^s_{t-1})}{\overline{x}^s_t-{x}^s_{t-1}}+\frac{L}{2}\norm{\overline{x}^s_t - {x}^s_{t-1}}^2+h(\overline{x}^s_t) - h({x}^s_{t-1})\\
&\leq \Phi({x}^s_{t-1})+\Iprod{\nabla f({x}^s_{t-1})}{\overline{x}^s_t-{x}^s_{t-1}}+\frac{1}{2\eta}\norm{\overline{x}^s_t - {x}^s_{t-1}}^2+h(\overline{x}^s_t) - h({x}^s_{t-1})\label{eq55}\\
&= \Phi({x}^s_{t-1})-\frac{\eta}{2} D_h({x}^s_{t-1},\frac{1}{\eta})\label{eq56}\\
&\leq \Phi({x}^s_{t-1})-\eta\mu(\Phi({x}^s_{t-1}) - \Phi^*)\label{eq57}
\end{align}
where \eqref{eq54} holds since $f$ has $L$-Lipschitz continuous gradient, \eqref{eq55} holds due to $\eta = \frac{1}{6L} < \frac{1}{L}$, \eqref{eq56} follows from the
definition of $D_h$ and recall $\overline{x}^s_t = \Po_{\eta h}({x}^s_{t-1}-\eta \nabla f({x}^s_{t-1}))$, and     
\eqref{eq57} follows from the definition of PL condition with from \eqref{eq8}. 

Then, adding $\frac{9}{11}$ times \eqref{eq17} and $\frac{2}{11}$ times \eqref{eq57}, we have 
\begin{align}
\Phi(\overline{x}^s_t) &\leq \Phi({x}^s_{t-1}) -\frac{9}{11}\left(\frac{1}{\eta}-\frac{L}{2}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2}{11}\eta\mu(\Phi(\overline{x}^s_{t-1})-\Phi^*)\notag\\
&\leq \Phi({x}^s_{t-1}) -\left(\frac{9}{11\eta}-\frac{9L}{22}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi(\overline{x}^s_{t-1})-\Phi^*)\label{eq58}
\end{align}
We add \eqref{eq58} and \eqref{eq16} to obtain the following inequality:
\begin{align}
\Phi({x}^s_t) &\leq \Phi({x}^s_{t-1}) +\frac{L}{2}\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\left(\frac{9}{11\eta}-\frac{9L}{22}-\frac{L}{2}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi({x}^s_{t-1})-\Phi^*)\notag\\
&-\frac{1}{\eta}\Iprod{{x}^s_t - {x}^s_{t-1}}{{x}^s_t - \overline{x}^s_{t}}+\Iprod{\nabla f({x}^s_{t-1}) - v_{t-1}^s}{{x}^s_t - \overline{x}^s_{t}}\notag\\
&= \Phi({x}^s_{t-1}) +\frac{L}{2}\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\left(\frac{9}{11\eta}-\frac{9L}{22}-\frac{L}{2}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi({x}^s_{t-1})-\Phi^*)\notag\\
&-\frac{1}{2\eta}(\norm{{x}^s_t - {x}^s_{t-1}}^2+\norm{{x}^s_t - \overline{x}^s_{t}}^2-\norm{\overline{x}^s_t - {x}^s_{t-1}}^2)+\Iprod{\nabla f({x}^s_{t-1}) - v_{t-1}^s}{{x}^s_t - \overline{x}^s_{t}}\notag\\
&= \Phi({x}^s_{t-1}) - (\frac{1}{2\eta} - \frac{L}{2})\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\left(\frac{7}{22\eta}-\frac{10L}{11}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi({x}^s_{t-1})-\Phi^*)\notag\\
&-\frac{1}{2\eta}\norm{{x}^s_t - \overline{x}^s_{t}}^2+\Iprod{\nabla f({x}^s_{t-1}) - v_{t-1}^s}{{x}^s_t - \overline{x}^s_{t}}\notag\\
&\leq \Phi({x}^s_{t-1}) - (\frac{1}{2\eta} - \frac{L}{2})\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\left(\frac{7}{22\eta}-\frac{10L}{11}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi({x}^s_{t-1})-\Phi^*)\notag\\
&-\frac{1}{8\eta}\norm{{x}^s_t - {x}^s_{t-1}}^2+\frac{1}{6\eta}\norm{\overline{x}^s_t - {x}^s_{t-1}}^2+\Iprod{\nabla f({x}^s_{t-1}) - v_{t-1}^s}{{x}^s_t - \overline{x}^s_{t}}\label{eq59}\\
&= \Phi({x}^s_{t-1}) - (\frac{5}{8\eta} - \frac{L}{2})\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\left(\frac{5}{33\eta}-\frac{10L}{11}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi({x}^s_{t-1})-\Phi^*)\notag\\
&+\Iprod{\nabla f({x}^s_{t-1}) - v_{t-1}^s}{{x}^s_t - \overline{x}^s_{t}}\notag\\
&\leq \Phi({x}^s_{t-1}) - (\frac{5}{8\eta} - \frac{L}{2})\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\left(\frac{5}{33\eta}-\frac{10L}{11}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi({x}^s_{t-1})-\Phi^*)\notag\\
&+\eta \norm{\nabla f({x}^s_{t-1}) - v_{t-1}^s}^2\label{eq60}
\end{align}
In the same way as \eqref{eq18} and \eqref{eq19}, \eqref{eq59} uses Young's inequality \eqref{eq20} (choose $\alpha = 3$) and \eqref{eq60} follows from Lemma \ref{}.

Now, we take expectations for \eqref{eq60} and then plug the variance bound \eqref{eq29} into it to obtain
\begin{align}
\E&[\Phi({x}^s_t)]\notag\\ 
&\leq \E\left[\Phi({x}^s_{t-1}) - (\frac{5}{8\eta} - \frac{L}{2})\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\left(\frac{5}{33\eta}-\frac{10L}{11}\right) \norm{\overline{x}^s_t - {x}^s_{t-1}}^2-\frac{2\eta\mu}{11}(\Phi({x}^s_{t-1})-\Phi^*)\notag\right.\\
&\left.+\frac{\eta L^2}{b}\norm{{x}^s_{t-1}-\tilde{x}^{s-1}}^2+\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\notag\\ 
&= \E\left[\Phi({x}^s_{t-1}) - \frac{13 L}{4}\norm{{x}^s_{t}-{x}^s_{t-1}}^2-\frac{\mu}{33L}(\Phi({x}^s_{t-1})-\Phi^*)+\frac{L}{6b}\norm{{x}^s_{t-1}-\tilde{x}^{s-1}}^2+\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq61}\\
&\leq \E\left[\Phi({x}^s_{t-1}) - \frac{13 L}{8t}\norm{{x}^s_{t}-\tilde{x}^{s-1}}^2-\frac{\mu}{33L}(\Phi({x}^s_{t-1})-\Phi^*)+\left(\frac{L}{6b}+\frac{13L}{8t-4}\right)\norm{{x}^s_{t-1}-\tilde{x}^{s-1}}^2+\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\label{eq62}
\end{align}
where \eqref{eq61} uses $\eta = \frac{1}{6L}$, and \eqref{eq62} uses Young's inequality by choosing $\alpha = 2t-1$.

Now, according to \eqref{eq62}, we obtain the following key inequality 
\begin{align}
\E&[\Phi({x}^s_t)-\Phi^*]\\ 
&\leq \E\left[(1-\frac{\mu}{33L})(\Phi({x}^s_{t-1})-\Phi^*) - \frac{13 L}{8t}\norm{{x}^s_{t}-\tilde{x}^{s-1}}^2+\left(\frac{L}{6b}+\frac{13L}{8t-4}\right)\norm{{x}^s_{t-1}-\tilde{x}^{s-1}}^2+\frac{I\{B < n\}\eta \sigma ^2}{B}\right]\notag\\
\end{align}
The remaining proof is exactly the same as our proof in Appendix B.1 from \eqref{eq46} to the end.
\section{Strongly Convex with Momentum Acceleration}
\begin{algorithm}\label{APGconvex-Algo}
\caption{ZO-PROXSVRG for convex Optimization}\begin{algorithmic}[1]
\State\Input initial point $x_0$, batch size $B$, minibatch size $b$, epoch length $m$, step size $\eta$
\State\Initialize $\tilde{x}^0 = x_0$
\For{ $s=1,2,\ldots, S$ }
\State $x_0^s = {x}_m^{s-1}$
\State $\hat{g}^s = \frac{1}{B} \sum_{j\in I_B} \hat{\nabla} f_j (\widetilde{x}^{s-1})$
\For{ $t=1,2,\ldots, m$ }
\State $y_{t-1} = \theta x_{t-1}^s+(1-\theta)\widetilde{x}^{s-1}$
\State ${v}_{t-1}^s = \frac{1}{b} \sum_{i\in I_b}\left(\hat{\nabla} f_{i}(y_{t-1})-\hat{\nabla} f_{i}(\tilde{x}^{s-1})\right)+\hat{g}^s$
\State $x_{t}^s= \Po_{\eta h}(x_{t-1}^s - \eta \hat{v}_{t-1}^s)$
\EndFor
\State $\tilde{x}^s=\frac{\theta}{m}\sum_{j=1}^{m}x_j^s + (1-\theta)\tilde{x}^{s-1}$ 
 \EndFor
 \State\Output ${\tilde{x}}_{S}$
\end{algorithmic}
\end{algorithm}
\end{document}

