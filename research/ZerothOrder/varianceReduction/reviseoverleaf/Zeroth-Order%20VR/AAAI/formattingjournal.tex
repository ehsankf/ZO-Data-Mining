\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai20}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage[hyphens]{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (2019 Formatting Instructions for Authors Using LaTeX)
/Author (AAAI Press Staff)}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm,amsmath}
\usepackage{mathtools}
\usepackage{newtxtext,newtxmath}
\let\openbox\relax
\usepackage{amsthm}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage{subfig}
\usepackage[dvipsnames]{xcolor}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\Po}{\text{Prox}}
\newcommand*{\Am}{\text{argmin}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\VRG}{\,\tilde{\nabla}_k^s}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\Iprod}[2]{\left\langle #1,#2\right\rangle}
\newcommand\myeq[2]{\mathrel{\stackrel{{{#1}}}{#2}}}
\newcommand{\abs}[1]{\left|#1\right|}

\renewcommand{\algorithmicrequire}{
\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\Initialize}{\textbf{Initialize:}{\,}}
\newcommand{\Input}{\textbf{Input:}{\,}}
\newcommand{\Output}{\textbf{Output:}{\,}}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{question}[theorem]{Question}
\newtheorem{corollary}[theorem]{Corollary} 
\newtheorem{definition}{Definition}
\newtheorem{statement}[theorem]{Statement}
\newtheorem{notation}[theorem]{Notation} 
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\setcounter{secnumdepth}{0}  
\newcommand{\keepcomment}{1}% Remove comment
\AtBeginDocument{\ifnum\keepcomment=1
  \excludecomment{comment}
\else
  \includecomment{comment}
\fi}
\allowdisplaybreaks
\includeonly{sub1}
\begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Efficient Zeroth-Order Proximal Stochastic Method for Nonconvex Nonsmooth Black-Box Learning}
\author{
Paper ID: 5869
}
% \author{
% Ehsan Kazemi, Deliang Fan, and Liqiang Wang\\
% Department of Computer Science,
% University of Central Florida\\
% %ehsan$\underline{~~}$kazemy@knights.ucf.edu, lwang@cs.ucf.edu\\
% }
%\iffalse
\maketitle
\begin{abstract}
Proximal gradient method has an important role
in solving nonsmooth composite optimization problems. However, in some machine learning problems related to black-box optimization models proximal gradient method could not be leveraged, where explicit gradient forms are difficult or infeasible to obtain. While First order methods are not suited for solving black-box optimization problems, zeroth-order (ZO) optimization methods can address these problems. Several varieties of proximal zeroth-order variance reduced stochastic algorithms have recently been introduced for nonconvex optimization based on the first-order techniques of stochastic variance reduction. However, all existing ZO-SVRG type  algorithms suffer from a slowdown and increase in function query complexities up  to a small-degree  polynomial  of  the  problem  size. To fill this gap, we propose a new stochastic gradient algorithm in the gradient-free regime for optimizing nonconvex, nonsmooth finite-sum problems, called ZO-PSVRG+. The analysis of ZO-PSVRG+ recovers several existing convergence results and improves their ZO oracle and proximal oracle calls. In particular, ZO-PSVRG+ yields simpler analysis  for a wide range of minibatch sizes, while the improved analysis of ZO-SVRG for smooth functions in \cite{ji2019improved} is only achieved for large minibatch sizes based on an involved parameter selection. Furthermore, we prove that ZO-PSVRG+ under Polyak-≈Åojasiewicz condition in contrast to the existent ZO-SVRG type methods obtains a global linear convergence for a wide range of minibatch sizes. Our empirical experiments on black-box binary classification and black-box adversarial attack from black-box neural networks demonstrate that the studied algorithms under our new analysis exhibit superior performance and faster convergence to a solution of high accuracy with a lower query complexity compared to state-of-the-art ZO optimization methods for nonconvex nonsmooth problems.
\end{abstract}
%\fi
\noindent 

\input{sub}
%\section{Acknowledgements}
%This work is partially supported by NSF IIS-1741431 award. 
\fontsize{9pt}{10.8pt} \selectfont
\bibliographystyle{aaai}
\bibliography{GTA}

\onecolumn % <==========================================================
%\renewcommand\theequation{A-\arabic{equation}}
%\renewcommand{\theequation}{S\arabic{equation}}
%\input{sub1}
%\bibliography{GTAsupp}

\end{document}
