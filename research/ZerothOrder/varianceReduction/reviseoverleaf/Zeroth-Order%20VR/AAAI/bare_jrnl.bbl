% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{ji2019improved}
K.~Ji, Z.~Wang, Y.~Zhou, and Y.~Liang, ``Improved zeroth-order variance reduced
  algorithms and analysis for nonconvex optimization,'' in \emph{International
  Conference on Machine Learning}, 2019, pp. 3100--3109.

\bibitem{johnson2013accelerating}
R.~Johnson and T.~Zhang, ``Accelerating stochastic gradient descent using
  predictive variance reduction,'' in \emph{Advances in neural information
  processing systems}, 2013, pp. 315--323.

\bibitem{reddi2016stochastic}
S.~J. Reddi, A.~Hefny, S.~Sra, B.~Poczos, and A.~Smola, ``Stochastic variance
  reduction for nonconvex optimization,'' in \emph{International conference on
  machine learning}, 2016, pp. 314--323.

\bibitem{nitanda2016accelerated}
A.~Nitanda, ``Accelerated stochastic gradient descent for minimizing finite
  sums,'' in \emph{Artificial Intelligence and Statistics}, 2016, pp. 195--203.

\bibitem{allen2016improved}
Z.~Allen-Zhu and Y.~Yuan, ``Improved svrg for non-strongly-convex or
  sum-of-non-convex objectives,'' in \emph{International conference on machine
  learning}, 2016, pp. 1080--1089.

\bibitem{lei2017non}
L.~Lei, C.~Ju, J.~Chen, and M.~I. Jordan, ``Non-convex finite-sum optimization
  via scsg methods,'' in \emph{Advances in Neural Information Processing
  Systems}, 2017, pp. 2348--2358.

\bibitem{wibisono2012finite}
A.~Wibisono, M.~J. Wainwright, M.~I. Jordan, and J.~C. Duchi, ``Finite sample
  convergence rates of zero-order stochastic optimization methods,'' in
  \emph{Advances in Neural Information Processing Systems}, 2012, pp.
  1439--1447.

\bibitem{sokolov2016stochastic}
A.~Sokolov, J.~Kreutzer, S.~Riezler, and C.~Lo, ``Stochastic structured
  prediction under bandit feedback,'' in \emph{Advances in Neural Information
  Processing Systems}, 2016, pp. 1489--1497.

\bibitem{nesterov2017random}
Y.~Nesterov and V.~Spokoiny, ``Random gradient-free minimization of convex
  functions,'' \emph{Foundations of Computational Mathematics}, vol.~17, no.~2,
  pp. 527--566, 2017.

\bibitem{brent2013algorithms}
R.~P. Brent, \emph{Algorithms for minimization without derivatives}.\hskip 1em
  plus 0.5em minus 0.4em\relax Courier Corporation, 2013.

\bibitem{spall2005introduction}
J.~C. Spall, \emph{Introduction to stochastic search and optimization:
  estimation, simulation, and control}.\hskip 1em plus 0.5em minus 0.4em\relax
  John Wiley \& Sons, 2005, vol.~65.

\bibitem{kurakin2016adversarial}
A.~Kurakin, I.~Goodfellow, and S.~Bengio, ``Adversarial machine learning at
  scale,'' \emph{arXiv preprint arXiv:1611.01236}, 2016.

\bibitem{papernot2017practical}
N.~Papernot, P.~McDaniel, I.~Goodfellow, S.~Jha, Z.~B. Celik, and A.~Swami,
  ``Practical black-box attacks against machine learning,'' in
  \emph{Proceedings of the 2017 ACM on Asia conference on computer and
  communications security}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2017,
  pp. 506--519.

\bibitem{chen2017zoo}
P.-Y. Chen, H.~Zhang, Y.~Sharma, J.~Yi, and C.-J. Hsieh, ``Zoo: Zeroth order
  optimization based black-box attacks to deep neural networks without training
  substitute models,'' in \emph{Proceedings of the 10th ACM Workshop on
  Artificial Intelligence and Security}.\hskip 1em plus 0.5em minus 0.4em\relax
  ACM, 2017, pp. 15--26.

\bibitem{choromanski2018structured}
K.~Choromanski, M.~Rowland, V.~Sindhwani, R.~E. Turner, and A.~Weller,
  ``Structured evolution with compact architectures for scalable policy
  optimization,'' \emph{arXiv preprint arXiv:1804.02395}, 2018.

\bibitem{taskar2005learning}
B.~Taskar, V.~Chatalbashev, D.~Koller, and C.~Guestrin, ``Learning structured
  prediction models: A large margin approach,'' in \emph{Proceedings of the
  22nd international conference on Machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax ACM, 2005, pp. 896--903.

\bibitem{chen2019bandit}
T.~Chen and G.~B. Giannakis, ``Bandit convex optimization for scalable and
  dynamic iot management,'' \emph{IEEE Internet of Things Journal}, vol.~6,
  no.~1, pp. 1276--1286, 2019.

\bibitem{liu2017zeroth}
S.~Liu, J.~Chen, P.-Y. Chen, and A.~O. Hero, ``Zeroth-order online alternating
  direction method of multipliers: Convergence analysis and applications,''
  \emph{arXiv preprint arXiv:1710.07804}, 2017.

\bibitem{fu2002optimization}
M.~C. Fu, ``Optimization for simulation: Theory vs. practice,'' \emph{INFORMS
  Journal on Computing}, vol.~14, no.~3, pp. 192--215, 2002.

\bibitem{lian2016comprehensive}
X.~Lian, H.~Zhang, C.-J. Hsieh, Y.~Huang, and J.~Liu, ``A comprehensive linear
  speedup analysis for asynchronous stochastic parallel optimization from
  zeroth-order to first-order,'' in \emph{Advances in Neural Information
  Processing Systems}, 2016, pp. 3054--3062.

\bibitem{ghadimi2016accelerated}
S.~Ghadimi and G.~Lan, ``Accelerated gradient methods for nonconvex nonlinear
  and stochastic programming,'' \emph{Mathematical Programming}, vol. 156, no.
  1-2, pp. 59--99, 2016.

\bibitem{huang2019faster}
F.~Huang, B.~Gu, Z.~Huo, S.~Chen, and H.~Huang, ``Faster gradient-free proximal
  stochastic methods for nonconvex nonsmooth optimization,'' in \emph{AAAI},
  2019.

\bibitem{liu2018zeroth}
S.~Liu, J.~Chen, P.-Y. Chen, and A.~Hero, ``Zeroth-order online alternating
  direction method of multipliers: Convergence analysis and applications,'' in
  \emph{International Conference on Artificial Intelligence and Statistics},
  2018, pp. 288--297.

\bibitem{polyak1963gradient}
B.~T. Polyak, ``Gradient methods for minimizing functionals,'' \emph{Zhurnal
  Vychislitel'noi Matematiki i Matematicheskoi Fiziki}, vol.~3, no.~4, pp.
  643--653, 1963.

\bibitem{duchi2015optimal}
J.~C. Duchi, M.~I. Jordan, M.~J. Wainwright, and A.~Wibisono, ``Optimal rates
  for zero-order convex optimization: The power of two function evaluations,''
  \emph{IEEE Transactions on Information Theory}, vol.~61, no.~5, pp.
  2788--2806, 2015.

\bibitem{agarwal2010optimal}
A.~Agarwal, O.~Dekel, and L.~Xiao, ``Optimal algorithms for online convex
  optimization with multi-point bandit feedback.'' in \emph{COLT}.\hskip 1em
  plus 0.5em minus 0.4em\relax Citeseer, 2010, pp. 28--40.

\bibitem{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan, ``Stochastic first-and zeroth-order methods for
  nonconvex stochastic programming,'' \emph{SIAM Journal on Optimization},
  vol.~23, no.~4, pp. 2341--2368, 2013.

\bibitem{nesterov2011random}
Y.~Nesterov and V.~Spokoiny, ``Random gradient-free minimization of convex
  functions,'' Universit{\'e} catholique de Louvain, Center for Operations
  Research and Econometrics (CORE), Tech. Rep., 2011.

\bibitem{liu2018stochastic}
L.~Liu, M.~Cheng, C.-J. Hsieh, and D.~Tao, ``Stochastic zeroth-order
  optimization via variance reduction method,'' \emph{arXiv preprint
  arXiv:1805.11811}, 2018.

\bibitem{reddi2016proximal}
S.~J. Reddi, S.~Sra, B.~P{\'o}czos, and A.~J. Smola, ``Proximal stochastic
  methods for nonsmooth nonconvex finite-sum optimization,'' in \emph{Advances
  in Neural Information Processing Systems}, 2016, pp. 1145--1153.

\bibitem{gu2018inexact}
B.~Gu, D.~Wang, Z.~Huo, and H.~Huang, ``Inexact proximal gradient methods for
  non-convex and non-smooth optimization,'' in \emph{Thirty-Second AAAI
  Conference on Artificial Intelligence}, 2018.

\bibitem{gu2018faster}
B.~Gu, Z.~Huo, C.~Deng, and H.~Huang, ``Faster derivative-free stochastic
  algorithm for shared memory machines,'' in \emph{International Conference on
  Machine Learning}, 2018, pp. 1807--1816.

\bibitem{balasubramanian2018zeroth}
K.~Balasubramanian and S.~Ghadimi, ``Zeroth-order (non)-convex stochastic
  optimization via conditional gradient and gradient updates,'' in
  \emph{Advances in Neural Information Processing Systems}, 2018, pp.
  3455--3464.

\bibitem{sahu2018towards}
A.~K. Sahu, M.~Zaheer, and S.~Kar, ``Towards gradient free and projection free
  stochastic optimization,'' \emph{arXiv preprint arXiv:1810.03233}, 2018.

\bibitem{chen2019zo}
X.~Chen, S.~Liu, K.~Xu, X.~Li, X.~Lin, M.~Hong, and D.~Cox, ``Zo-adamm:
  Zeroth-order adaptive momentum method for black-box optimization,'' in
  \emph{Advances in Neural Information Processing Systems}, 2019, pp.
  7202--7213.

\bibitem{gao2018information}
X.~Gao, B.~Jiang, and S.~Zhang, ``On the information-adaptive variants of the
  admm: an iteration complexity perspective,'' \emph{Journal of Scientific
  Computing}, vol.~76, no.~1, pp. 327--363, 2018.

\bibitem{flaxman2005online}
A.~D. Flaxman, A.~T. Kalai, and H.~B. McMahan, ``Online convex optimization in
  the bandit setting: gradient descent without a gradient,'' in
  \emph{Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete
  algorithms}.\hskip 1em plus 0.5em minus 0.4em\relax Society for Industrial
  and Applied Mathematics, 2005, pp. 385--394.

\bibitem{shamir2017optimal}
O.~Shamir, ``An optimal algorithm for bandit and zero-order convex optimization
  with two-point feedback.'' \emph{Journal of Machine Learning Research},
  vol.~18, no.~52, pp. 1--11, 2017.

\bibitem{parikh2014proximal}
N.~Parikh, S.~Boyd \emph{et~al.}, ``Proximal algorithms,'' \emph{Foundations
  and Trends{\textregistered} in Optimization}, vol.~1, no.~3, pp. 127--239,
  2014.

\bibitem{xiao2014proximal}
L.~Xiao and T.~Zhang, ``A proximal stochastic gradient method with progressive
  variance reduction,'' \emph{SIAM Journal on Optimization}, vol.~24, no.~4,
  pp. 2057--2075, 2014.

\bibitem{li2018simple}
Z.~Li and J.~Li, ``A simple proximal stochastic gradient method for nonsmooth
  nonconvex optimization,'' in \emph{Advances in Neural Information Processing
  Systems}, 2018, pp. 5564--5574.

\bibitem{hajinezhad2019zone}
D.~Hajinezhad, M.~Hong, and A.~Garcia, ``Zone: Zeroth order nonconvex
  multi-agent optimization over networks,'' \emph{IEEE Transactions on
  Automatic Control}, 2019.

\bibitem{karimi2016linear}
H.~Karimi, J.~Nutini, and M.~Schmidt, ``Linear convergence of gradient and
  proximal-gradient methods under the polyak-{\l}ojasiewicz condition,'' in
  \emph{Joint European Conference on Machine Learning and Knowledge Discovery
  in Databases}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2016, pp.
  795--811.

\end{thebibliography}
