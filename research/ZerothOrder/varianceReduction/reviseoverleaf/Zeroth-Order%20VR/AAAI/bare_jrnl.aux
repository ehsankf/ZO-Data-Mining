\relax 
\citation{johnson2013accelerating}
\citation{reddi2016stochastic}
\citation{nitanda2016accelerated}
\citation{allen2016improved}
\citation{lei2017non}
\citation{wibisono2012finite}
\citation{sokolov2016stochastic}
\citation{nesterov2017random}
\citation{brent2013algorithms}
\citation{spall2005introduction}
\citation{kurakin2016adversarial}
\citation{papernot2017practical}
\citation{chen2017zoo}
\citation{choromanski2018structured}
\citation{taskar2005learning}
\citation{chen2019bandit}
\citation{liu2017zeroth}
\citation{fu2002optimization}
\citation{lian2016comprehensive}
\citation{ghadimi2016accelerated}
\citation{huang2019faster}
\citation{ghadimi2016accelerated}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\relax Introduction}{1}}
\newlabel{problem}{{1}{1}}
\citation{ji2019improved}
\citation{huang2019faster}
\citation{nesterov2017random}
\citation{ghadimi2016accelerated}
\citation{liu2018zeroth}
\citation{ji2019improved}
\citation{liu2018zeroth}
\citation{ghadimi2016accelerated}
\citation{huang2019faster}
\citation{ghadimi2016accelerated}
\citation{huang2019faster}
\citation{ji2019improved}
\citation{liu2018zeroth}
\citation{ji2019improved}
\citation{polyak1963gradient}
\citation{duchi2015optimal}
\citation{ji2019improved}
\@writefile{toc}{\contentsline {section}{\relax Main contributions}{2}}
\citation{agarwal2010optimal}
\citation{nesterov2017random}
\citation{chen2017zoo}
\citation{liu2018zeroth}
\citation{ghadimi2013stochastic}
\citation{nesterov2011random}
\citation{liu2018stochastic}
\citation{liu2018zeroth}
\citation{ji2019improved}
\citation{liu2018zeroth}
\citation{huang2019faster}
\citation{reddi2016proximal}
\citation{ghadimi2016accelerated}
\citation{liu2018stochastic}
\citation{gu2018inexact}
\citation{lian2016comprehensive}
\citation{gu2018faster}
\citation{lian2016comprehensive}
\citation{gu2018faster}
\citation{balasubramanian2018zeroth}
\citation{sahu2018towards}
\citation{liu2017zeroth}
\citation{chen2019zo}
\citation{nesterov2017random}
\citation{ghadimi2016accelerated}
\citation{liu2018zeroth}
\citation{ji2019improved}
\citation{huang2019faster}
\citation{huang2019faster}
\citation{nesterov2017random}
\citation{gao2018information}
\citation{flaxman2005online}
\citation{shamir2017optimal}
\citation{gao2018information}
\citation{lian2016comprehensive}
\citation{gu2018inexact}
\citation{gu2018faster}
\citation{liu2018zeroth}
\citation{ghadimi2013stochastic}
\citation{reddi2016stochastic}
\citation{lei2017non}
\citation{liu2018zeroth}
\citation{ghadimi2016accelerated}
\citation{reddi2016proximal}
\citation{huang2019faster}
\@writefile{toc}{\contentsline {section}{\relax Related Works}{3}}
\@writefile{toc}{\contentsline {section}{\relax Preliminaries}{3}}
\newlabel{gradestrand}{{2}{3}}
\newlabel{gradestcoord}{{3}{3}}
\newlabel{po-operator}{{5}{3}}
\citation{ghadimi2016accelerated}
\citation{reddi2016proximal}
\citation{parikh2014proximal}
\citation{parikh2014proximal}
\citation{xiao2014proximal}
\citation{reddi2016proximal}
\citation{li2018simple}
\citation{reddi2016proximal}
\citation{li2018simple}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Summary of convergence rate and function query complexity of various SZO algorithms. S: Smooth, NS: Nonsmooth, NC: Nonconvex, C: Convex, SC: Strong Convexity, and PL: Polyak-\IeC {\L }ojasiewicz Condition. $b$ denotes the minibatch size, $m$ denotes the epoch size, $\lambda $ is the constant in PL condition \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 18\hbox {}\unskip \@@italiccorr )}} and $s_n = \qopname  \relax m{min}\{n, \frac  {1}{\epsilon }\}$. *: The single-minibatch version.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table-compare}{{I}{4}}
\@writefile{toc}{\contentsline {section}{\relax ZO Proximal Stochastic Method (ZO-PSVRG+)}{4}}
\newlabel{grad-fo}{{7}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Zeroth-Order Proximal Stochastic Method\relax }}{4}}
\newlabel{APGnonconvex-Algo}{{1}{4}}
\newlabel{zo-grad-fo}{{8}{4}}
\newlabel{zo-grad-fo-rand}{{9}{4}}
\citation{ghadimi2016accelerated}
\citation{nesterov2017random}
\citation{liu2018zeroth}
\citation{lian2016comprehensive}
\citation{liu2018stochastic}
\citation{liu2018zeroth}
\citation{liu2017zeroth}
\citation{hajinezhad2019zone}
\citation{reddi2016proximal}
\@writefile{toc}{\contentsline {section}{\relax Convergence Analysis}{5}}
\newlabel{Lip-Zoo}{{1}{5}}
\newlabel{Var-Zoo}{{2}{5}}
\newlabel{var-estimate-lem}{{1}{5}}
\newlabel{RandSGE-var-estimate-lem}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\relax Analysis for ZO-PSVRG+}{5}}
\newlabel{noncon-zoo-coord}{{3}{5}}
\newlabel{corr11}{{4}{5}}
\newlabel{SZO-call-nocon}{{12}{5}}
\citation{ji2019improved}
\citation{polyak1963gradient}
\citation{karimi2016linear}
\citation{li2018simple}
\citation{ji2019improved}
\newlabel{SZO-call-par-nocon}{{13}{6}}
\@writefile{toc}{\contentsline {subsection}{\relax Analysis for ZO-PSVRG+ (RandSGE)}{6}}
\newlabel{noncon-zoo-rand}{{5}{6}}
\newlabel{eq-noncon-zoo-rand}{{14}{6}}
\newlabel{corr11-rand}{{6}{6}}
\newlabel{SZO-call-nocon-rand}{{15}{6}}
\newlabel{SZO-call-par-nocon-rand}{{16}{6}}
\@writefile{toc}{\contentsline {section}{\relax Convergence Under PL Condition}{6}}
\newlabel{zo-pl-cond}{{18}{6}}
\@writefile{toc}{\contentsline {subsection}{\relax ZO-PSVRG+ Under PL Condition}{6}}
\citation{duchi2015optimal}
\citation{nesterov2017random}
\citation{liu2018zeroth}
\citation{reddi2016stochastic}
\citation{ji2019improved}
\citation{reddi2016proximal}
\citation{ji2019improved}
\citation{gu2018faster}
\citation{ghadimi2016accelerated}
\newlabel{PL-Zoo}{{7}{7}}
\newlabel{PL-eq-error}{{19}{7}}
\newlabel{PL-Zo-Cor}{{8}{7}}
\newlabel{eq54}{{20}{7}}
\@writefile{toc}{\contentsline {subsection}{\relax ZO-PSVRG+ (RandSGE) Under PL Condition}{7}}
\newlabel{PL-Zoo-rand}{{9}{7}}
\newlabel{PL-eq-error-rand}{{21}{7}}
\newlabel{PL-Zo-Cor-rand}{{10}{7}}
\citation{chen2017zoo}
\citation{liu2018zeroth}
\@writefile{toc}{\contentsline {section}{\relax Experimental Results}{8}}
\@writefile{toc}{\contentsline {subsection}{\relax Black-Box Binary Classification}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Summary of training datasets.\relax }}{8}}
\newlabel{metadata}{{II}{8}}
\@writefile{toc}{\contentsline {subsection}{\relax Adversarial Attacks on Black-Box DNNs}{8}}
\newlabel{mnist-attack-loss}{{22}{8}}
\citation{ji2019improved}
\bibstyle{IEEEtran}
\bibdata{GTA}
\bibcite{ji2019improved}{1}
\bibcite{johnson2013accelerating}{2}
\bibcite{reddi2016stochastic}{3}
\bibcite{nitanda2016accelerated}{4}
\bibcite{allen2016improved}{5}
\bibcite{lei2017non}{6}
\bibcite{wibisono2012finite}{7}
\bibcite{sokolov2016stochastic}{8}
\bibcite{nesterov2017random}{9}
\bibcite{brent2013algorithms}{10}
\bibcite{spall2005introduction}{11}
\bibcite{kurakin2016adversarial}{12}
\bibcite{papernot2017practical}{13}
\bibcite{chen2017zoo}{14}
\bibcite{choromanski2018structured}{15}
\bibcite{taskar2005learning}{16}
\bibcite{chen2019bandit}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of different zeroth-order algorithms for logistic regression loss residual $f(x) - f(x^*)$ versus the number of epochs (top) and ZO queries (bottom)\relax }}{9}}
\newlabel{binary-fig}{{1}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ijcnn}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {comparison on covtype}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {w8a}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {mnist}}}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of different zeroth-order algorithms for generating black-box adversarial examples from a black-box DNN\relax }}{9}}
\newlabel{attack-fig}{{2}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Loss vs iterations: $n = 10$}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Loss vs queries: $n = 10$}}}{9}}
\@writefile{toc}{\contentsline {section}{\relax Conclusion}{9}}
\@writefile{toc}{\contentsline {section}{References}{9}}
\bibcite{liu2017zeroth}{18}
\bibcite{fu2002optimization}{19}
\bibcite{lian2016comprehensive}{20}
\bibcite{ghadimi2016accelerated}{21}
\bibcite{huang2019faster}{22}
\bibcite{liu2018zeroth}{23}
\bibcite{polyak1963gradient}{24}
\bibcite{duchi2015optimal}{25}
\bibcite{agarwal2010optimal}{26}
\bibcite{ghadimi2013stochastic}{27}
\bibcite{nesterov2011random}{28}
\bibcite{liu2018stochastic}{29}
\bibcite{reddi2016proximal}{30}
\bibcite{gu2018inexact}{31}
\bibcite{gu2018faster}{32}
\bibcite{balasubramanian2018zeroth}{33}
\bibcite{sahu2018towards}{34}
\bibcite{chen2019zo}{35}
\bibcite{gao2018information}{36}
\bibcite{flaxman2005online}{37}
\bibcite{shamir2017optimal}{38}
\bibcite{parikh2014proximal}{39}
\bibcite{xiao2014proximal}{40}
\bibcite{li2018simple}{41}
\bibcite{hajinezhad2019zone}{42}
\bibcite{karimi2016linear}{43}
