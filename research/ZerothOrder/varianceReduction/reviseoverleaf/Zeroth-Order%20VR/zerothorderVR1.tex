\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage{mathptmx}  
\usepackage[scaled=.92]{helvet}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage[round]{natbib}  
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}  %%check
\usepackage[dvipsnames]{xcolor}
%%%Algorithms
\usepackage[noend]{algpseudocode}
\usepackage{algorithm,amsmath}
\usepackage{mathtools}
\usepackage{newtxtext,newtxmath}
\usepackage[tight,footnotesize]{subfigure}

\newcommand*{\R}{\mathbb{R}}
\newcommand*{\G}{\mathcal{G}}
\newcommand*{\Po}{\text{Prox}}
\newcommand*{\Am}{\text{argmin}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\VRG}{\,\tilde{\nabla}_k^s}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\Iprod}[2]{\left\langle #1,#2\right\rangle}
\newcommand\myeq[2]{\mathrel{\stackrel{\makebox[0pt]{\mbox{#1}}}{#2}}}

\renewcommand{\algorithmicrequire}{
\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\Initialize}{\textbf{Initialize:}{\,}}
\newcommand{\Input}{\textbf{Input:}{\,}}
\newcommand{\Output}{\textbf{Output:}{\,}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{question}[theorem]{Question}
\newtheorem{corollary}[theorem]{Corollary} 
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{statement}[theorem]{Statement}
\newtheorem{notation}[theorem]{Notation} 
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\allowdisplaybreaks

\title{An Acceleration of Derivative-Free Proximal Stochastic Gradient Method for Nonconvex Nonsmooth Optimization}
\date{March 2018}

\begin{document}

\maketitle
\begin{abstract}
We provide a simpler analysis with better dependence on dimension of problem $d$. 
\end{abstract}

\section{Introduction}
\subsection{what we want to do}
{\color{Green}\label{problem}
{\color{Violet}
In this paper, we consider nonsmooth nonconvex finite-sum optimization problems of the form
}
\begin{equation}
\min_{x\in\R^d} F(x) =  f(x) + h(x),\,\,\,f(x):=\frac{1}{n}\sum_{i=1}^n f_i(x)
\end{equation}
where each $f_i(x)$ is {\color{Violet} possibly} nonconvex and smooth loss function, and $h(x)$ is a nonsmooth but convex structure regularizer such as $l_1$-norm regularization. }
{\color{Brown}
The generic form \eqref{problem} encompasses
many machine learning problems, ranging from generalized linear models to neural networks 
{\color{Violet} and from convex optimization such as Lasso, SVM to highly nonconvex problem such as optimizing deep neural networks.}


We will study the design and analysis of {\color{Green} a class of} variance reduced and faster converging stochastic zeroth-order (SZO) optimization methods for \eqref{problem}. To reduce the variance accelerate SZO optimization, one can draw
motivations from similar ideas in the first-order regime. The stochastic variance reduced gradient
(SVRG) is a commonly-used, effective first-order approach to reduce the variance \cite{johnson2013accelerating,reddi2016stochastic,nitanda2016accelerated,allen2016improved,lei2017non}. Due to
the variance
reduction, it improves the convergence rate of stochastic gradient descent (SGD) complexity from
$O(1/\epsilon^2)$ to $O(1/{\epsilon})$.


}

\subsection{Background in research}
{\color{Green}
In recent research \cite{beck2009fast},
the accelerated proximal gradient (PG) methods are proposed to solve convex problems by using the Nesterov's accelerated technique. After that, \cite{li2015accelerated} presented a class of accelerated PG methods for nonconvex optimization.  To solve the big data problems, the incremental or stochastic PG methods \cite{bertsekas2011incremental,xiao2014proximal} were developed for large-scale convex optimization. 
}
{\color{Violet}
There has been extensive research when $f(x)$ is convex (see e.g., {  
\cite{nesterov2013gradient}}, \cite{xiao2014proximal,defazio2014saga,lan2017optimal,allen2017katyusha}. In particular, if each $f_i$ is strongly-convex, \cite{xiao2014proximal} proposed the Prox-SVRG algorithm {\color{green} for large-scale problems}, which achieves a linear convergence rate, based on the well-known variance reduction technique SVRG
developed in \cite{johnson2013accelerating}. In recent years, due to the increasing popularity of deep learning, the nonconvex case has attracted significant attention. For a general nonsmooth nonconvex case, the research is still somewhat limited. {\color{Green} Correspondingly, \cite{ghadimi2016accelerated,reddi2016proximal} proposed the stochastic PG methods for large-scale nonconvex optimization.}
Very recently, \cite{li2018simple} proposed an algorithm generalized the results by \cite{reddi2016proximal} and improved the stochastic gradient complexity. 
}
\subsection{Reason to use zeroth--order techniques}
{\color{DarkOrchid}
The main issue for these accelerated algorithms is that most of their algorithm designs involve computing first-order information. 
}
{\color{Green}
{\color{RubineRed}
However, there exist situations where the first-order gradient information is computationally infeasible, expensive, or impossible,
while the zeroth-order functional information can be easily obtained
}

{\color{RubineRed}
 For example, in online auctions and advertisement
selections, only function values are revealed as feedbacks for algorithms \cite{wibisono2012finite}. In stochastic structured predictions, explicit differentiations may be difficult to perform while the functional evaluations of predicted structures are easily obtained \cite{sokolov2016stochastic}. }

 Even worse, in bandit \cite{shamir2017optimal} and black-box learning \cite{chen2017zoo} problems, only the objective function values are available (the explicit gradients cannot be calculated). 
Clearly, the above
PG methods will fail in dealing with these scenarios. The gradient-free (zeroth-order) optimization method \cite{nesterov2017random} is a promising choice to address these problems because it only uses the function values in optimization process. 
{\color{RubineRed} The optimization problem of equation \eqref{problem} in such situations is referred to stochastic proximal zeroth-order optimization.}
{\color{Brown} These algorithms achieve gradient-free optimization by approximating the full gradient via gradient estimators based on only the function values \cite{brent2013algorithms,spall2005introduction}.} 
}
{\color{Brown}
Hence, SZO optimization is increasingly embraced for solving machine learning
problems where explicit expressions of the gradients are difficult or infeasible to obtain. Recent examples have shown zeroth-order (ZO) based generation of prediction-evasive, black-box adversarial attacks on deep neural networks (DNNs) as effective as state-of-the-art white-box attacks, despite leveraging only the inputs and outputs of the targeted DNN \cite{papernot2017practical,madry2017towards,chen2017zoo} {\color{Green} \cite{conn2009introduction}}. Additional classes of applications include network control and management with time-varying constraints and limited computation capacity \cite{chen2019bandit,liu2017zeroth}, and parameter inference of black-box systems \cite{fu2002optimization,lian2016comprehensive}. 
}

{\color{DarkOrchid}
The main issue for those accelerated algorithms is that most of their algorithm designs (e.g., (Allen-Zhu, 2017)
and (Hien et al., 2017)) involve tracking at least two highly
correlated coupling vectors 2 (in the inner loop). This kind
of algorithm structure prevents us from deriving efficient
(lock-free) asynchronous sparse variants for those algorithms.
}
\subsection{Problem with existing methods}
{\color{Brown}
Although many SZO algorithms have recently been developed and analyzed \cite{liu2017zeroth,flaxman2005online,shamir2013complexity,agarwal2010optimal,nesterov2017random,duchi2015optimal,shamir2017optimal,dvurechensky2018accelerated,wang2017stochastic}, they often
suffer from the high variances of SZO gradient estimates, and in turn, hampered convergence rates.
{\color{RubineRed}
Thus, a useful technique to accelerate the convergence of SZO is by leveraging variance reduction method.
}

 In addition, these algorithms are mainly designed for convex settings, which limits their applicability in a wide range of (nonconvex) machine learning problems.
}

{\color{Green}  Until now, there are few zeroth-order stochastic methods
for solving the problem \eqref{problem} e.g., \cite{ghadimi2016accelerated} and \cite{huang2019faster}. Specifically, \cite{ghadimi2016accelerated} have proposed a randomized stochastic projected gradient-free method (RSPGF),
i.e., a zeroth-order proximal stochastic gradient method. However, due to the large variance of zeroth-order estimated gradient generated from randomly selecting the sample and the direction of derivative, the RSPGE only has a iteration complexity of  $O(\frac{d}{\epsilon^2})$ based on two function evaluations, which is significantly slower than $O(\frac{d}{{\epsilon}})$,
the best known convergence rate of the zeroth-order stochastic algorithm. To accelerate the RSPGF algorithm, the zeroth-order variance reduction methods, i.e., ZO-SVRG \cite{liu2018zeroth} and ZO-ProxSVRG/SAGA \cite{huang2019faster} with the iteration
complexity of $O(\frac{d}{{\epsilon}})$, were introduced to reduce the variance of estimated stochastic gradient. 
{\color{RubineRed}
A key concern in the development of iterative stochastic zeroth-oder algorithms for solving  \eqref{problem} is the order of the necessary number of functional evaluations, namely SZO calls or iteration complexity.
}
{\color{Brown}
 Without a
careful treatment, the dimension dependent factor in the convergence rate (i.e., $d$) could be a critical factor affecting
the optimization performance. To mitigate this factor, we propose an accelerated ZO proximal  variants, utilizing reduced variance gradient estimators. These yield a faster iteration complexity towards $O(\sqrt{d}/\epsilon )$, which is, to our knowledge, the best iteration complexity bound so far achieved for ZO stochastic optimization with nonconvex structure.
{\color{RubineRed} This indicates an improvement over existing results up to a factor of ${\sqrt{d}}$.}

}
}

\subsection{Compare with other methods}
{\color{Violet}
We list the results of proposed algorithms and other related ones in Table \ref{table-compare} and Figure \ref{}.
}
{\color{Brown}
In Table \ref{table-compare}, we summarize the convergence rates
and the function query complexities of ZO-PSVRG+. 
For comparison, we also present the results of ZO-SGD \cite{ghadimi2013stochastic}, ZO-SVRC \cite{gu2016zeroth}, ZO-SVRG-Coord \cite{liu2018zeroth} and ZO-ProxSVRG/SAGA \cite{huang2019faster}. Table \ref{table-compare} shows
that RGF has the highest query complexity and nevertheless has the worst convergence rate. ZO-SVRG-coord
and ZO-ProxSVRG/SAGA yield the best convergence rate in the cost of high query complexity. By contrast, ZO-PSVRG+ could achieve better trade-offs between the
convergence rate and the query complexity.
}



\section{Main Challenge}
{\color{Brown}
Although ProxSVRG has shown a great promise, applying similar ideas to ZO optimization is not a trivial
task. 
{\color{DarkOrchid}
Zeroth-order method are applicable for problem that the first-order gradient is not available, but, due to the existence of
perturbation, gradient estimation has complex coupling structures, which makes
them hard to be extended to more settings. 
}
The other main challenge is due to the fact that ProxSVRG relies upon the assumption that a stochastic gradient is an unbiased estimate of the true batch/full gradient, which unfortunately does not hold in the ZO case. Therefore, it is an open question whether the ZO stochastic variance reduced gradient could enable faster convergence of ZO algorithms. In this paper, we attempt to fill the gap between
SZO optimization and ProxSVRG and improve the efficiency of exiting ZO variance reduced methods for problem \eqref{problem}.
}


\section{Main contributions}

{\color{Green}

 
{\color{Brown}
We propose and evaluate a novel SZO algorithm for nonconvex nonsmooth stochastic optimization,
ZO-PSVRG+, which integrates ProxSVRG with ZO gradient estimators. We show that compared to
ProxSVRG, ZO-PSVRG+ achieves a similar convergence with SZO complexity of $O(\sqrt{d}/\epsilon)$. Our work offers a comprehensive study on how ZO gradient estimators affect ProxSVRG on both iteration complexity (i.e., convergence rate) and function query complexity. Note that our considered problem does
not necessarily satisfy bounded gradients assumption in \cite{ghadimi2016accelerated,huang2019faster}.
} 
{\color{Violet}
Our main technical contribution lies in the new convergence analysis of ZO-PSVRG+,
which has notable difference from that of ZO-SVRG \cite{liu2018zeroth} and ZO-ProxSVRG \cite{huang2019faster}. The convergence results are stated in terms of the number of stochastic zeroth-order (SZO) calls and proximal
oracle (PO) calls. 
} 
 
In particular, our algorithm has iteration complexity $O(\frac{\sqrt{d}}{{\epsilon}})$ compared with $O(\frac{d}{\epsilon^2})$ of RGF \cite{ghadimi2016accelerated}  and $O(\frac{d}{\epsilon})$ of ZO-ProxSVRG/SAGA  \cite{huang2019faster} (the existing variance-reduce SZO proximal algorithm for solving nonconvex nonsmooth problems).  
{\color{RubineRed}In other words, the proposal expectational results have better dependence on
$d$ compared to the existing variance-reduced SZO methods and {\color{Brown}
 can strike a balance between iteration complexity and function query complexity.}} 
 

{\color{Violet}
We would like to highlight the following results yielded by our new analysis:

1) ZO-PSVRG+ is $\frac{n\sqrt{b}}{\sqrt{d}}$ (resp. $\frac{\sqrt{b}}{\sqrt{d}\epsilon}$) times faster than RSPGF in terms of the number of SZO calls when $bm\leq 1/\epsilon$ (resp. $bm \leq n$ ){\color{red} , and $n/m\sqrt{bd}\epsilon$ times faster than ProxGD when $bm > 1/\epsilon$ (resp. $bm > n$ )}. Note that the number of PO calls equal to $O(1/\epsilon)$  and $O(1/\epsilon^2)$ for ZO-PSVRG+ and RSPGF, respectively. Obviously, for any super constant $b$, ZO-PSVRG+ is strictly better than RSPGF. ZO-PSVRG+ also matches the best result achieved by ZO-ProxSVRG {\color{red} at $b = n^{2/3}$ for $m = \sqrt{b}$, and it is strictly better for smaller $b$ (using less PO calls).} See Figure \ref{} for an overview.
}

{\color{Violet} 2) Assuming that the variance of the stochastic zeroth-order gradient is bounded (see Assumption \ref{}), i.e. online/stochastic setting,
ZO-PSVRG+ generalizes the best result achieved by ZO-SVRG-Coord, recently proposed by \cite{liu2018zeroth} for the smooth nonconvex case, i.e., $h(x) = 0$ in form \eqref{problem} (see Table \ref{table-compare}, the 4th row). ZO-PSVRG+ is more straightforward than ZO-SVRG-Coord proposed by \cite{liu2018zeroth} very recently, and yields simpler proof. Our results also improve the results of ZO-SVRG-Coord  in terms of the number of SZO calls, by a factor of $\sqrt{d}$. {\color{Brown}
Hence, we partially answer the open question if the dependence on the dimension $d$ for the convergence analysis proposed in \cite{liu2018zeroth} is optimal. 
}
}



}
{\color{Violet}
We also note that RGF \cite{} and ZO-ProxSVRG \cite{liu2018zeroth} achieved their best convergence
results with $b = 1$ and $b = n^{2/3}$ respectively, while ZO-PSVRG+ achieves the best result with $b = 1/\epsilon^{2/3}$ (see
Figure \ref{}), which is a moderate minibatch size (which is not too small for parallelism/vectorization and not too
large for better generalization). In our experiments, the best $b$ for ZO-PSVRG+ and ZO-ProxSVRG in the MNIST
experiments is 4096 and 256, respectively (see the second row of Figure \ref{}).

3) For the nonconvex functions satisfying Polyak-Łojasiewicz condition \cite{polyak1963gradient}, we prove that ZO-PSVRG+
achieves a global linear convergence rate similar to first-order ProxSVRG. Thus, ZO-PSVRG+ can automatically switch to
the faster linear convergence in some regions. This generalizes the results of MD \cite{duchi2015optimal} and  achieves linear convergence compared with sublinear rate of MD, (see Table \ref{table-compare}). Also see the remarks after Theorem \ref{zo-pl-cond} for more details. {\color{RubineRed}
To the best of
our knowledge, this is the first paper that leverages the PL condition for improving the convergence of SZO. It is also notable that the convergence rate achieved in this case can be as good as first-order ProxSVRG.
}
}
{\color{DarkOrchid}
This method achieves an improved oracle complexity by a factor of $\sqrt{d}$ versus RGF for nonsmooth convex problems.

}

{\color{Brown}
 To demonstrate the flexibility of our approach in managing trade-off between the rate of convergence and the number of SZO calls, we conduct an
empirical evaluation of our proposed algorithms and other state-of-the-art algorithms on two diverse
applications: black-box chemical material classification and generation of universal adversarial
perturbations from black-box deep neural network models. Extensive experimental results and
theoretical analysis validate the effectiveness of our approaches.
}


\section{Related Works}
{\color{Green}
Gradient-free (zeroth-order) methods have been effectively used to solve many machine learning problems, where the explicit gradient is difficult or infeasible to obtain, and have also been widely studied. 
{\color{Brown}
In ZO algorithms, a full gradient is typically approximated using either a one-point or a two-point gradient estimator, where the former acquires a gradient estimate $\hat{\nabla} f(x)$ by querying $f$ at a single random location close to $x$ \cite{flaxman2005online,shamir2013complexity}, and the latter computes a finite difference using two random function queries \cite{agarwal2010optimal,nesterov2017random}. In this paper, we focus on the two-point gradient estimator since it has a lower variance and thus improves the complexity bounds of ZO algorithms.
}

Nesterov and Spokoiny (2017) \cite{nesterov2017random} proposed several random gradient-free
methods by using Gaussian smoothing technique. Duchi et al. \cite{duchi2015optimal} proposed a zeroth-order mirror descent algorithm. 

{\color{Brown}


Despite the meteoric rise of two-point based ZO algorithms, most of the work is restricted to convex problems \cite{liu2017zeroth,duchi2015optimal,shamir2017optimal,dvurechensky2018accelerated,wang2017stochastic}. For example, a ZO mirror descent algorithm proposed by \cite{duchi2015optimal} has an exact rate $O({d}/ \epsilon^2)$, where $d$ is the number of optimization variables. The same rate is obtained by bandit convex optimization \cite{shamir2017optimal} and ZO online alternating direction method of multipliers \cite{liu2017zeroth} for nonsmooth problems. More recently, \cite{yu2018generic,dvurechensky2018accelerated} presented the accelerated zeroth-order methods for the convex optimization. Current studies suggested that ZO algorithms typically agree with the iteration complexity of first-order
algorithms up to a small-degree polynomial of the problem size $d$.

{\color{Green} The above zeroth-order methods mainly focus on the
(strongly) convex problems. Thus, exploring zeroth-order stochastic methods for the nonconvex optimization is indeed desirable. In fact, there exist many nonconvex machine learning tasks, whose explicit gradients are not available, such as the nonconvex black-box learning problems \cite{chen2017zoo,liu2018zeroth}. 
{\color{Brown}
In contrast to the convex setting, nonconvex ZO algorithms are comparatively under-studied except
a few recent attempts \cite{lian2016comprehensive,nesterov2017random,
ghadimi2013stochastic,hajinezhad2017zeroth,gu2016zeroth,kazemi2018proximal}.
}
For example, \cite{ghadimi2013stochastic} proposed the randomized stochastic gradient-free (RSGF) method, i.e., a zeroth-order stochastic gradient method. To accelerate optimization, more recently, \cite{liu2018stochastic,liu2018zeroth} proposed the zeroth-order stochastic variance reduction gradient (ZO-SVRG)
methods. 
{\color{Brown}
In contrast to convex optimization, in nonconvex setting the stationary condition is used to measure the convergence of stationary points.
}
{\color{Violet}
Recently, for the nonsmooth nonconvex case, \cite{huang2019faster} provided two algorithms called ZO-ProxSVRG and ZO-ProxSAGA, which are based on the well-known variance reduction techniques ProxSVRG and ProxSAGA \cite{reddi2016proximal}. Before that,  \cite{ghadimi2013stochastic} also considered the stochastic case (here we denote
it as RSPGF). However, RSPGF requires the batch sizes being a large number (i.e., $\Omega(1/\epsilon)$) or increasing with
iterations. Note that RSPGF may reduce to deterministic proximal gradient descent (ZO-ProxGD) after some iterations due to the
increasing batch sizes. Note that from the perspectives of both computational efficiency and statistical generalization,
always computing full-gradient (GD or RSPGF) may not be desirable for large-scale machine learning problems. A reasonable minibatch size is also desirable in practice, since the computation of minibatch stochastic gradients can be
implemented in parallel. In fact, practitioners typically use moderate minibatch sizes. Hence, it is important to study the convergence in moderate and constant minibatch size regime.

}
Moreover, to solve the large-scale machine learning problems, some asynchronous parallel stochastic zeroth-order algorithms have been proposed in \cite{gu2018inexact,lian2016comprehensive,gu2018faster}.
}
In \cite{lian2016comprehensive}, an asynchronous ZO stochastic coordinate
descent (ZO-SCD) was derived for parallel optimization and achieved the rate of $O(d/\epsilon^2)$. {\color{Violet} Note that from the perspectives of both computational efficiency and statistical generalization, always computing full-gradient may not be desirable for large-scale machine learning problems.} This motivates our study on a more general framework ZO-SVRG under different gradient estimators.
}

\subsection{Motivation at the end of related works}
Although the above zeroth-order stochastic methods can effectively solve the nonconvex optimization, there are few zeroth-order stochastic methods for the nonconvex nonsmooth composite optimization except the RSPGF method presented in \cite{ghadimi2016accelerated} and ZO-ProxSVRG/SAGA \cite{huang2019faster}. We emphasize that, unlike these methods, we do not assume  bounded gradient since it is not the case for many unconstrained optimization problems. In addition, Liu et al. \cite{liu2018zeroth} have also studied the zeroth-order algorithm for solving the nonconvex nonsmooth problem, which is different from problem \eqref{problem}.
}
\begin{table}\label{table-compare}
\begin{center}
\begin{tabular}{ |c|c|c|c|c| } 
 \hline
 Method & Problem & Stepsize& Convergence rate & SZO complexity\\ 
 \hline
  
 RGF\cite{nesterov2017random} & NS(C) & $O\left(\frac{1}{\sqrt{dT}}\right)$ & $O\left(\frac{d^2}{\epsilon^2}\right)$ &$O\left(\frac{nd^2}{\epsilon^2}b\right)$\\
 RSPGF\cite{ghadimi2016accelerated} & S(NC)+NS(C) & $O\left(1\right)$ & $O\left(\frac{d}{\epsilon^2}\right)$ &$O\left(\frac{nd}{\epsilon^2}\right)$\\ 
 MD \cite{duchi2015optimal} & S(SC) & $\frac{1}{\sqrt{dT}}$  & $O\left(\frac{d}{\epsilon^2}\right)$ & $O\left(\frac{db}{\epsilon^2}\right)$\\ 
 ZO-SVRG-Coord \cite{liu2018zeroth} & S(NC)& $O\left(\frac{1}{{d}}\right)$ & $O\left(\frac{d}{\epsilon}\right)$ & $O(\frac{nd^2}{\epsilon}+\frac{d^2b}{\epsilon})$\\
  ZO-ProxSVRG\cite{gu2018faster} & S(NC)+NS(C) & $O\left(\frac{1}{{d}}\right)$ & $O\left(\frac{d}{\epsilon}\right)$ & $O(\frac{nd^2}{\epsilon\sqrt{b}}+\frac{md^2\sqrt{b}}{\epsilon})$\\
   ZO-ProxSAGA\cite{gu2018faster} & S(NC)+NS(C)& $O\left(\frac{1}{{d}}\right)$ & $O\left(\frac{d}{\epsilon}\right)$ & $O(\frac{nd^2}{\epsilon\sqrt{b}})$\\
   Ours & S(NC)+NS(C) & $O\left(\frac{1}{{\sqrt{d}}}\right)$ & $O\left(\frac{\sqrt{d}}{\epsilon}\right)$ & $O\left(\min\{n,\frac{1}{\epsilon}\}\frac{d\sqrt{d}}{\epsilon\sqrt{b}}+\frac{m d\sqrt{db}}{\epsilon}\right)$\\
   Ours & S(PL)+NS(C) & $O\left(\frac{1}{{\sqrt{d}}}\right)$ & $O\left(\frac{\sqrt{d}}{\epsilon}\right)$ & {\scriptsize$O(\min\{n,\frac{1}{\epsilon}\}\frac{d\sqrt{d}}{\sqrt{b}}\log\frac{1}{\epsilon}+{m d\sqrt{db}}\log\frac{1}{\epsilon})$}\\
 \hline
\end{tabular}\caption{Summary of convergence rate and function query complexity of SZO algorithms. NC: Nonconvex, C: Convex, SC: Strong Convexity, and PL: Polyak-Łojasiewicz Condition.}
\end{center}
\end{table}

\subsection{Zeroth-order (ZO) gradient estimators}

{\color{Brown}
We
next provide a background on ZO gradient estimators.
Given an individual cost function $f_i$, a two-point random stochastic gradient estimator (RandSGE) $\hat{\nabla} f_i(x)$ is defined \cite{nesterov2017random,gao2018information}
\begin{equation}\label{gradestrand}
\hat{\nabla} f_i(x) = \frac{d(f_i(x+\mu u_i) - f_i(x))}{\mu}u_i,\qquad i\in [n],
\end{equation}
where recall that $d$ is the number of optimization variables, $\mu > 0$ is a smoothing parameter, and
$\{u_i\}$ are i.i.d. random directions drawn from a uniform distribution over a unit sphere \cite{flaxman2005online,shamir2017optimal,gao2018information}. In general, RandSGE is a biased approximation to the true gradient $\nabla f_i(x)$, and its bias reduces as $\mu$ approaches zero. However, in a practical system, if $\mu$ is too small, then the function difference
could be dominated by the system noise and fails to represent the function differential \cite{lian2016comprehensive}.}
{\color{Green}
To obtain a better estimated gradient, we
can use the coordinate gradient estimator
(CoordSGE) \cite{gu2018inexact,gu2018faster,liu2018zeroth} to estimate the gradients as follows:
\begin{equation}\label{gradestcoord}
\hat{\nabla} f_i(x) = \sum_{j=1}^d \frac{f_i(x+\mu_je_j) - f_i(x-\mu_je_j)}{2\mu_j}e_j,\qquad i\in [n],
\end{equation}
where $\mu_j$ is a coordinate-wise smoothing parameter, and $e_j$ is a standard basis vector with $1$ at its $j$-th coordinate,
and $0$ otherwise. 
}
{\color{Brown}
Compared to RandSGE, CoordSGE is deterministic and requires $d$ times more function queries. 
However, it is evident that it yields an improved  convergence rate and iteration complexity \cite{liu2018zeroth}. More details on ZO gradient estimation can be found in \cite{kazemi2018proximal}.
}
 

{\color{red} In this work we only consider CoordSGE and the extension to RandSGE is straightforward.}

\subsection{Accelerated Proximal Gradient Method}
{\color{Green}
Because proximal gradient method needs to compute
the gradient at each iteration, it cannot be applied to solve the problems, where the explicit gradient of function $f(x)$ is
not available. For example, in the black-box machine learning model, only function values (e.g., prediction results) are
available \cite{chen2017zoo}. To avoid computing explicit gradient, we use the zeroth-order gradient estimators \cite{nesterov2017random,liu2018zeroth}(Nesterov and Spokoiny, 2017; Liu et al., 2018c) to estimate the
gradient only by function values.
Based on the estimated gradients \eqref{gradestcoord}, we give a zeroth-order proximal gradient descent method, which performs  iterations similar to:
\begin{equation}
x_{t}^s= \Po_{\eta h}(x_{t-1}^s - \eta \hat{\nabla} f(x_{t-1}^s)),\qquad t=1, 2, \ldots
\end{equation}
where $\hat{\nabla} f=\frac{1}{n}\sum_{i=1}^n \hat{\nabla} f_i(x)$ and 
\begin{equation}\label{po-operator}
\Po_{\eta h}(x) := \text{arg}\,\,\min_{y\in\R^d}\left(h(y)+\frac{1}{2\eta}\norm{y-x}^2\right)
\end{equation}
}
{\color{Green}


{\color{Violet}
We also assume that the
nonsmooth convex function $h(x)$ in \eqref{problem} is well structured, i.e., the proximal operator \eqref{po-operator} on $h$ can be computed efficiently.

}
}
\section{Accelerated Proximal Gradient Method}


\begin{algorithm}\label{APGnonconvex-Algo}
\caption{ZO-PSVRG+}
\begin{algorithmic}[1]
\State\Input initial point $x_0$, batch size $B$, minibatch size $b$, epoch length $m$, step size $\eta$
\State\Initialize $\tilde{x}^0 = x_0$
\For{ $s=1,2,\ldots, S$ }
\State $x_0^s = \tilde{x}^{s-1}$
\State $\hat{g}^s = \frac{1}{B} \sum_{j\in I_B} \hat{\nabla} f_j (\tilde{x}^{s-1})$
\For{ $t=1,2,\ldots, m$ }
\State ${\hat{v}}_{t-1}^s = \frac{1}{b} \sum_{i\in I_b}\left(\hat{\nabla} f_{i}(x_{t-1}^s)-\hat{\nabla} f_{i}(\tilde{x}^{s-1})\right)+\hat{g}^s$
\State $x_{t}^s= \Po_{\eta h}(x_{t-1}^s - \eta \hat{v}_{t-1}^s)$
\EndFor
\State $\tilde{x}^{s} = x_m^s$
 \EndFor
 \State\Output $\hat{x}$ chosen uniformly from $\{x_{t-1}^s\}_{t\in [m], s\in [S]}$
\end{algorithmic}
\end{algorithm}
{\color{Violet}
In this section, we propose a proximal stochastic gradient algorithm called ZO-PSVRG+, by using VR technique of ProxSVRG in \cite{xiao2014proximal,reddi2016proximal,li2018simple}.
(similar to nonconvex ZO-ProxSVRG \cite{huang2019faster} and ZO-SVRG-Coord \cite{liu2018zeroth}). The details
are described in Algorithm \ref{APGnonconvex-Algo}.
{\color{Brown}
Our algorithm has two kinds of random procedure. That is, in outer iteration,
we compute the gradient include $B$ samples. In inner iteration, we randomly select a mini-batch of samples $b$ to estimate the gradient.}
 We call $B$ the batch size and $b$ the minibatch size.

Compared with ZO-SVRG-Coord, ZO-ProxSVRG analyzed the nonconvex nonsmooth functions while ZO-SVRG-Coord only analyzed the smooth functions. The major difference of our ZO-PSVRG+ is that
we avoid the computation of the full gradient at the beginning of each epoch, i.e., $B$ may not equal to $n$ (see Line 5 of Algorithm \ref{APGnonconvex-Algo}) while ZO-SVRG-Coord and ZO-ProxSVRG used $B = n$. Note that even if we choose $B = n$, our analysis is
stronger than ZO-SVRG-Coord and ZO-ProxSVRG. As a result, our straightforward ZO-PSVRG+ generalizes these variance-reduced methods to a more general nonsmooth nonconvex zeroth-order case and yields simpler analysis.
}
{\color{Green}
 {\color{Brown}It has been shown in \cite{johnson2013accelerating,reddi2016stochastic,li2018simple} that the first-order ProxSVRG achieves the convergence rate $O(1/\epsilon )$, yielding $O(1/\epsilon)$ times less iterations than the ordinary SGD for solving finite sum problems.}
{\color{Green}
{\color{Brown} The key step of corresponding variance-reduced algorithmic framework is to generate an auxiliary sequence $\tilde{x}^{s-1}$ at which the full gradient is used as a reference in building a modified stochastic gradient estimate 
\begin{equation}\label{grad-fo}
{{v}}_{t-1}^s = \frac{1}{b} \sum_{i\in I_b}\left({\nabla} f_{i}(x_{t-1}^s)-{\nabla} f_{i}(\tilde{x}^{s-1})\right)+{g}^s
\end{equation}
where ${{v}}_{t-1}^s$ denotes the gradient estimate at $x_{t-1}^s$. The key property of \eqref{grad-fo} is that ${{v}}_{t-1}^s$ is an unbiased gradient estimate of $\nabla f(x_{t-1}^s)$. In the ZO setting, the gradient blending \eqref{grad-fo} is approximated using only function values,
\begin{equation}\label{zo-grad-fo}
{\hat{v}}_{t-1}^s = \frac{1}{b} \sum_{i\in I_b}\left(\hat{\nabla} f_{i}(x_{t-1}^s)-\hat{\nabla} f_{i}(\tilde{x}^{s-1})\right)+\hat{g}^s
\end{equation}
where $\hat{g}^s= \frac{1}{B}\sum_{i\in I_B}\hat{\nabla} f_{i}(\tilde{x}^{s-1})$ and $\hat{\nabla} f_{i}$ is a ZO gradient estimate specified by CoordSGE.  Replacing \eqref{grad-fo} with \eqref{zo-grad-fo} in ProxSVRG leads to a new ZO
algorithm, which we call ZO-PSVRG+ (Algorithm \ref{APGnonconvex-Algo}). We also avoid the computation of the full gradient at the beginning of each epoch, i.e., $B \neq n$.
}
Note that, $\E_{I_b}[\hat{v}_{t-1}^s] = \hat{\nabla} f(x_{t-1}^s) \neq {\nabla} f(x_{t-1}^s)$, i.e., this stochastic gradient is a biased estimate of the true full gradient.
{\color{Brown} That is, the unbiased assumption on gradient estimates used in ProxSVRG \cite{reddi2016proximal,li2018simple} no longer holds. We highlight that although ZO-PSVRG is similar
to ProxSVRG except the use of ZO gradient estimators to estimate batch, mini-batch, as well as blended
gradients, this seemingly minor differences yield an essential difficulties in the analysis of ZO-PSVRG+.
}
Thus, adapting the similar ideas of ProxSVRG to zeroth-order algorithm \ref{APGnonconvex-Algo} is not a trivial task {\color{Brown} and a careful analysis of ZO-PSVRG+ is much needed.} To address this issue, we analyze the upper bound for the variance of the estimated gradient $\hat{v}_t^s$, and choose the appropriate step size $\eta$ and smoothing parameter $\mu$ to control
this variance, which will be in detail discussed in the below theorems.
}
}

\subsection{reason to avoid full gradient calculation}
{\color{Green}
Since ZO-ProxGD needs to estimate full gradient
$\hat{\nabla}=\frac{1}{n}\sum_{i=1}^n f_i(x)$ when $n$ is large in the problem \eqref{problem}, its high cost per iteration is prohibitive. As a result, \cite{ghadimi2016accelerated} proposed the RSPGF with calculating the gradient on the mini-batch $\mathcal{I}_t$. {\color{Violet} Note that from the perspectives of both computational efficiency and statistical generalization, always computing full-gradient may not be desirable for large-scale machine learning problems.}
}

\section{Convergence Analysis}
{\color{Green}
Next, we give some
mild assumptions regarding problem \eqref{problem} as follows:
}
\begin{assumption}\label{Lip-Zoo}
For $\forall i\in{1,2,\ldots,n}$, gradient of the function $f_i$ is Lipschitz continuous with a Lipschitz constant $L > 0$, such that 
\[
\norm{\nabla f_i(x) - \nabla f_i(y)}\leq L \norm{x-y},\,\,\forall x,y\in\R^d.
\]
\end{assumption}

\begin{assumption}\label{Var-Zoo}
For $\forall x\in\R^d$, $\E\left[\norm{\hat{\nabla} f_i(x) - \hat{\nabla} f(x)}^2\right] \leq \sigma^2$, where $\sigma > 0$ is a constant and $\hat{\nabla} f_i(x)$ is a CoordSGE gradient estimator of $\nabla f_i(x)$.
\end{assumption}
{\color{Green}
{\color{Brown}
Both Assumptions \ref{Lip-Zoo} and \ref{Var-Zoo} are the standard assumptions used in nonconvex optimization.}
The first assumption is used for the convergence analysis of the zeroth-order algorithms \cite{ghadimi2016accelerated,nesterov2017random,
liu2018zeroth}.} {\color{Green} The second assumption gives the bounded variance of zeroth-order gradient estimates \cite{lian2016comprehensive,liu2018stochastic,
liu2018zeroth,hajinezhad2017zeroth}. {\color{Brown}
We emphasize that, unlike \cite{duchi2015optimal,ghadimi2013stochastic,huang2019faster}  we do not assume  bounded gradient since it is not the case for many unconstrained optimization problems. Note that assumption \ref{Var-Zoo} is milder than the assumption of bounded gradients \cite{liu2017zeroth,hajinezhad2017zeroth}.}
{\color{Green}Although, we are able to analyze more complex problem \eqref{problem} including a non-smooth part and drive faster convergence rates.} {\color{Violet}Such an assumption is necessary if one wants the convergence result to be independent of $n$.
}
{\color{Green}
We start by deriving an upper bounds for the variance of estimated gradient $\hat{v}_{t-1}^s$ based on the CoordSGE.}
}
\begin{lemma}\label{var-estimate-lem}
Using CoordSGE given the mixture estimated gradient $\hat{v}_{t-1}^s = \frac{1}{b} \sum_{i\in I_b}\left(\hat{\nabla} f_{i}(x_{t-1}^s)-\hat{\nabla} f_{i}(\tilde{x}^{s-1})\right)+\hat{g}^s$ with $\hat{g}^s = \frac{1}{B} \sum_{j\in I_B} \hat{\nabla} f_j (\tilde{x}^{s-1})$, then the following inequality holds. 
\begin{equation}
\begin{split}
\E\left[\eta\norm{\nabla f(x_{t-1}^s)-{\hat{v}_{t-1}^s}}^2\right] \leq&  \frac{2\eta L^2 d}{b}\E\left[\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2\right]\\
&+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
\end{split}
\end{equation}
\end{lemma}
\begin{proof}
We have
\begin{align}
  \E&\left[\eta\norm{\nabla f(x_{t-1}^s)-{\hat{v}_{t-1}^s}}^2\right]\notag\\
   =&\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s)-\hat{g}^s\right)}^2\right]\notag\\
   =&\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s)-\frac{1}{B}\sum_{j\in I_B}\hat{\nabla} f_j(\tilde{x}^{s-1})\right)}^2\right]\notag\\
   =&\notag\E\left[\eta\norm{\frac{1}{b}\sum_{i\in I_b}\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)+ \left(\frac{1}{B}\sum_{j\in I_B}\hat{\nabla} f_j(\tilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\\
   =&\notag\eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})\right) - \left(\nabla f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)+ \frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\tilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\\
   \leq &\notag2\eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})\right) - \left(\hat{\nabla} f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)+ \frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\tilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\\
   &\,\, + 2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\label{var-estimate-lem-1}\\
    = &\notag 2 \eta\E\left[\norm{\frac{1}{b}\sum_{i\in I_b}\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})\right) - \left(\hat{\nabla} f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)}^2\right]\\
   &\,\, +2 \eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\tilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]+ 2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\label{var-estimate-lem-2}\\
   = &\notag\frac{2\eta}{b^2}\E\left[\sum_{i\in I_b}\norm{\left(\left(\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})\right) - \left({\hat{\nabla}} f(x_{t-1}^s) - \hat{\nabla} f(\tilde{x}^{s-1}) \right)\right)}^2\right]\\
   &\,\, \label{var-estimate-lem-3}+ 2\eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\tilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]+2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
   \leq  &\frac{2\eta}{b^2}\E\left[\sum_{i\in I_b}\norm{\hat{\nabla} f_i(x_{t-1}^s)-\hat{\nabla} f_i(\tilde{x}^{s-1})}^2\right]+ 2\eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\tilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\notag\\
   &\,\,\label{var-estimate-lem-4}+2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
    \leq  &\frac{2\eta L^2 d}{b}\E\left[\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2\right]+ 2\eta \E \left[ \norm{\frac{1}{B}\sum_{j\in I_B}\left(\hat{\nabla} f_j(\tilde{x}^{s-1}) - \hat{\nabla} f(\tilde{x}^{s-1})\right)}^2\right]\notag\\
   &\,\,\label{var-estimate-lem-5}+2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
   \leq  &\label{var-estimate-lem-6}\frac{2\eta L^2 d}{b}\E\left[\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+2\eta \E \norm{\hat{\nabla} f(x_{t-1}^s)-\nabla f(x_{t-1}^s)}^2\\
   \leq  &\label{var-estimate-lem-7}\frac{2\eta L^2 d}{b}\E\left[\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2\right]+ 2\frac{I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
 \end{align}
 where, recalling that a deterministic gradient estimator is used, the expectations are taking with respect to $I_b$ and $I_B$. The inequality \eqref{var-estimate-lem-1} holds by the Jensen’s inequality. \eqref{var-estimate-lem-2} and \eqref{var-estimate-lem-3} are based on $\E[\norm{x_1+x_2+\ldots+x_k}^2] = \sum_{i=1}^k \E[\norm{x_i}^2]$ if $x_1,x_2,\ldots,x_k$ are independent and of mean zero (note that $I_b$ and $I_B$ are also independent). \eqref{var-estimate-lem-4} uses the fact that $\E[\norm{x-\E[x]}^2] \leq \E[\norm{x}^2]$, for any random variable $x$. \eqref{var-estimate-lem-5} holds due to the following inequality  
 \begin{equation}
 \begin{split}
 \E \norm{\hat{\nabla} f_i(x_{t}^s)-\hat{\nabla} f_i(\tilde{x}^s)}^2 &= \E \norm{ \sum_{j=1}^d\frac{f_{i,\mu_j}(x_{t}^s)}{\partial x_j}e_j-\frac{f_{i,\mu_j}(\tilde{x}^s)}{\partial x_j}e_j}^2\\
 &\leq d \sum_{j=1}^d \E \norm{ \frac{f_{i,\mu_j}(x_{t}^s)}{\partial x_j}-\frac{f_{i,\mu_j}(\tilde{x}^s)}{\partial x_j}}^2\\
 &\leq L^2 d \sum_{j=1}^d \E \norm{x_{t,j}^s-\tilde{x}_{j}^s}^2 = L^2 d \norm{x_{t}^s-\tilde{x}^s}^2\\
 \end{split}
 \end{equation}
  where the last inequality used the fact that $f_{i,\mu_j}$ is $L$-smooth. \eqref{var-estimate-lem-6} is by Assumption \ref{Var-Zoo} and \eqref{var-estimate-lem-7} uses Lemma \ref{CooSGE}. The proof is now complete.
\end{proof}
{\color{Green}
Lemma \ref{var-estimate-lem} shows that variance of $\hat{v}_{t-1}^s$ has an upper bound. As the number of iterations increases, based on convergence analysis both $x_{t-1}^s$ and $\tilde{x}^{s-1}$ will approach the same stationary point $x^*$, then the
variance of stochastic gradient decreases, but does not vanishes, due to using the zeroth-order estimated gradient and variance with respect to the full gradient.
}

In the sequel we frequently use the following inequality
\begin{equation}\label{young}
\norm{x-z}^2 \leq (1+\frac{1}{\beta})\norm{x-y}^2 + (1+\beta) \norm{{y-z}}^2, \forall \beta> 0
\end{equation}
\subsection{Gradient Mapping}
{\color{Violet}
For convex problems, one typically uses the optimality gap $F(x) - F(x^*)$ as the convergence criterion. But for general nonconvex problems, one typically uses the gradient norm as the convergence
criterion. E.g., for smooth nonconvex problems (i.e., $h(x) = 0$), \cite{ghadimi2013stochastic,reddi2016stochastic,
lei2017non,liu2018zeroth}  used $\norm{\nabla F(x)}^2$ (i.e., $\norm{\nabla f(x)}^2$ ) to measure the convergence results. In order to analyze the
convergence results for nonsmooth nonconvex problems, we need to define the gradient mapping as follows (as in \cite{ghadimi2016accelerated,reddi2016proximal,huang2019faster}):
\begin{equation}
g_{\eta}(x) = \frac{1}{\eta}(x-\Po_{\eta,h}(x-\eta \nabla f(x)))
\end{equation}
Note that if $h(x)$ is a constant function (in particular, zero), this gradient mapping reduces to the ordinary gradient:
$g_{\eta}(x) = \nabla F(x) = \nabla f(x)$. In this paper, we use the gradient mapping $g_{\eta}(x)$ as the convergence criterion (same as
\cite{ghadimi2016accelerated,reddi2016proximal,parikh2014proximal}).}
{\color{Green}
For the nonconvex problems, if $g_{\eta}(x) = 0$, the point $x$ is a critical point (Parikh, Boyd, and others, 2014). Thus, we can
use the following definition as the convergence metric.
\begin{definition}
Solution $x$ is called $\epsilon$-accurate, if $\E\norm{g_{\eta}(x)}^2 \leq \epsilon$, for some $\eta > 0$.
\end{definition}
}
\subsection{Convergence}
In Theorem \ref{noncon-zoo}, we focus on the effect of CoordSGE on the convergence rate of ZO-PSVRG+ and give some remarks.

\begin{theorem}\label{noncon-zoo}
Suppose Assumptions \ref{Lip-Zoo} and \ref{Var-Zoo} hold, and the coordinate gradient estimator CoordSGE is used. The output $\hat{x}$ of Algorithm \ref{APGnonconvex-Algo} satisfies
  \begin{equation}\label{noncon-zoo-main}
  \begin{split}
\E[\norm{g_{\eta}(\hat{x})}^2] & \leq \frac{6\left(F(x_0) - F({x}^*)\right)}{\eta Sm} + \frac{I\{B < n\}12\sigma ^2}{B}+3{L^2 d^2 \mu^2}
\end{split}
 \end{equation}
where $\eta = \min\{\frac{1}{8L}, \frac{\sqrt{b}}{6mL\sqrt{d}}\}$ denotes the step size and $x^*$ denotes the optimal value of problem \ref{problem}.
\end{theorem}


\begin{proof}
Now, we apply Lemma \ref{lemma1} to prove Theorem \ref{noncon-zoo}. Let $x_t^s = \Po_{\eta h} (x_{t-1}^s - \eta \hat{v}_{t-1}^s)$ and $\overline{x}_t^s = \Po_{\eta h} (x_{t-1}^s - \eta \nabla f(x_{t-1}^s))$. By letting $x^+ = x_t^s$, $x = x_{t-1}^s$, $v = \hat{v}_{t-1}^s$ and $z = \overline{x}_t^s$ in \eqref{eq10}, we have
\begin{align}
F(x^s_t) \leq& F(\overline{x}_t^s) + \Iprod{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}{x_t^s-\overline{x}_t^s}-\frac{1}{\eta} \Iprod{x_t^s-x_{t-1}^s}{x_t^s-\overline{x}_t^s}\notag\\
&+\frac{L}{2}\norm{x_t^s-x_{t-1}^s}^2+\frac{L}{2}\norm{\overline{x}_t^s-x_{t-1}^s}^2.\label{eq16}
\end{align}
Besides, by letting $x^+ = \overline{x}_t^s$, $x = x_{t-1}^s$, $v = \nabla f(x_{t-1}^s)$ and $z = x = {x}_{t-1}^s$ in \eqref{eq10}, we have
\begin{align}
F(\overline{x}_t^s) \leq& F({x}_{t-1}^s) - \frac{1}{\eta}\Iprod{\overline{x}_t^s-x_{t-1}^s}{\overline{x}_t^s - x_{t-1}^s}+\frac{L}{2}\norm{\overline{x}_t^s-x_{t-1}^s}^2\notag\\
 =& F({x}_{t-1}^s) -(\frac{1}{\eta}-\frac{L}{2})\norm{\overline{x}_t^s-x_{t-1}^s}^2.\label{eq17} 
\end{align}
Combining \eqref{eq16} and \eqref{eq17} we have 
 \begin{align}
 F({x}_t^s) \leq& F({x}_{t-1}^s) +\frac{L}{2}\norm{{x}_t^s-x_{t-1}^s}^2 - \left(\frac{1}{\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}{x_t^s - \overline{x}_t^s}\notag\\
 & -\frac{1}{\eta} \Iprod{x_t^s-x_{t-1}^s}{x_t^s-\overline{x}_{t}^s}\notag\\
  =& F({x}_{t-1}^s)  +\frac{L}{2}\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}{x_t^s - \overline{x}_t^s}\notag\\
 &-\frac{1}{2\eta} \left(\norm{x_t^s-x_{t-1}^s}^2+ \norm{x_t^s-\overline{x}_{t}^s}^2-\norm{\overline{x}_{t}^s-x_{t-1}^s}^2\right)\notag\\
   =& F({x}_{t-1}^s)  -(\frac{1}{2\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{2\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}{x_t^s - \overline{x}_t^s}\notag\\
 & -\frac{1}{2\eta} \norm{x_t^s-\overline{x}_{t}^s}^2\notag\\
 \leq & F({x}_{t-1}^s)  -(\frac{1}{2\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{2\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}{x_t^s - \overline{x}_t^s}\notag\\
 & -\frac{1}{8\eta} \norm{x_t^s-{x}_{t-1}^s}^2 + \frac{1}{6\eta} \norm{\overline{x}_{t}^s-{x}_{t-1}^s}^2\label{eq18}\\
  = &  F({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\Iprod{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}{x_t^s - \overline{x}_t^s}\notag\\
  \leq & F({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\eta \norm{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}^2\label{eq19}
 \end{align}
 where the second inequality uses \eqref{young} with $\beta = 3$ and the last inequality holds due to the Lemma \ref{lemm-est-grad}.
 
Note that $x_t^s = \Po_{\eta h}(x_{t-1}^s - \eta \hat{v}_{t-1}^s)$ is the iterated form in our algorithm.  By taking the expectation with respect to all random variables in \eqref{eq19} we obtain
 \begin{equation}\label{eq25}
 \begin{split} 
\E[F({x}_{t}^s)] \leq \E\left[F({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2+\eta \norm{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}^2\right]\\
 \end{split}
 \end{equation}
In \eqref{eq25}, we further bound $\eta \norm{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}^2$ using Lemma \ref{var-estimate-lem} to obtain
 \begin{align} 
\E&[F({x}_{t}^s)] \notag
\notag
\\ \leq& \E\left[F({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{1}{3\eta}-L\right)\norm{\overline{x}_t^s-x_{t-1}^s}^2\right]\notag\\
&+ \frac{2\eta L^2 d}{b}\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 + \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\notag
\\ =& \E\left[F({x}_{t-1}^s)  -(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-x_{t-1}^s}^2- \left(\frac{\eta}{3}-L\eta^2\right)\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag
\\&+\frac{2\eta L^2 d}{b}\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{theor1-31}
\\ \leq& \E\left[F({x}_{t-1}^s)  -\frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \left(\frac{\eta}{3}-L\eta^2\right)\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag\\
&+(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2
+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{theor1-32}
 \end{align}
where recalling $\overline{x}_t^s := \Po_{\eta h}(x_{t-1}^s - \eta \nabla f(x_{t-1}^s))$, \eqref{theor1-31}  is based on the definition of gradient mapping $g_{\eta}(x_{t-1}^s)$. \eqref{theor1-32} uses \eqref{young} by choosing $\beta = 2t-1$.
 
Taking a telescopic sum for $t = 1, 2, \ldots, m$ in epoch $s$ from \eqref{theor1-32} and recalling that $x_m^s = \tilde{x}^s$ and $x_0^s = \tilde{x}^{s-1}$, we obtain
 \begin{align} 
\E&[F(\tilde{x}^s)] \notag
\\ \leq& \E\left[F(\tilde{x}^{s-1})  -\sum_{t=1}^m\frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \left(\frac{\eta}{3}-L\eta^2\right)\sum_{t=1}^m\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag\\
&+\sum_{t=1}^m(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2\notag\\
&+ \sum_{t=1}^m \frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \eta \frac{L^2 d^2 \mu^2}{2}\notag\\
\leq& \E\left[F(\tilde{x}^{s-1})  -\sum_{t=1}^{m-1}\frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \left(\frac{\eta}{3}-L\eta^2\right)\sum_{t=1}^m\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag\\
&+\sum_{t=2}^m(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2\notag\\
&+ \sum_{t=1}^m \frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \eta \frac{L^2 d^2 \mu^2}{2}\label{theor1-34}
\\
=& \E\left[F(\tilde{x}^{s-1}) - \left(\frac{\eta}{3}-L\eta^2\right)\sum_{t=1}^m\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag\\
&-\sum_{t=1}^{m-1}\left((\frac{1}{2t} - \frac{1}{2t+1})(\frac{5}{8\eta} - \frac{L}{2})-\frac{2\eta L^2 d}{b})\right)\E\norm{x_{t}^s-\tilde{x}^{s-1}}^2\notag\\
&+ \sum_{t=1}^m \frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \eta \frac{L^2 d^2 \mu^2}{2}\notag\\
\leq& \E\left[F(\tilde{x}^{s-1}) - \left(\frac{\eta}{3}-L\eta^2\right)\sum_{t=1}^m\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag\\
&-\sum_{t=1}^{m-1}\left(\frac{1}{6t^2}(\frac{5}{8\eta} - \frac{L}{2})-\frac{2\eta L^2 d}{b})\right)\E\norm{x_{t}^s-\tilde{x}^{s-1}}^2\notag\\
&+ \sum_{t=1}^m \frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \eta \frac{L^2 d^2 \mu^2}{2}\notag\\
\leq& \E\left[F(\tilde{x}^{s-1}) - \frac{\eta}{6}\sum_{t=1}^m\norm{g_{\eta}(x_{t-1}^s)}^2\right]+ \sum_{t=1}^m \frac{2I\{B < n\}\eta \sigma ^2}{B}+\sum_{t=1}^m \eta \frac{L^2 d^2 \mu^2}{2}\label{theor1-35}
 \end{align}
 where \eqref{theor1-34} holds since norm is always non-negative and $x_0^s = \tilde{x}^{s-1}$. In \eqref{theor1-35} we have used the fact that $(\frac{1}{6t^2}(\frac{5}{8\eta} - \frac{L}{2})-\frac{2\eta L^2 d}{b})\geq 0$ for all $1\leq t \leq m$ and $\frac{\eta}{6} \leq \frac{\eta}{3}-L\eta^2$ since $\eta = \min\{\frac{1}{8L}, \frac{\sqrt{b}}{6mL\sqrt{d}}\}$. 
 Telescoping the sum for $s = 1, 2, \ldots, S$ in \eqref{theor1-35}, we obtain
 \begin{equation*}
\begin{split} 
0 &\leq \E[F(\tilde{x}^S) - F({x}^*)] \\
&\leq \E\left[F(\tilde{x}^{0}) - F({x}^*) - \sum_{s=1}^S\sum_{t=1}^m\frac{\eta}{6}\norm{g_{\eta}(x_{t-1}^s)}^2 + \sum_{s=1}^S\sum_{t=1}^m(\frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2})\right]
 \end{split}
 \end{equation*}
 Thus, we have
  \begin{align}
\E[\norm{g_{\eta}(\hat{x})}^2] & \leq \frac{6\left(F(x_0) - F({x}^*)\right)}{\eta Sm} + \frac{I\{B < n\}12\sigma ^2}{B}+3{L^2 d^2 \mu^2}\label{theor1-eq36}
 \end{align}
 where \eqref{theor1-eq36} holds since we choose  $\hat{x}$ uniformly randomly from $\{x_{t-1}^s\}_{t\in [m], s\in [S]}$. 
\end{proof} 
{\color{Violet}
The proof for Theorem \ref{noncon-zoo} is notably different from that of ZO-SVRG-Coord and ZO-ProxSVRG/SAGA as they used a Lyapunov function to show that the accumulated gradient mapping decreases with epoch $s$. In our proof, we directly show that $F(x^s)$ decreases by  using a different analysis. This is made possible by tightening the inequalities using Young's inequality and Lemma \ref{var-estimate-lem} which yields a much simpler analysis for our ZO-PSVRG+ compared with ZO-SVRG-Coord, ZO-ProxSVRG and ZO-ProxSAGA.  Also, our convergence result holds for any minibatch size and any epoch size $m$ unlike ZO-SVRG-Coord which holds true only for specific values of $m$ with an involved parameter setting.
We also avoid the computation of the full gradient at the beginning of each epoch, i.e., $B \neq n$.
} \eqref{noncon-zoo-main} shows that a large batch size $B$ indeed reduces the variance of estimated full gradient and improves the convergence of ZO-PSVRG+.
 
{\color{Brown}
Compared to the convergence rate of SVRG as given in \cite{reddi2016proximal}, Theorem \ref{noncon-zoo} exhibits two
additional errors $\frac{I\{B < n\}\sigma ^2}{B}$ and $O(L^2d^2\mu^2)$ due to batch gradient estimation $B < n$ and the use of SZO gradient estimates, respectively. The error due to $B < n$ is eliminated only when $B = n$. Roughly
speaking, if we choose the smoothing parameter $\mu$ reasonably small, and the batch size $B$ reasonably large, then the error \eqref{noncon-zoo-main}
would reduce, leading to non-dominant effect on the convergence rate of ZO-PSVRG+. 

{\color{Brown}
If $B = n$, ZO-PSVRG+ reduces to ZO-ProxSVRG  since Step 7 of Algorithm \ref{APGnonconvex-Algo} becomes
$\frac{1}{B}\sum_{i\in I_B} \nabla f_i(\tilde{x}^s_{t-1}) = \nabla f(\tilde{x}^s_{t-1})$. }
Note that the stepsize  $\eta$ is involved, relying on the epoch length $m$, the minibatch size $b$ , and the number of optimization variables $d$. 
}

{\color{Brown}
In order to acquire explicit dependence on these parameters and to explore deeper insights of convergence, with the aid of Theorem \ref{noncon-zoo}, Corollary \ref{corr11} provides the convergence rate of ZO-PSVRG+ in terms of precision at the solution $\hat{x}$ and  simplifies \eqref{noncon-zoo-main} for a specific parameter setting, as formalized below.
}


 \begin{corollary}\label{corr11}
We set the batch size $B = \min\{12\sigma^2/\epsilon, n\}$ and the smoothing parameter $\mu \leq \frac{\sqrt{\epsilon}}{3\sqrt{dL}}$. Suppose $\hat{x}$ returned by Algorithm \ref{APGnonconvex-Algo}  is an $\epsilon$-accurate solution for problem \eqref{problem}. Recalling that CoordSGE require $O(d)$ function queries, the number of SZO calls is at most 
\begin{equation}\label{SZO-call-nocon}
{\color{Brown}d(SB+Smb) = 6d \left(F(x_0) - F({x}^*)\right) (\frac{B}{\epsilon\eta m}+\frac{b}{\epsilon\eta})} = O\left(\frac{Bd}{\epsilon\eta m}+\frac{bd}{\epsilon\eta}\right).
\end{equation} 
and the number of PO calls is equal to $T = Sm = \frac{6\left(F(x_0) - F({x}^*)\right)}{\epsilon\eta} = O\left(\frac{1}{\epsilon\eta}\right)$. In particular, by setting $m=\sqrt{b}$ and {\color{Brown}$\eta = \frac{1}{6L\sqrt{d}}$}, the number of ZO calls is at most 
\begin{equation}\label{SZO-call-par-nocon}
36d L (F(x_0)-F(x^*))\left(\frac{B\sqrt{d}}{\epsilon\sqrt{b}}+\frac{b\sqrt{d}}{\epsilon}\right) = O\left(s_n\frac{d\sqrt{d}}{\epsilon \sqrt{b}}+\frac{bd\sqrt{d}}{\epsilon}\right).
\end{equation}
where $s_n = \min\{n,\frac{1}{\epsilon}\}$ and the number of PO calls equals to $T = Sm = S\sqrt{b} = \frac{6\sqrt{d}\left(F(x_0) - F({x}^*)\right)}{\epsilon} = O\left(\frac{\sqrt{d}}{\epsilon}\right)$. 
\end{corollary}
\begin{proof}
Using Theorem \ref{noncon-zoo}, we have $\eta = \min\{\frac{1}{8L}, \frac{\sqrt{b}}{6mL\sqrt{d}}\}$
\begin{align}
\E[\norm{g_{\eta}(\hat{x})}^2] & \leq \frac{6\left(F(x_0) - F({x}^*)\right)}{\eta Sm} + \frac{I\{B < n\}12 \sigma ^2}{B}+3{L^2 d^2 \mu^2} = 3\epsilon\label{theor1-eq37}
 \end{align} 
 Now we obtain the total number of iterations  {\color{Brown} $T = Sm = \frac{6\left(F(x_0) - F({x}^*)\right)}{\epsilon\eta}$}. Since $\mu \leq \frac{\sqrt{\epsilon}}{3\sqrt{dL}}$, and for $B = n$, the second term in the bound \eqref{theor1-eq37} is $0$, the proof is finished as the number of SFO call equals to {\color{Brown}$Sn+Smb = 6 \left(F(x_0) - F({x}^*)\right) (\frac{n}{\epsilon\eta m}+\frac{b}{\epsilon\eta})$}. If  $B < n$ the number of SZO calls equal to  {\color{Brown}$d(SB+Smb) = 6d \left(F(x_0) - F({x}^*)\right) (\frac{B}{\epsilon\eta m}+\frac{b}{\epsilon\eta})$} by noting that $\frac{I\{B < n\}12\sigma^2}{B} \leq \epsilon$ due to $B \geq 12\sigma^2 /\epsilon$. The second part of corollary is obtained by setting $m = \sqrt{b}$ in the first part.
\end{proof}
Roughly speaking, Corollary \ref{corr11} shows  that if we choose the smoothing parameter $\mu$ reasonably small and the batch size $B$ sufficiently large, then the error induced by these terms would reduce, leading to non-dominant effect on the convergence rate of ZO-PSVRG+.
The error term inherited by batch size  is eliminated only when $B = n$  (i.e., $I\{B < n\} = 0$). In
this case, ZO-PSVRG+ reduces to ZO-ProxSVRG since Step 5 of Algorithm \ref{APGnonconvex-Algo} becomes $\hat{g}^s = \hat{\nabla} f(\tilde{x}^{s-1})$. 

If the smoothing parameter and batch size are selected appropriately, then we obtain the error term $O(\sqrt{d}/T)$, with is better than the convergence rate of competitor SZO methods by factor of $\frac{1}{\sqrt{d}}$. Moreover, ZO-PSVRG+ uses much less SZO oracle which is detailed in Table \ref{table-compare}.

It is worth mentioning that the condition on the value of step size in Theorem \ref{noncon-zoo} is less restrictive than several SZO algorithms  in Table \ref{table-compare}. For example, ZO-SVRG-Coord required $\eta = O(\frac{1}{d})$ which is smaller by a factor of $\sqrt{d}$ than ours. {\color{Brown}On the other hand, the condition on the value of smoothing parameter $\mu$ in Corollary \ref{corr11} is more restrictive than several SZO algorithms. For instance, ZO-ProxSVRG required $\mu= O(\frac{1}{\sqrt{d}})$ with a stepsize $\eta$ which scales by $\frac{1}{d}$.}


{\color{Green}
It is noted from equation \eqref{SZO-call-par-nocon} that the SZO complexity is increased by a factor $\sqrt{b}$, which is smaller than the size of the minibatch. However, the corresponding complexity of RGF is increased by multiplying a factor of $b$ (see Table \ref{table-compare}), so our algorithm has a better dependency to the mini-batch size in this special case.

}
{\color{Brown}
Further, our work and reference \cite{f} show that a large batch $B$  for $B \neq n$ indeed reduces the error inherited by variance and improves the convergence of SZO optimization methods.
}
\section{Convergence Under PL Condition}
{\color{Violet} In this section, we provide the global linear convergence rate for nonconvex functions under the Polyak-Łojasiewicz
(PL) condition \cite{polyak1963gradient}.}
The original form of PL condition is
\begin{equation}
\exists \lambda >0, \text{such~that} \norm{\nabla f(x)}^2 \geq 2\lambda (f(x) - f^*),\,\, \forall x,
\end{equation}
where $f^*$ denotes the (global) optimal function value. It is worth noting that $f$ satisfies PL condition when $f$ is $\lambda$-strongly convex.
{\color{RubineRed}
This condition specifies how fast the objective function grows in a local neighborhood of optimal solutions.
}
{\color{RubineRed}
In particular, we propose a generic convergence framework for accelerating existing SZO algorithms for functions satisfying PL settings by leveraging variance reduced methods. This
is accomplished by a novel synthesis of existing SZO algorithms.
}{\color{Brown} We show the iteration complexity of ZO-PSVRG+ (Algorithm \ref{APGnonconvex-Algo}) is improved by applying PL condition .}



Due to the nonsmooth term $h(x)$ in problem \eqref{problem}, we use the gradient mapping to define a more general form of PL condition as follows
\begin{equation}\label{zo-pl-cond}
\exists \lambda >0, \text{such~that} \norm{g_{\eta}(x)}^2 \geq 2\lambda (F(x) - F^*),\,\, \forall x.
\end{equation}
Recall that if $h(x)$ is a constant function, the gradient mapping reduces to $g_{\eta}(x) = \nabla f(x)$.
{\color{RubineRed}
Note that the PL condition has been studied thoroughly in \cite{karimi2016linear}. The authors show that PL condition is weaker than broad family of  conditions. For example, when $f(x)$ is convex,  quadratic growth condition holds \cite{luo1993error,anitescu2000degenerate}. 
}

The PL condition \eqref{zo-pl-cond} is arguably natural and considered in several existing works \cite{li2018simple}.
{\color{Violet}
Similar to Theorem \ref{noncon-zoo}, we provide the convergence result of ZO-PSVRG+ (Algorithm \ref{APGnonconvex-Algo}) under PL-condition in the
following Theorem \ref{PL-Zoo}. Note that under PL condition (i.e. \eqref{zo-pl-cond} holds), ZO-PSVRG+ can directly use the final iteration $\tilde{x}^S$
as the output point instead of the randomly chosen
one $\hat{x}$. 
}

\begin{theorem}\label{PL-Zoo}
Let Assumptions \ref{Lip-Zoo} and \ref{Var-Zoo} hold, and  CoordSGE  is  used  in
 Algorithm \ref{APGnonconvex-Algo} with step size $\eta \leq \min\{\frac{1}{8L}, \frac{\sqrt{\gamma b}}{5 m L \sqrt{d}}\}$ where $\gamma = 1-\frac{2\lambda\eta}{3} m-\frac{\lambda\eta}{3} > 0$. Then 
\begin{equation}\label{PL-eq-error}
\begin{split}
\E[F(\tilde{x}^S) - {F}^*] & \leq   \left(1-\frac{\lambda\eta}{3}\right)^{Sm} \E[F(\tilde{x}^0) - {F}^*] + \frac{6I\{B < n\} \sigma ^2}{\lambda B}+\frac{3 L^2 d^2 \mu^2}{2\lambda}
\end{split}
\end{equation}
where $x^*$ is same as Theorem \ref{noncon-zoo}.
\end{theorem}
\begin{proof}
We start by recalling inequality \eqref{theor1-31} from the proof of Theorem \ref{noncon-zoo}, i.e.,

{\color{Brown}
\begin{align} 
\E&[F({x}_{t}^s)] \notag
\\ \leq& \E\left[F({x}_{t-1}^s)  -\frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \left(\frac{\eta}{3}-L\eta^2\right)\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag\\
&+(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2
+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\notag
\\ \leq& \E\left[F({x}_{t-1}^s)  -\frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \frac{\eta}{6}\norm{g_{\eta}(x_{t-1}^s)}^2\right]\notag\\
&+(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2
+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{theo2-eq44}
 \end{align}
 where in \eqref{theo2-eq44} inequality we applied $\eta L \leq \frac{1}{6}$.
 }
Moreover, substituting PL inequality, i.e., 
\begin{equation}
\norm{g_{\eta}(x)}^2 \geq 2\lambda (F(x) - F^*)
\end{equation}
into \eqref{theo2-eq44}, we obtain

{\color{Brown}
\begin{align} 
\E&[F({x}_{t}^s)] \notag
\\ \leq& \E\left[F({x}_{t-1}^s)  -\frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2- \lambda\frac{\eta}{3}(F({x}_{t-1}^s) - F^*) \right]\notag\\
&+(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2
+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}
 \end{align}
 }
Thus, we have
{\color{Brown}
\begin{align} 
\E&[F({x}_{t}^s)] \notag
\\ \leq& \E\left[ (1-\lambda\frac{\eta}{3})(F({x}_{t-1}^s) - F^*)   -\frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right]\notag\\
&+(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\E\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2
+ \frac{2I\{B < n\}\eta \sigma ^2}{B}+\eta \frac{L^2 d^2 \mu^2}{2}\label{theo2-eq46}
 \end{align}
 }
Let {\color{Brown}$\alpha := 1 - \lambda\frac{\eta}{3}$} and $\Psi_t^s := \frac{\E[F({x}_{t}^s)-F^*]}{\alpha^t}$. Combining these definitions with \eqref{theo2-eq46}, we have  

{\color{Brown}
\begin{align} 
\Psi_t^s \notag
\\ \leq& \Psi_{t-1}^s - \frac{1}{\alpha^t}\E\left[ \frac{1}{2t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2-(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2}))\norm{x_{t-1}^s-\tilde{x}^{s-1}}^2\right]\notag\\
&+ \frac{1}{\alpha^t}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1}{\alpha^t}\eta \frac{L^2 d^2 \mu^2}{2}\label{theo2-eq47}
 \end{align}
 }
Similar to the proof of Theorem \ref{noncon-zoo}, summing \eqref{theo2-eq47} for $t=1, 2, \ldots, m$ in epoch $s$ and recalling that $x_m^s = \tilde{x}^s$ and $x_0^s = \tilde{x}^{s-1}$, we have 
{\color{Brown}
\begin{align}
\E&[F(\tilde{x}^s)-F^*]\notag\\
\leq\alpha^m\E&\left[(F(\tilde{x}^{s-1}) - F^*)\right] +\alpha^m\sum_{t=1}^m \frac{1}{\alpha^t}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\alpha^m\sum_{t=1}^m \frac{1}{\alpha^t}\eta \frac{L^2 d^2 \mu^2}{2}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{1}{2t\alpha^t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=1}^m\frac{1}{\alpha^t}(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2})) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(F(\tilde{x}^{s-1}) - F^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha} \frac{\eta L^2 d^2 \mu^2}{2}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^m\frac{1}{2t\alpha^t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2-\sum_{t=1}^m\frac{1}{\alpha^t}(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2})) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(F(\tilde{x}^{s-1}) - F^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{\eta L^2 d^2 \mu^2}{2}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{1}{2t\alpha^t}(\frac{5}{8\eta} - \frac{L}{2})\norm{{x}_t^s-\tilde{x}^{s-1}}^2\right]\notag\\
+\alpha^m\E&\left[\sum_{t=2}^m\frac{1}{\alpha^t}(\frac{2\eta L^2 d}{b}+\frac{1}{2t-1}(\frac{5}{8\eta} - \frac{L}{2})) \norm{x_{t-1}^s-\tilde{x}^{s-1}}^2 \right]\label{eq48}\\
\leq\alpha^m\E&\left[(F(\tilde{x}^{s-1}) - F^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{\eta L^2 d^2 \mu^2}{2}\notag
\\-\alpha^m\E&\left[\sum_{t=1}^{m-1}\frac{1}{\alpha^{t+1}}\left((\frac{\alpha}{2t} - \frac{1}{2t+1})(\frac{5}{8\eta} - \frac{L}{2})-\frac{2\eta L^2 d}{b})\right) \norm{x_{t}^s-\tilde{x}^{s-1}}^2 \right]\notag\\
\leq\alpha^m\E&\left[(F(\tilde{x}^{s-1}) - F^*)\right] + \frac{1-\alpha^m}{1-\alpha}\frac{2I\{B < n\}\eta \sigma ^2}{B}+\frac{1-\alpha^m}{1-\alpha}\frac{\eta L^2 d^2 \mu^2}{2}\label{eq50}
\end{align}
}
where \eqref{eq48} since $\norm{.}^2$ always is non-negative and $x_0^s=\tilde{x}^{s-1}$. \eqref{eq50} holds since it is sufficient to show ${\color{Brown} (\frac{\alpha}{2t} - \frac{1}{2t+1})(\frac{5}{8\eta} - \frac{L}{2})- \frac{2\eta L^2 d}{b}} \geq 0$, for all $t=1, 2,\ldots, m$. 
It is easy to see that this inequality holds since $\eta \leq \min\{\frac{1}{8L}, \frac{\sqrt{\gamma b}}{5 m L \sqrt{d}}\}$, where $\gamma = 1-2\beta m-\beta > 0$. Similarly, let  $\tilde{\alpha} = \alpha^m$ and $\tilde{\Psi}^s = \frac{\E[F(\tilde{x}^{s})-F^*]}{\tilde{\alpha}^s}$. Substituting these definitions into \eqref{eq50}, we have
{\color{Brown}
\begin{equation}\label{eq51}
\tilde{\Psi}^s \leq \tilde{\Psi}^{s-1} + \frac{1}{\tilde{\alpha}^s} \frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{I\{B < n\}\eta \sigma ^2}{B}+ \frac{1}{\tilde{\alpha}^s} \frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{L d \mu^2}{12}
\end{equation}
}
Taking a telescopic sum from \eqref{eq51} for all epochs $1 \leq s \leq S$, we obtain
{\color{Brown}
\begin{align}
\E[F(\tilde{x}^S) - {F}^*] & \leq \tilde{\alpha}^{S} \E[F(\tilde{x}^0) - {F}^*] + \tilde{\alpha}^{S} \sum_{s=1}^S \frac{1}{\tilde{\alpha}^s}\frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{2I\{B < n\}\eta \sigma ^2}{B} + \tilde{\alpha}^{S} \sum_{s=1}^S \frac{1}{\tilde{\alpha}^s}\frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{\eta L^2 d^2 \mu^2}{2}\notag\\
& = {\alpha}^{Sm} \E[F(\tilde{x}^0) - {F}^*] + \frac{1-\tilde{\alpha}^S}{1-\tilde{\alpha}}\frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{2I\{B < n\}\eta \sigma ^2}{B}+ \frac{1-\tilde{\alpha}^S}{1-\tilde{\alpha}}\frac{1-\tilde{\alpha}}{1-{\alpha}}\frac{\eta L^2 d^2 \mu^2}{2}\notag\\
& \leq {\alpha}^{Sm} \E[F(\tilde{x}^0) - {F}^*] + \frac{1}{1-{\alpha}}\frac{2 I\{B < n\}\eta \sigma ^2}{B}+ \frac{1}{1-{\alpha}}\frac{\eta L^2 d^2 \mu^2}{2}\notag\\
& = \left(1-\frac{\lambda\eta}{3}\right)^{Sm} \E[F(\tilde{x}^0) - {F}^*] + \frac{6I\{B < n\} \sigma ^2}{\lambda B}+\frac{3 L^2 d^2 \mu^2}{2\lambda}\label{eq52}
\end{align}
}
where in \eqref{eq52} we recall that {\color{Brown}$\alpha = 1-\frac{\lambda\eta}{3}$}.
\end{proof}
{\color{Brown}
Theorem \ref{PL-Zoo} shows that if the batch size and smoothing parameter are  appropriately chosen, ZO-PSVRG+ has a dominant linearly decaying convergence rate. 

}
Further, by comparing with Theorem \ref{noncon-zoo}, it is evident from \eqref{PL-eq-error} that the
 common error term $\frac{6I\{B < n\} \sigma ^2}{B}+\frac{3L^2 d^2 \mu^2}{2}$ is amplified through multiple $1/\lambda$. Thus, the error induced by these terms ceases to be significantly improved if $\lambda >> 1$. We next study the number of oracle calls in ZO-PSVRG+ under PL condition to obtain an $\epsilon$-accurate solution, as formalized in Corollary \ref{PL-Zo-Cor}. 

\begin{corollary}\label{PL-Zo-Cor}
Suppose the final iteration point $\tilde{x}^S$ in Algorithm \ref{APGnonconvex-Algo} satisfies $\E[F(\tilde{x}^S) - F^*]\leq \epsilon$ under PL condition. Under Assumptions \ref{Lip-Zoo} and \ref{Var-Zoo}, we let batch size $B = \min\{\frac{6\sigma^2}{\lambda\epsilon},n\}$ and the smoothing parameter $\mu\leq \frac{\epsilon}{2Ld\lambda}$. The number of SZO calls is bounded by
\[
{\color{Brown}d(SB+Smb) = O(\frac{s_n d}{\lambda\eta m}\log\frac{1}{\epsilon}+\frac{b d}{\lambda\eta}\log\frac{1}{\epsilon})}
\]
where $s_n = \min \{n,\frac{1}{\lambda \epsilon}\}$.
The number of PO calls equals to the total number of iterations $T$ which is bounded by
\[
{\color{Brown} T = Sm = O(\frac{1}{\lambda\eta}\log\frac{1}{\epsilon})}
\]
In particular, given the setting  $m=\sqrt{b}$ and {\color{blue}$\eta = \frac{\sqrt{\gamma}}{5 L \sqrt{d}}$}, the number of SZO calls  simplifies to 
{\color{blue}$d(SB+Smb) = O(\frac{Bd\sqrt{d}}{\lambda\sqrt{\gamma} m}\log\frac{1}{\epsilon}+\frac{bd\sqrt{d}}{\lambda\sqrt{\gamma}}\log\frac{1}{\epsilon})$.}
\end{corollary}
\begin{proof}
From Theorem \ref{PL-Zoo}, we have
\begin{align}
\E[F(\tilde{x}^S) - {F}^*] & \leq   \left(1-\frac{\lambda\eta}{3}\right)^{Sm} \E[F(\tilde{x}^0) - {F}^*] + \frac{6I\{B < n\} \sigma ^2}{\lambda B}+\frac{3 L^2 d^2 \mu^2}{2\lambda}= 3 \epsilon
\end{align}
which gives the total number of iterations {\color{Brown} $T = Sm = O(\frac{1}{\lambda\eta}\log\frac{1}{\epsilon})$} and equals to the number of PO calls. Since $\mu \leq \frac{\sqrt{2\lambda\epsilon}}{L d}$, we have $\frac{3 L^2 d^2 \mu^2}{2\lambda} \leq \epsilon$. The number of SZO calls equals to {\color{Brown}$d(SB+Smb) = O(\frac{Bd}{\lambda\eta m}\log\frac{1}{\epsilon}+\frac{bd}{\lambda\eta}\log\frac{1}{\epsilon})$}.  Note that if $B < n$ then $\frac{6I\{B < n\} \sigma ^2}{\lambda B} \leq \epsilon$ since $B \geq 6 {\sigma ^2}/{\lambda \epsilon}$. With $m=\sqrt{b}$ and $\eta = \frac{\sqrt{\gamma}}{5 L \sqrt{d}}$, the number of PO calls equals to {\color{Blue} $T = Sm = O(\frac{B\sqrt{d}}{\lambda\sqrt{\gamma}}\log\frac{1}{\epsilon})$} and the number of SZO calls equals to {\color{blue}$d(SB+Smb) = O(\frac{Bd\sqrt{d}}{\lambda\sqrt{\gamma} m}\log\frac{1}{\epsilon}+\frac{bd\sqrt{d}}{\lambda\sqrt{\gamma}}\log\frac{1}{\epsilon})$}.
\end{proof}
Corollary \ref{PL-Zo-Cor} shows that the use of PL condition improves the dominant convergence rate, where the error of
order $O(d/\epsilon)$ in Corollary \ref{corr11} improves to $O(\sqrt{d}\log(1/\epsilon))$, resulting in a significant speed up.
{\color{Brown}
 Compared to the aforementioned ZO algorithms \cite{duchi2015optimal,nesterov2017random,liu2018zeroth}, the convergence performance of ProxSVRG+ under PL condition has a global linear rather than sub-linear convergence rate and therefore  uses much less ZO oracle calls. 
}

{\color{Violet}
We show that ProxSVRG+ directly obtains a global linear convergence rate without restart by a nontrivial proof. Note that Reddi et al. [2016b] used PL-SVRG/SAGA to restart ProxSVRG/SAGA $O(log(1/\epsilon))$ times to obtain
the linear convergence rate under PL condition.
Moreover, similar to Table 2, if we choose $b = 1$ or $n$ for ProxSVRG+, then its convergence result is $O(\log \frac{1}{\epsilon})$,
which is the same as ProxGD [Karimi et al., 2016]. If we choose $b = n$ for ProxSVRG+, then the convergence result is $O()$, the same as the best result achieved by ProxSVRG/SAGA [Reddi et al., 2016b]. If we
choose $b = $ for ProxSVRG+, then its convergence result is $O(\log\frac{1}{\epsilon})$
which generalizes the best result of SCSG [Lei et al., 2017] to the more general nonsmooth nonconvex case and is better than ProxGD and ProxSVRG/SAGA. Also note that our ProxSVRG+ uses much less proximal oracle calls than ProxSVRG/SAGA if $b < n ^{2/3}$.

}

\begin{remark}
Compared to the convergence rate of ZO-PSVRG+ as given in Theorem \ref{noncon-zoo}, Theorem \ref{PL-Zoo} exhibits additional parameter $\gamma$ for parameter selection due to the use of PL condition. 
If we assume the condition number $\lambda/L\leq \frac{1}{n^{1/3}}$ and choose $m = n^{1/3}$ and $\rho\leq \frac{1}{2}$, then the definition of $\gamma$ yields  
\begin{align}
\gamma &= 1-\frac{2\lambda\eta}{3} m-\frac{\lambda\eta}{3}\notag \\
& = 1 - \frac{2\lambda\rho}{3L}m - \frac{\lambda\rho}{3L}\notag\\
& \geq  1 - \frac{2\rho}{3n^{1/3}}m - \frac{\rho}{3n^{1/3}}\notag\\
& \geq  1 - \rho \geq \frac{1}{2}\label{eq54}
\end{align}
According to Theorem \ref{PL-Zoo}, equation \eqref{eq54} implies $\eta \leq \min\{\frac{1}{8L}, \frac{\sqrt{b}}{5\sqrt{2} m L \sqrt{d}}\}$. 
Hence, choosing $b = n^{2/3}$ leads to the constant step size  $\eta = \frac{1}{10 L \sqrt{d}}$.
Note that the assumption $\lambda/L \leq \frac{1}{{n}^{1/3}}$ is milder than the assumption $\lambda/L < \frac{1}{\sqrt{n}}$ in \cite{reddi2016proximal} for condition number.
\end{remark}
\section{Experimental results}

We present our empirical results in this section. We evaluate the following variance reduction algorithms for our experiments: 1) ZO-ProxSVRG \cite{Johnson12}, 2) ZO-ProxSVRG\cite{pedregosa2017breaking} and 3) ZO-ProxSGD on two applications: black-box binary classification and adversarial attacks on black-box deep neural networks
(DNNs). Note that the ZO-ProxSGD is obtained by combining RSPGF and CoordSGE \eqref{gradestcoord} for gradient estimation. It is shown in \cite{huang2019faster} that the stochastic gradiet method using CoordSGE shows better performance than counterparts based on RandSGE.


\subsection{Black-Box Binary Classification}
{\color{Melon}
In this subsection, we consider the following zeroth-order logistic regression problem with two classes 
\[
\min_{w\in\R^d} \frac{1}{n}\sum_{i=1}^n l(w'x_i, y_i)
\]
where $x_i\in\R^d$ denote the features, $y_i\in\{\pm 1\}$ are the classification labels, $l$ is the cross-entropy loss, and we
set $\alpha = 0.1$. For this problem, we use datasets indicated in Table \ref{}. 

As shown in Fig. 2, ZO-SVRG-Coord-Rand, ZO-SVRGCoord
and ZO-SPIDER-Coord converges much faster than
ZO-SGD , ZO-SVRG-Ave and SPIDER-SZO in terms of
number of iterations for both datasets. In terms of function
query complexity, ZO-SVRG-Coord converges much faster
than ZO-SVRG-Ave for both datasets and slightly faster
than ZO-SGD for ijcnn1 dataset, which corroborates our
new complexity analysis for ZO-SVRG-Coord. The convergence
and complexity performance of ZO-SPIDER-Coord
is similar to ZO-SVRG-Coord. Among these algorithms,
ZO-SVRG-Coord-Rand has the best function query complexity
for both datasets.
}
For the first set of our experiments, we study logistic regression loss function with $L_1$ and $L_2$ regularization {\color{Green} to learn the black-box binary classification problem}. The problem can be formulated as optimization problem \eqref{problem} with $f_i(x) = \log(1+e^{-y_iz^T_i{x}})$, $h(x) = \lambda_1\|{x}\|_1 + \frac{\lambda_2}{2}\|{x}\|^2$, where $z_i\in\R^d$ and $y_i$ is the corresponding label for each $i$. The $L_1$ regularization and $L_2$ regularization weights $\lambda_1$ and $\lambda_2$ are set $10^{-4}$ in all the experiments. The learning rates are tuned for competitive algorithms in our experiments, and the results shown in this section are based on the best learning rate for each algorithm we achieved.


We run our experiments on datasets from LIBSVM website{\footnote{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html}}, as shown in Table \ref{metadata}. The epoch size $m$ is chosen as {\color{red} 50} in all our experiments and we fix the minibatch size to $b = ?$. In ZO-PSVRG+, we set step sizes $\eta$ and $\mu$ to satisfy our assumptions in lemmas and theorems with $\eta\sim O(\frac{1}{\sqrt{d}})$.

\begin{table}[htbp]
\begin{center}
\caption{Summary of training datasets.}
\begin{tabular}{ c|c|c|c } 
 \hline
 Datasets &  Data & Features & Non-zeros \\ 
 \hline
  ijcnn & 49990 & 22 &  455,000\\
  a9a & 32561 & 123 & 7,521,450\\ 
 w8a & 64,700  & 300 & 2 \\ 
 mnist & 60000 & 784 &  50,233,657\\
%  \hline
% SUSY & 5,000,000 & 18 &  88,938,127 \\
 %epsilon &  400,000 & 2,000 &  800,000,000 \\
 %kdd12 & 119,705,032 & 54,686,452 & 0 \\
 \hline
\end{tabular}
\label{metadata}
\end{center}
\end{table}

{\color{Brown}
In Fig. \ref{}, we present the training loss against the number of epochs (i.e., iterations divided by the
epoch length $m = 50$) and function queries.
}
Since the computation complexity of each epoch of these
algorithms is different, in Figure \ref{fig:algo_comp} (bottom) we directly plot the objective value versus the number of zeroth order queries of these algorithms.
Results in Figure \eqref{fig:FSVRG_speedup} compare the performance of ZO-PSVRG+ with the variants of variance reduction stochastic gradient descent described earlier in this section.  
{\color{Brown}
Fig. 2-(a) presents the
convergence trajectories of ZO algorithms as functions of the number of epochs, where ZO-SVRG is evaluated under different mini-batch sizes $b \in \{1, 10, 40\}$. We observe that the convergence
error of ZO-PSVRG+ decreases as b increases, and for a small mini-batch size $b \leq 10$, ZO-PSVRG+
likely converges to a neighborhood of a critical point as shown by Corollary \ref{}. We also note that
our proposed algorithms ZO-PSVRG+($b = 40$) have faster
convergence speeds (i.e., less iteration complexity) than the existing variance reduced algorithms ZO-ProxSVRG and ZO-ProxSAGA.
}
It is seen that the performance of ZO-PSVRG+ outperforms other variants of SGD methods in all cases.
{\color{Green}
Figures \ref{} and \ref{} show that both objective values and test losses of the proposed methods faster
decrease than the ZO-ProxSGD method, as the time increases.
In particular ZO-PSVRG+ shows better performances than both ZO-ProxSVRG and ZO-ProxSAGA
using the CoordSGE . From these results, we
find that the ZO-PSVRG+ shows better performances. Moreover, these results also demonstrate that both the ZO-ProxSVRG and
ZO-ProxSAGA using the CoordSGE have a relatively faster
convergence rate than the counterparts using the GauSGE.
}
 It is observed that ZO-PSVRG+ always exhibits better convergence than other ZO-ProxSGD variants. In particular, compared to ZO-ProxSVRG, as an alternative ZO method, ZO-PSVRG+ shows significantly better convergence.  
As seen in the figure, ZO-PSVRG+ outperforms variance-reduced ZO-ProxSGD algorithms in all the cases. In particular compared to ZO-ProxSVRG and ZO-ProxSAGA, as ZO-PSVRG+ uses only a batch of size $B < n$ for calculation of full gradient, it has lower complexity per iteration and so shows better convergence speed.  
{\color{Brown}
Particularly, the use of $B = \min\{n, \frac{1}{\epsilon}\}$ in ZO-PSVRG+ significantly
accelerates ZO-PSVRG+ since the ZO-queries of order $O(n)$ is reduced to $O(\min\{n, \frac{1}{\epsilon}\})$) (see Table \ref{}), leading
to a non-dominant factor $O(I_{\{B < n\}}/B)$ in the convergence rate of ZO-PSVRG+.}

{\color{Brown} Fig. 2-(b)
presents the training loss against the number of function queries. For the same experiment, Table \ref{}
shows the number of iterations and the testing error of algorithms studied in Fig. \ref{}-(b) using $7.3 \times 10^6$ function queries. We observe that the performance of ZO-ProxSVRG degrades due to the need of large number of function queries to construct
coordinate-wise gradient estimates. By contrast, ZO-PSVRG+ yields better training results, while
ZO-ProxSGD consumes an extremely large number of iterations ($14600$ epochs). As a result, ZO-PSVRG+
($b = 40$) achieve better tradeoffs between the iteration and the function query complexity.
}




{\color{Green}
Since the ZO-ProxSAGA has less function query complexity
than the ZO-ProxSVRG, it shows the better performances
than the ZO-ProxSVRG. For example, the ZO-ProxSVRG-
CooSGE needs $O(ndS + bdT)$ function queries, while ZO-
SAGA-CoordSGE needs $O(bdT)$  function queries.}

\begin{figure*}[htbp]
\subfigure[comparison on ijcnn]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[{comparison on covtype}]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on real-sim]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on rcv1]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\setlength{\abovecaptionskip}{2pt}
\caption{Convergence performance of SVRG, SAGA, Katyusha, and AcPSVRG with single worker.}
\label{fig:AcPSVRG_seq}
\end{figure*}

\begin{figure*}[htbp]

\subfigure[comparison on ijcnn]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on covtype]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on real-sim]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on rcv1]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%

\subfigure[comparison on ijcnn]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on covtype]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on real-sim]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\subfigure[comparison on rcv1]{
\centering
\includegraphics[width=0.24\linewidth]{Figures/rcv1_obj_dev_comparison_time.eps}}%
\setlength{\abovecaptionskip}{2pt}
\caption{Training loss residual $f(x) - f(x^*)$ versus iteration (top) and time (bottom) plot of Async-SVRG, ProxASAGA, Async-Katyusha, and Async-AcPSVRG. }
\label{fig:algo_comp}
\end{figure*}

  
\iffalse
\begin{table}
\begin{center}
\caption{Parameter setting for algorithm implementation. Lmax denotes Lipschitz constant, n is the number of data, and $\rho$ and  $m_s$ are specified to 1.25 and n/4. }
\begin{tabular}{ c|c|c|c } 
 \hline
 Algorithms & initial $\eta$ & Inner Loops & $\lambda$  \\ 
 \hline
 SVRG & 1/(10*Lmax) & 2n & 1e-4\\
 SAGA & 1/(10*Lmax) & n & 1e-4\\
 %SVRG++ & 1/(7*Lmax) & 2n \\ 
 %SVRG-Prox & 1/(7*Lmax) & 2n \\
 Katyusha & Lmax & 2n & 1e-4 \\
 AcPSVRG & 1.5/Lmax &  $\lceil{\rho\cdot m_s}\rceil$ & 1e-4\\
 \hline
\end{tabular}
\label{par_setting}
\end{center}
\end{table}
\fi



\subsection{Adversarial Attacks on Black-Box DNNs}

{\color{Brown}
In image classification, adversarial
examples refer to carefully crafted perturbations such that, when added to the natural images, are
visually imperceptible but will lead the target model to misclassify. In the setting of "zeroth order"
attacks [2, 3, 35], the model parameters are hidden and acquiring its gradient is inadmissible. Only
the model evaluations are accessible. We can then regard the task of generating a universal adversarial
perturbation (to $n$ natural images) as an ZO optimization problem of the form (1).
{\color{Green}
More exactly,
we use the zeroth-order algorithms to find an universal adversarial perturbation $x\in\R^d$ that could fool the samples $\{a_i \in \R^d, l_i\in\mathbb{N} \}_{i=1}^n$, which can be specified as the following elastic-net attacks to black-box DNNs problem:
\begin{equation}
\min_{x\in\R^d} \frac{1}{n} \sum_{i=1}^n \max\{F_{l_i}(a_i+x) - \max_{j\neq l_i}F_j(a_i+x),0\} + \lambda_1 \norm{x}_{1} + \lambda_2 \norm{x}^{2}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are nonnegative parameters to balance attack success rate, distortion and sparsity. Here $F(a) = \left[F_1(a),\ldots,F_K(a)\right]\in [0, 1]^K$ represents the final layer output of neural network, which is the probabilities of $K$ classes.

}

We use a well-trained DNN 7 on the MNIST handwritten digit classification task as the target black-
box model, which achieves $99.4\%$ test accuracy on natural examples. Two ZO optimization methods,
ZO-SGD and ZO-SVRG-Ave, are performed in our experiment. Note that ZO-SVRG-Ave reduces
to ZO-SVRG when $q = 1$. We choose $n = 10$ images from the same class, and set the same
parameters $b = 5$ and constant step size $30/d$ for both ZO methods, where $d = 28 \times 28$ is the image
dimension. For ZO-SVRG-Ave, we set $m = 10$ and vary the number of random direction samples
$q \in \{10, 20, 30\}$.


{\color{Green}
In addition, we set $\lambda_1 = 10^{-3}$ and $\lambda_2 = 1$ in the experiment.
}
}


{\color{Green}

Figure \ref{} shows that both objective values and black-box attack losses (i.e. the first part of the problem \eqref{}) of the proposed algorithms faster decrease than the RSPGF method, as the number of iteration increases. Here, we add ZO-ProxSGD-CooSGE method for comparison, which is obtained by combining the ZO-ProxSGD method with 
CooSGE. Interestingly, ZO-PSVRG+ shows better performance than both ZO-ProxSVRG and ZO-ProxSAGA. Although having a relatively good performance in generating the adversarial samples, ZO-ProxSGD still shows worse performance than both the ZO-ProxSVRG and ZO-ProxSAGA, due to not using the VR technique.}

{\color{Brown}
{
\color{Melon}
In image classification, adversary attack crafts input images
with imperceptive perturbation to mislead a trained classifier.
The resulting perturbed images are called adversarial examples,
which are commonly used to understand the robustness
of learning models. In the black-box setting, the attacker can
access only the model evaluations, and hence the problem
falls into the framework of zeroth-order optimization. We use a well-trained DNN
$F(\cdot) = [F_1(\cdot), \ldots, F_K(\cdot)]$ for
the MNIST handwritten digit classification as the target
black-box model, where $F_k(\cdot)$ returns the prediction score
of the $k^{th}$ class. We attack a batch of $n$ correctly-classified
images $\{a_i\}_{i=1}^n$ from the same class, and adopt the same
black-box attacking loss as in \cite{Chen et al. 2017; Liu et al.
2018b.}. The $i^{th}$ individual loss function $f_i(x)$ is given by
\begin{equation}
\begin{split}
f_i(x) =& \max \{\log F_{y_i} (a_i^{adv}) - \max_{t\neq y_i}\log F_{t} (a_i^{adv}), 0 \}\\
&+ \lambda \norm{a_i^{adv} - a_i}^2
\end{split}
\end{equation}
where $a_i^{adv}  = 0.5 \tanh(\tanh^{-1}(2a_i)+x)$ is the adversarial example of the $i^{th}$ natural image $a_i$, and $y_i$ is the true label of image $a_i$. I our experiments, we set the regularization parameter $\lambda  = 1$ for digit 1 image class, and set $\lambda = 0.1$ for digit 4 class. 

Fig. 1 and Fig. 3 (in the supplementary materials) provide
comparison of the performance for the algorithms of interest.
Two major observations can be made. First, our proposed
two algorithms ZO-SVRG-Coord-Rand and ZO-SPIDER-Coord as well as ZO-SVRG-Coord (with the large stepsize due to our improved analysis) have much better performance
both in convergence rate (iteration complexity) and
function query complexity than ZO-SGD, ZO-SVRG-Ave
and SPIDER-SZO. Among them, ZO-SVRG-Coord-Rand
achieves the best performance. Second, our ZO-SPIDERCoord
algorithm converges much faster than SPIDER-SZO
in the initial optimization stage, and more importantly, has
much lower function query complexity, which is largely
due to the $\epsilon$-level stepsize required by SPIDER-SZO. In
addition, we present the generated adversarial examples for
attacking digit “4” class in Table 3 in the supplementary
materials, where our ZO-SVRG-Coord-Rand achieves the
lowest image distortion.
}

 In Fig. \ref{}, we show the black-box attack loss (against the number of epochs) as well as the least $l_2$ distortion of the successful (universal) adversarial perturbations. To reach the same
attack loss (e.g., $7$ in our example), ZO-SVRG-Ave requires roughly $30\times (q = 10)$, $77\times (q = 20)$
and $380\times (q = 30)$ more function evaluations than ZO-SGD. The sharp drop of attack loss in each
method could be caused by the hinge-like loss as part of the total loss function, which turns to $0$ only
if the attack becomes successful. Compared to ZO-ProxSGD, ZO-PSVRG+ offers a faster convergence
to a more accurate solution, and its convergence trajectory is more stable as $q$ becomes larger (due to
the reduced variance of Avg-RandGradEst). In addition, ZO-PSVRG+ improves the $l_2$ distortion
of adversarial examples compared to ZO-ProxSGD (e.g., $30\%$ improvement when $q = 30$). 
}
\section{Appendix}
{\color{Green}
In this section, we provide the detailed proofs of the above lemmas and theorems. First, we give some useful properties of the CooSGE and the GauSGE, respectively.
}
\begin{lemma}[Three-Point Property] Let $F(\cdot)$ be a convex function, and let $D_{l}(\cdot,\cdot)$ be the Bregman distance for $l(\cdot)$. For a given vector $z$, let 
\[
z^+ = \text{arg}\,\,\min_{x\in\R^d}\{F(x)+D_{l}(x,z)\}.
\]
Then 
\begin{equation}
F(x) + D_l(x,z) \geq F(z^+) + D_l(x^+,z) + D_l(x,z^+)\qquad for\,\,all\,\,x\in\R^n
\end{equation}
with equality holding in the case when $F(\cdot)$ is a linear function and $l(\cdot)$ is a quadratic function.
\end{lemma}


\begin{lemma}\label{CooSGE}
Assume that the function $f(x)$ is $L$-smooth. Let $\hat{\nabla} f(x)$ denote the estimated gradient defined by CoordSGE. Define $f_{\mu_j} = \E_{u\sim U[\mu_j, \mu_j]} f(x+ue_j)$, where $U[-\mu_j,\mu_j]$ denotes the uniform distribution at the interval $[\mu_j, \mu_j]$. Then we have 
1) $f_{\mu_j}$ is $L$-smooth, and 
\begin{equation}
\hat{\nabla} f(x) = \sum_{j=1}^d \frac{\partial f_{\mu_j}}{\partial x_j}e_j
\end{equation} 
where $\partial f/\partial x_j$ denotes the partial derivative with respect to $j$th coordinate.

2) For $j\in [d]$, 
\begin{align}
\abs{f_{\mu_j}(x) - f(x)} &\leq \frac{L\mu_j^2}{2}\\
\abs{\frac{\partial f_{\mu_j}(x)}{\partial x_j}} \leq \frac{L\mu_j^2}{2}
\end{align}
 
 3) If $\mu = \mu_j$ for $j\in [d]$, then 
 \begin{equation}
 \norm{\hat{\nabla} f(x) - {\nabla} f(x)} ^2 \leq \frac{L^2 d^2 \mu^2}{4}
\end{equation}  
\end{lemma}
{\color{Melon}
\begin{lemma}
For any given smoothing parameter $\delta > 0$ and any $x \in\R^d$, we have 
 \begin{equation}
 \norm{\hat{\nabla} f(x) - {\nabla} f(x)} ^2 \leq \frac{L^2 d \mu^2}{4}
\end{equation}
\end{lemma}
\begin{proof}
Applying the mean value theorem to the gradient $\nabla f(x)$, we have for any given $\mu > 0$,
 \begin{equation}
 \begin{split}
 \norm{\hat{\nabla} f(x) - {\nabla} f(x)} ^2 &= \norm{\frac{1}{2\mu}\sum_{j=1}^d(\frac{\partial f(x+(2t_j-1)\mu e_j)}{\partial x_j})e_j - \nabla f(x)}^2, \text{\,\,for $0 < t_j < 1$}\\
 & = \sum_{j=1}^d\norm{(\frac{\partial f(x+(2t_j-1)\mu e_j)}{\partial x_j})e_j - \nabla f(x)}^2\\
 & = \sum_{j=1}^d\norm{\nabla f(x+(2t_j-1)\mu e_j) - \nabla f(x)}^2\\
 &\leq L^2 \sum_{j=1}^d \norm{(2t_j-1)\mu e_j}^2 \leq L^2 d \mu^2
 \end{split}
\end{equation}
where the first inequality follows from the definition of $e_j$ and Euclidean norm, and the second inequality is by the Lipschitz continuity of $f$.
\end{proof}
}
\begin{lemma}\label{lemma1}
Let $x^+ = \Po_{\eta h}(x-\eta v)$, then the following inequality holds:
\begin{equation}\label{eq10}
F(x^+) \leq F(z) + \Iprod{\nabla f(x)-v}{x^+-z}-\frac{1}{\eta} \Iprod{x^+-x}{x^+-z}+\frac{L}{2}\norm{x^+-x}^2+\frac{L}{2}\norm{z-x}^2, \forall z\in \R^d. 
\end{equation}
\end{lemma}
\begin{proof}
First, we recall the proximal operator 
\begin{equation}\label{eq11}
\Po_{\eta h}(x-\eta v) := \text{arg}\,\,\min_{y\in\R^d}\left(h(y)+\frac{1}{2\eta}\norm{y-x}^2+\Iprod{v}{y}\right)
\end{equation}
For the nonsmooth function $h(x)$, we have 
\begin{equation}\label{eq12}
\begin{split}
h(x^+) &\leq h(z) + \Iprod{p}{x^+-z}\\
&= h(z) - \Iprod{v+\frac{1}{\eta}(x^+-x)}{x^+-z}
\end{split}
\end{equation}
where $p\in \partial h(x^+)$ such that $p+\frac{1}{\eta}(x^+-x)+v = 0$ according to the optimality condition of \eqref{eq11}, and \eqref{eq12} due to the convexity of $h$.
\begin{equation}\label{eq14}
f(x^+) \leq f(x)+\Iprod{\nabla f(x)}{x^+-x}+\frac{L}{2}\norm{x^+-x}^2
\end{equation}
\begin{equation}\label{eq15}
-f(z) \leq -f(x)+\Iprod{-\nabla f(x)}{z-x}+\frac{L}{2}\norm{z-x}^2
\end{equation}
where \eqref{eq14} holds since $f(x)$ has $L$-Lipschitz continuous gradient, and \eqref{eq15} holds since $-f(x)$ has the same $L$-Lipschitz continuous gradient as $f(x)$. 

 This lemma is proved by adding \eqref{eq12}, \eqref{eq14}, \eqref{eq15}, and recalling $F(x) = f(x)+h(x)$. 
\end{proof}
\begin{lemma}\label{lemm-est-grad}
Let  $x_{t}^s= \Po_{\eta h}(x_{t-1}^s - \eta \hat{v}_{t-1}^s)$ and $\overline{x}_{t}^s= \Po_{\eta h}(x_{t-1}^s - \eta \nabla f(x_{t-1}^s))$. Then the following inequality holds
\[
\Iprod{\nabla f(x_{t-1}^s) -\hat{v}_{t-1}^s)}{x_t^s -\overline{x}_t^s} \leq \eta\norm{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}^2
\]
\end{lemma}
\begin{proof}
First, we obtain $\norm{x_{t}^s-\overline{x}_{t}^s}$ and $\norm{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}$ as follows 
\begin{align}
h(x_t^s)&\leq h(\overline{x}_t^s) - \Iprod{\hat{v}_{t-1}^s+\frac{1}{\eta}(x_t^s-x_{t-1}^s)}{x_t^s-\overline{x}_t^s}\label{lemm-est-grad-1}\\
h(\overline{x}_t^s)&\leq h({x}_t^s) - \Iprod{\nabla f(x_{t-1}^s)+\frac{1}{\eta}(\overline{x}_t^s-x_{t-1}^s)}{\overline{x}_t^s-x_t^s}\label{lemm-est-grad-2}
\end{align}
where \eqref{lemm-est-grad-1} and \eqref{lemm-est-grad-2} hold due to \eqref{eq12}. Adding \eqref{lemm-est-grad-1} and \eqref{lemm-est-grad-2}, we have 
\begin{align}
\frac{1}{\eta}\Iprod{x_t^s -\overline{x}_t^s}{x_t^s -\overline{x}_t^s} &\leq \Iprod{\nabla f(x_{t-1}^s) -\hat{v}_{t-1}^s}{x_t^s -\overline{x}_t^s}\notag\\
\frac{1}{\eta}\norm{x_t^s -\overline{x}_t^s}^2 &\leq \norm{\nabla f(x_{t-1}^s) -\hat{v}_{t-1}^s}\norm{x_t^s -\overline{x}_t^s}\label{lemm-est-grad-3}\\
\norm{x_t^s -\overline{x}_t^s} &\leq \eta\norm{\nabla f(x_{t-1}^s) -\hat{v}_{t-1}^s}\label{lemm-est-grad-4}
\end{align}
where \eqref{lemm-est-grad-3} uses Cauchy-Schwarz inequality. Now this lemma is proved using Cauchy-Schwarz inequality and \eqref{lemm-est-grad-4}, i.e., $\Iprod{\nabla f(x_{t-1}^s) -\hat{v}_{t-1}^s)}{x_t^s -\overline{x}_t^s} \leq \norm{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s} \norm{x_t^s -\overline{x}_t^s} \leq \eta\norm{\nabla f(x_{t-1}^s)-\hat{v}_{t-1}^s}^2$.
\end{proof}


\bibliographystyle{plainnat}
\bibliography{GTA}
\end{document}

