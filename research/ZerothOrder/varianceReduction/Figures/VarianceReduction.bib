
@string{aaai = {Proceedings of \ AAAI Conference on Artificial Intelligence}}

@string{colt = {Proceedings \ Annual Conf.\ on Learning Theory}}

@string{cvpr = {Proc.\ CVPR}}

@string{ecml = {Proc.\ European Conf.\ on Mach. Learn.}}

@string{ewcbr = {Proc.\ the European Workshop on Advances in Case-Based Reasoning}}

@string{fg = {Proc.\ Int.\ Conf.\ on Face and Gesture Recognition}}

@string{focs = {IEEE Symposium on Foundations of Computer Science}}

@string{iccv = {Proc.\ Int.\ Conf.\ on Computer Vision}}

@string{icdm = {Proc.\ Int.\ Conf.\ on Data Mining}}

@string{icml = {Proceedings of the International Conference on Machine Learning}}

@string{icpr = {Proc.\ Int.\ Conf.\ on Pattern Recognition}}

@string{icvp = {Proc.\ Int.\ Conf.\ Vision and Pattern Recognition}}

@string{ijcai = {Proc.\ Int.\ Joint Conf.\ on Artificial Intelligence}}

@string{ijcv = {Int.\ J.\ of Computer Vision}}

@string{kdd = {Proc.\ ACM SIGKDD Int.\ Conf.\ on Knowledge Discovery and Data Mining}}

@string{mm = {Proc.\ ACM Multimedia}}

@string{nips = {Proc.\ Conf.\ on Advances in Neural Information Processing Systems}}

@string{pami = {IEEE Trans.\ Pattern Anal.\ Mach.\ Intell.}}

@string{pcm = {Proc.\ Pacific-Rim Conf.\ on Multimedia}}

@string{sigir = {Proc.\ ACM SIGIR}}

@string{tvcg = {IEEE Trans. Visualization and Computer Graphics}}

@string{uai = {Proc.\ Uncertainty in Artificial Intelligence}}

@inproceedings{Agarwal2014,
  title={Distributed delayed stochastic optimization},
  author={Agarwal, Alekh and Duchi, John C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={873--881},
  year={2011}
}

@inproceedings{Allen-Zhu2016,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={ICML},
  pages={699--707},
  year={2016}
}


@inproceedings{Allen-Zhu2016I,
  title={Improved SVRG for non-strongly-convex or sum-of-non-convex objectives},
  author={Allen-Zhu, Zeyuan and Yuan, Yang},
  booktitle={ICML},
  pages={1080--1089},
  year={2016}
}

@article{Allen-Zhu17,
  title={Katyusha: The first truly accelerated stochastic gradient method},
  author={Allen-Zhu, Zeyuan},
  journal={ArXiv e-prints, abs/1603.05953},
  year={2016}
}

@inproceedings{Defazio2014,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}

@article{Fercoq2015,
  title={Accelerated, parallel, and proximal coordinate descent},
  author={Fercoq, Olivier and Richt{\'a}rik, Peter},
  journal={SIAM Journal on Optimization},
  volume={25},
  number={4},
  pages={1997--2023},
  year={2015},
  publisher={SIAM}
}

@article{Hazan2011,
  title={An optimal algorithm for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={arXiv preprint arXiv:1006.2425},
  year={2010}
}

@inproceedings{Hu2009,
  title={Accelerated gradient methods for stochastic optimization and online learning},
  author={Hu, Chonghai and Pan, Weike and Kwok, James T},
  booktitle={Advances in Neural Information Processing Systems},
  pages={781--789},
  year={2009}
}

@inproceedings{Johnson12,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@article{Konecny2016,
  title={Mini-batch semi-stochastic gradient descent in the proximal setting},
  author={Kone{\v{c}}n{\`y}, Jakub and Liu, Jie and Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={10},
  number={2},
  pages={242--255},
  year={2016},
  publisher={IEEE}
}


@article{Langford2009,
  title={Sparse online learning via truncated gradient},
  author={Langford, John and Li, Lihong and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Mar},
  pages={777--801},
  year={2009}
}


@inproceedings{Li2014,
  title={Scaling Distributed Machine Learning with the Parameter Server.},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={OSDI},
  volume={14},
  pages={583--598},
  year={2014}
}

@inproceedings{Lin2015,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3384--3392},
  year={2015}
}

@article{LiP2016,
  title={Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic Gradient Descent},
  author={Li, Yitan and Xu, Linli and Zhong, Xiaowei and Ling, Qing},
  journal={arXiv preprint arXiv:1605.06619},
  year={2016}
}

@inproceedings{Li2016,
  title={Stochastic variance reduced optimization for nonconvex sparse learning},
  author={Li, Xingguo and Zhao, Tuo and Arora, Raman and Liu, Han and Haupt, Jarvis},
  booktitle={ICML},
  pages={917--925},
  year={2016}
}





@article{Meng2016Spark,
  title={Mllib: Machine learning in apache spark},
  author={Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and others},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1235--1241},
  year={2016},
  publisher={JMLR. org}
}


@inproceedings{Meng2016,
  title={Asynchronous Accelerated Stochastic Gradient Descent.},
  author={Meng, Qi and Chen, Wei and Yu, Jingcheng and Wang, Taifeng and Ma, Zhiming and Liu, Tie-Yan},
  booktitle={IJCAI},
  pages={1853--1859},
  year={2016}
}

@inproceedings{Meng2017,
  title={Asynchronous Stochastic Proximal Optimization Algorithms with Variance Reduction.},
  author={Meng, Qi and Chen, Wei and Yu, Jingcheng and Wang, Taifeng and Ma, Zhiming and Liu, Tie-Yan},
  booktitle={AAAI},
  pages={2329--2335},
  year={2017}
}

@article{Nemirovski2009,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@book{Nesterov2004,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer}
}

@inproceedings{Nitanda2014,
  title={Stochastic proximal gradient descent with acceleration techniques},
  author={Nitanda, Atsushi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1574--1582},
  year={2014}
}

@inproceedings{Rakhlin2012,
  title={Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization.},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik and others},
  booktitle={ICML},
  year={2012},
  %organization={Citeseer}
}

@inproceedings{Recht2011,
  title={Hogwild: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  booktitle={Advances in neural information processing systems},
  pages={693--701},
  year={2011}
}

@inproceedings{Reddi2015,
  title={On variance reduction in stochastic gradient descent and its asynchronous variants},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alexander J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2647--2655},
  year={2015}
}

@inproceedings{Shalev-Shwartz2014,
  title={Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  booktitle={ICML},
  pages={64--72},
  year={2014}
}

@inproceedings{Shen2016,
  title={Adaptive Variance Reducing for Stochastic Gradient Descent.},
  author={Shen, Zebang and Qian, Hui and Zhou, Tengfei and Mu, Tongzhou},
  booktitle={IJCAI},
  pages={1990--1996},
  year={2016}
}

@article{Xiao2014,
  title={A proximal stochastic gradient method with progressive variance reduction},
  author={Xiao, Lin and Zhang, Tong},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={2057--2075},
  year={2014},
  publisher={SIAM}
}

@inproceedings{Zhao2014,
  title={Accelerated mini-batch randomized block coordinate descent method},
  author={Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},
  booktitle={Advances in neural information processing systems},
  pages={3329--3337},
  year={2014}
}

@article{Mania2017,
  title={Perturbed iterate analysis for asynchronous stochastic optimization},
  author={Mania, Horia and Pan, Xinghao and Papailiopoulos, Dimitris and Recht, Benjamin and Ramchandran, Kannan and Jordan, Michael I},
  journal={arXiv preprint arXiv:1507.06970},
  year={2015}
}

@inproceedings{Zhao2015,
  title={Stochastic optimization with importance sampling for regularized loss minimization},
  author={Zhao, Peilin and Zhang, Tong},
  booktitle={ICML},
  pages={1--9},
  year={2015}
  
  @inproceedings{pedregosa2017breaking,
  title={Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization},
  author={Pedregosa, Fabian and Leblond, R{\'e}mi and Lacoste-Julien, Simon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={55--64},
  year={2017}
}


@article{shang2018guaranteed,
  title={Guaranteed Sufficient Decrease for Stochastic Variance Reduced Gradient Optimization},
  author={Shang, Fanhua and Liu, Yuanyuan and Zhou, Kaiwen and Cheng, James and Ng, Kelvin KW and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.09933},
  year={2018}
}
@article{fang2018accelerating,
  title={Accelerating Asynchronous Algorithms for Convex Optimization by Momentum Compensation},
  author={Fang, Cong and Huang, Yameng and Lin, Zhouchen},
  journal={arXiv preprint arXiv:1802.09747},
  year={2018}
}
