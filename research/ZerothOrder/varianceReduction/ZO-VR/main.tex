%\documentclass{article}
\documentclass[10pt, conference, compsocconf]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage{mathptmx}  
\usepackage[scaled=.92]{helvet}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage[numbers]{natbib}  
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}  %%check
%%%Algorithms
\usepackage[noend]{algpseudocode}
\usepackage{algorithm,amsmath}
\usepackage{mathtools}
\usepackage{epstopdf}
\usepackage{epsfig}
\usepackage[export]{adjustbox} %for large figures
\usepackage[tight,footnotesize]{subfigure}
\usepackage{caption}
\usepackage{subcaption}


\newcommand*{\R}{\mathbb{R}}
\newcommand*{\Po}{\text{Prox}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\VRG}{\,\widetilde{\nabla}_k^s}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\Iprod}[2]{\left\langle #1,#2\right\rangle}

\renewcommand{\algorithmicrequire}{
\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\Initialize}{\textbf{Initialize:}{\,}}
\newcommand{\Input}{\textbf{Input:}{\,}}
\newcommand{\Output}{\textbf{Output:}{\,}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{question}[theorem]{Question}
\newtheorem{corollary}[theorem]{Corollary} 
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{statement}[theorem]{Statement}
\newtheorem{notation}[theorem]{Notation} 
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{Assumption}{Assumption}



\title{Distributed accelerated proximal optimization algorithms with variance reduction}
\date{March 2018}

\begin{document}

\maketitle
\begin{abstract}
    The proximal stochastic gradient descent (ProxSGD) has been widely used to solve composite convex optimization problems. However, the random sampling in ProxSGD introduces noisy gradient updates with high variance, which makes that to use a vanishing step size and so slows down the convergence as the iterates approach the optimum.  In addition, although ProxSGD with variance-reduction enjoyed great success in applications at small and moderate scales, but distributed version of these algorithms is crucial requirement in the availability of larger-scale training datasets. In this paper, we propose a synchronous method, Sync-AcPSVRG, and an asynchronous method, ASync-AcPSVRG, that integrate variance-reduction and momentum acceleration techniques in a distributed manner to speed up  ProxSGD gradient updates and scale near linearly with the number of workers. AcPSVRG enjoys lower iteration  complexity than existing  accelerated  stochastic  variance  reduction  methods, which need more updates per iteration. Furthermore, we allow the number of updates to grow with the epochs which decreases the number of communication frequencies between local node and master node. We compared distributed AcPSVRG with modern parallel stochastic proximal optimization methods on a few datasets. The empirical results verified our theoretical findings and indicated that our proposed accelerated methods with variance reduction exhibit highly efficient and effective algorithms than other ProxSGD variants.
\end{abstract}


\section{Introduction}

In this paper, we consider the following composite optimization problem 
\begin{equation}\label{problem}
    \min_{x\in\R^d}\,F(x) = h(x) + f(x)= h(x) + \frac{1}{n} \sum_{i=1}^n f_i(x),
\end{equation}
where $f_i(x)$, $i=1,2,\ldots,n$, are smooth convex loss functions, and $h(x)$ is a non-smooth regularization term. Proximal stochastic gradient descent (ProxSGD) \cite{Nemirovski2009} is a general approach for solving minimization problem \eqref{problem} which employs the fact that the objective function decomposes into a sum over many terms. 

For problems where each $f_i$ in \eqref{problem} corresponds to a single data observation, ProxSGD selects a single data index $i_k$ on each iteration $k$, and then performs update by solving the proximal optimization subproblem
\[
x^{k+1} = \Po\{x^{k} -\eta \nabla f_{i_k} (x_k)\},
\]
and the proximal mapping is defined as 
\[
\Po_{\eta}R(y) = \displaystyle{\it argmin}_{x\in\R^d}\{\frac{1}{2\eta}\norm{x-y}^2+R(x)\}
\]
where $\norm{\cdot}$ is the $L_2$-norm and $\eta$ is the step size.
Typically, $i_k$ is chosen uniformly at random from $\{1, 2,\ldots, n\}$ on each iteration $k$, thus making the gradient approximation unbiased. 

However, because random sampling in ProxSGD introduces variance, vanishing step sizes are needed to guarantee the algorithm's convergence,
and the convergence rate is only sublinear \cite{Langford2009, Rakhlin2012}. In \cite{Xiao2014}, a variance reduction technique for proximal algorithms was introduced and it is proved that it can achieve linear convergence. The convergence rate of ProxSGD with variance reduction can be further improved with the so-called momentum acceleration technique \cite{Allen-Zhu17}.

Another major drawback of ProxSGD is the sequential nature of algorithm. For massive datasets, parallel or distributed algorithms are sorely needed, making more practical impacts on parallel SGD variants to solve large-scale problems. Furthermore, in some applications, the training dataset might be distributed over a cluster of multiple nodes. There is quite a bit of recent works in the area of distributed optimization which have been successfully applied to accelerate many optimization algorithms including SGD \cite{Agarwal2014,Recht2011,Mania2017} and ProxSGD \cite{LiP2016,Meng2017}.   

In this paper, we propose to enhance the scalability of proximal algorithms using variance reduction and momentum acceleration techniques, yielding ProxSGD methods which scale linearly and can provide better convergence rate.

\section{Backgrounds}
Since SGD estimates the gradient from only one or a few samples, the variance of the stochastic gradient estimator may be large \cite{Johnson12,Zhao2015}, which leads to slowdowns and poor performance. Recently there has been a lot of interest in methods which reduce the variance incurred due to stochastic
gradients. Variance reduction (VR) methods \cite{Johnson12,Defazio2014} reduce the variance in the stochastic gradient estimates, and are able to alleviate the effects of vanishing step sizes that usually hit SGD, and yield methods which improve convergence rates both theoretically and empirically. 
Inspired by the success of these methods in reducing the gradient noise in stochastic gradient base optimizations, recently many variants of variance reduction methods have been proposed \cite{Konecny2016,Li2016,Shen2016,Xiao2014,Allen-Zhu2016}. Variance reduction methods begin with an initial estimate $\widetilde{x}$, and then generates a sequence of iterates $x_k$ from
\[x_k = x_{k-1} - \eta\left[\nabla f_{i_k}(x_{k-1})-f_{i_k}(\widetilde{x}) + \widetilde{g}(\widetilde{x})\right]
\]
where $\eta$ is the step size, $\widetilde{g}(\widetilde{x})$ is an approximation of the true gradient, and $\widetilde{x}$ is chosen to be a recent iterate from algorithm history. In particular, an error correction term is subtracted from regular update rules in stochastic optimizations to reduce the variance of gradients in order to deal with the problems of instability and slower convergence hit SGD.


Recently several acceleration techniques based on momentum compensations were introduced to further speed up the VR methods mentioned above \cite{Hu2009,Lin2015, Nitanda2014,Allen-Zhu17}. The momentum acceleration technique, which is basically proposed by Nesterov \cite{Nesterov2004} for gradient methods, introduces one or multiple auxiliary variables to record the momentum, and updates the parameter according to the gradient at the auxiliary variable. To be specific, with momentum acceleration, the update rule becomes
\begin{equation}
\begin{split}
x_{k+1}& = z_k-\eta \nabla f_{i_k}(z_k),\\
z_{k+1}& = x_{k+1} + \beta (x_{k+1}-x_{k}),
\end{split}
\end{equation}
where $\beta$ is the momentum weight. It can be proven that the convergence rate of SGD
is improved by using the Nesterov acceleration technique \cite{Hu2009}.
After that, many accelerated algorithms have been designed to achieve faster convergence rates for stochastic optimization methods \cite{Shalev-Shwartz2014,Allen-Zhu17,Fercoq2015,Lin2015}.

Although these acceleration techniques have great value in general, for large-scale problems, however, with the availability of very big training data, sequential SGD is very inefficient. Therefore, integrating the VR algorithms and acceleration techniques to distributed settings remain indispensable. In \cite{Mania2017, Reddi2015} SVRG is extended to the parallel asynchronous setting. In particular, in \cite{Meng2016} a parallel algorithm mitigated by variance reduction, coordinate sampling, and Nesterovâ€™s method is introduced. In this paper, we introduce distributed synchronous and asynchronous SGD-based algorithms with variance reduction integrated with acceleration techniques, which are not well studied in the literature, to the best of our knowledge. We proved that the proposed distributed algorithms have desirable convergence property and perform empirical investigations on it. The parallel algorithms are easy to implement, highly scalable, use a constant learning rate, and converges linearly to the optimal solution in the strongly convex case.

\section{Contributions}
In this work, we attempt to introduce momentum to accelerate distributed variance reduction SGD algorithms for convex problems. We do this by demonstrating that one step of momentum with only one weight provides a faster convergence rate in expectation, while we assume an upper bound for delay $\tau$ between delayed gradient and the latest one. 

By applying this technique in distributed stochastic algorithms, we allow many local nodes to run simultaneously, while communicating 
with the server node through the exchange of updates. This new algorithm allows many asynchronous processes to work towards a central solution which are
faster by order than existing variance-reduced ProxSGD algorithms. 

This work has three main contributions:

1. We propose accelerated variance reduction technique with the momentum acceleration technique with one momentum weight, built on \cite{Allen-Zhu17}, for distributed algorithms for convex problems. We show that synchronous and asynchronous variations of proposed algorithm can scale ({\color{red} near}) linearly with the number of processors. 

2. We theoretically study the convergence for general proximal algorithm and obtained improved convergence rate for convex and strongly convex functions.
The analysis could be applied to other stochastic asynchronous algorithms like ASCD.
 
3. Finally, we present several empirical results over several distributed VR algorithms to demonstrate that the new accelerated algorithm 
can lead to better performance improvements, which agrees with the theory, than competing options.
\section{Single-Worker Accelerated ProxSVRG}
We begin by proposing our new accelerated proximal variance reduction scheme with momentum acceleration (AcPSVRG), in the single-worker case. 
As we will see later, the proposed method has a natural generalization to the distributed setting that has low communication requirement. Similar to the epoch gradient descent algorithm \cite{Hazan2011}, we increase the number of iterations by a constant $\gamma$ for every epoch. 
\subsection{Algorithm Overview}
Our proposed VR scheme is divided into $S$ epochs, with $m_s$ updates taking place in each epoch.  Throughout the paper, we will
use the superscript for the index of each epoch, and the subscript for the index of iterations within each epoch.

Let the iterates generated in the $s$-th epoch be written as $\{x_k^s\}_{k=0}^{m_s-1}$. The update rule for AcPSVRG can be formulated as follows: 

In each iteration the following type of proximal subproblem is solved,
\begin{equation}
z_{k+1}^s = \Po_{\frac{\eta}{\beta_s}} h \left(z_{k}^s - \frac{\eta}{\beta_s}\widetilde{\nabla}_k^s\right),
\end{equation}
where
\begin{equation}\label{nabla-update}
\widetilde{\nabla}_k^s = \nabla f_{i_k^s}(x_k^s) - \nabla f_{i_k^s}(\widetilde{x}^s) + \nabla f(\widetilde{x}^s).
\end{equation}
We let snapshot $\widetilde{x}^s$ be a weighted average of $x_k^{s}$ in the most recent epoch $s$, which is updated at the end of each epoch, i.e., after every $m_s$ parameter updates. We compensate the momentum term and introduce a new extrapolation rule to update $x_k^{s}$,
\[
x_{k+1}^s = \widetilde{x}^s+\beta_s(z_{k+1}^s-\widetilde{x}^s)
\]
where $\beta_s \in [0, 1]$ is the momentum weight. Indeed, $z_{k+1}^s$ is a momentum which adds a weighted sum of gradient history into $x_k^{s}$. Comparably, this update has only one momentum weight $\beta_s$ versus two weights $\tau_1$ and $\tau_2$ in \cite{Allen-Zhu17} through introducing two momentum vectors.  
Similar to \cite{Hazan2011}, we increase the number of
iterations by a constant $\gamma$ for every epoch, i.e., $m_{s+1} = \gamma m_{s}$.
The choice of growing the iterations per epoch can reduce the number of full gradient calculations and the frequency of communication between
the server and the local nodes in the distributed setting, while it can speed up the convergence \cite{Allen-Zhu2016I}.

Note that if $i_k$ in \eqref{nabla-update} is chosen uniformly at random from the set $\{1,2,\ldots,n\}$ on each iteration $k$, then, conditioning on all history,
\[
\E[\nabla f_{i_k^s}(\widetilde{x})] = \nabla f(\widetilde{x}).
\] 
Thus, the error correction term has expected value 0, and $\E[\widetilde{\nabla}_k^s] = \nabla f(x_{k}^s)$, i.e., the approximate gradient $\widetilde{\nabla}_k^s$ is unbiased.

\subsection{Algorithm Details for AcPSVRG}
The detailed steps of AcPSVRG are listed in Algorithm \ref{VR-Algo-Single}. Note, the number of updates per epoch $m_s$ and $\widetilde{x}^s$ are initialized with $m_1$ and $\widetilde{x}^0$. At the beginning of each epoch we initialization $x_0^s=z_0^s=\widetilde{x}^{s-1}$, where $\widetilde{x}^{s-1}$ is the average of the past $m_{s-1}$ stochastic iterations.

AcPSVRG builds on the Katyusha momentum compensation method \cite{Allen-Zhu17}, but with the lower space complexities. Namely, on every iteration, one gradient computation is required and one optimization subproblem solved with one auxiliary variable. Furthermore, at the end of each epoch, $n$ gradients $\{\nabla f_i (\widetilde{x})\}_{1}^{n}$ also should be calculated.

\begin{algorithm}[H]
\caption{AcPSVRG}\label{VR-Algo-Single}
\begin{algorithmic}
\State\Input The number of epochs $S$ and the step size $\eta$.
\State\Initialize $\widetilde{x}^0$, $m_1$, $\theta_1$, and $\rho > 1$.
\For{ $s=1$ {\bf to} $S$ }
\State $\widetilde{x} = \widetilde{x}^{s-1}$;
\State  $\nabla f(\widetilde{x}) = \frac{1}{n}\sum_{i=1}^n\nabla f_i(\widetilde{x})$;
 \State $x_0^s = z_0^s = \widetilde{x}$;
 \For{$k=0$ {\bf to} $m_s-1$}
 \State Pick $i_k^s$ uniformly at random from $\{1,\ldots,n\}$;
 \State $\widetilde{\nabla}_k^s = \nabla f_{i_k^s}(x_{k}^s) - \nabla f_{i_k^s}(\widetilde{x}) + \nabla f(\widetilde{x})$;
 \State $\delta_k^s = \text{argmin}_{\delta}\,\,h(z_k^s+\delta)+\langle\widetilde{\nabla}_k^s,\delta\rangle + \frac{{\color{red}\beta_s}}{2\eta}\norm{\delta}^2$;
 \State $z_{k+1}^s = z_{k}^s + \delta_k^s$;
 \State $x_{k+1}^s = \widetilde{x}+\beta_s(z_{k+1}^s-\widetilde{x})$;
 \EndFor
$\widetilde{x}^s = \frac{1}{m_s}\sum_{k=0}^{m_s-1} x_{k}^s$, $m_{s+1} = \gamma m_s$
 \EndFor
 \State\Output $\widetilde{x}^S$
\end{algorithmic}
\end{algorithm}

\section{Distributed Algorithms}

We now consider the distributed setting, on a master-slave framework with a master machine and $p$ local clients, each of which contains a portion of
the data set. In this setting, the whole data $\Omega$ is decomposed into disjoint
subsets $\{\Omega_k\}$,  where $k$ denotes  a  particular  local  worker,
and $\cup_{l=1}^p{\Omega_l} = \Omega$ and if $i\neq j$, $\Omega_i\cap\Omega_i = \emptyset$. Different clients can not communicate with each other and the  clients  can
only communicate with the central server. Our model of
computation is similar to the ones used in Parameter
Server \cite{Li2014} and Mllib \cite{Meng2016Spark}. Our goal is to
derive stochastic algorithms that scale linearly to high $p$.

\subsection{Synchronous Accelerated SGD}

AcPSVRG naturally extends to the distributed synchronous
setting,  and  is  presented  in  Algorithm  \ref{SyncVR-Algo-Single}.  To  distinguish  the
algorithm from the single worker case, we call it Sync-AcPSVRG. On  each  epoch,  the  local  nodes  first  retrieve  a  copy of  the  central  iterate $\widetilde{x}^{s-1}$, and compute the sum of the gradients on its local data, i.e., $\sum_{i\in\Omega_k}{\nabla f_i{(\widetilde{x}^{s-1})}}$ and send it to the master node. The accelerated VR method is then locally performed on each node by only using the local data, and will send the most recent parameter denoted as $\widetilde{x}_k$ for worker $k$ to the master. The parameter $\widetilde{x}^s$ is then updated by the master after all the locally updated parameters have been gathered. As we put now constraint on how training data are partitioned on different workers, by sharing full gradient $\nabla f(\widetilde{x})$ across nodes, we ensure  that  the  local  gradient  updates  utilize  global  gradient information from remote nodes. We ensure also local updates are not far away from global updates through including momentum. This speeds up the convergence of stochastic optimization and controls the difference between the local update and global update, even if each local  node  runs  for  one  whole  epoch  before  communicating back with the central node. In Sync-AcPSVRG, we assume that each worker has access to a subset and not the entire data set and each local node performs local updates for one epoch, or iterations, before communicating with
the  server. This  is  a  rather  low  communication  frequency
compared  to  a mini-batch parameter  server  model such as parallel implementation of SVRG \cite{Zhao2014} or mS2GD \cite{Konecny2016} in which stochastic optimization is performed on the master node based on the whole dataset, and updates and gradients are frequently transferred between workers and master. This  makes  a  significant  difference  in  runtimes  when  the
number of local nodes is large.
 We also increase the number of inner updates per epoch by a constant $\gamma$ and it reduces the communication rate between server and worker because there is no communication during the inner iterations of each epoch and yields fastest convergence..
\begin{algorithm}[H]
\caption{Sync-AcPSVRG}\label{SyncVR-Algo-Single}
\begin{algorithmic}
\State\Input The number of epochs $S$ and the step size $\eta$.
\State\Initialize $\widetilde{x}^0$, $m_1$, $\theta_1$, and $\rho > 1$.
\For {Worker $l$} 
\For{ $s=1$ {\bf to} $S$ }
\State Receive $\widetilde{x} = \widetilde{x}^{s-1}$ from master node;
\State  Send $\nabla_l f(\widetilde{x}) = \sum_{i\in\Omega_l}\nabla f_i(\widetilde{x})$ to master node;
\State  Receive $\nabla f(\widetilde{x})$ from master node;
 \State $x_{l,0}^s = z_{l,0}^s = \widetilde{x}$;
 \For{$k=0$ {\bf to} $m_s-1$}
 \State Pick $i_{l,k}^s$ uniformly at random from $\Omega_l$;
 \State $\widetilde{\nabla}_{l,k}^s = \nabla f_{i_{l,k}^s}(x_{l,k}^s) - \nabla f_{i_{l,k}^s}(\widetilde{x}) + \nabla f(\widetilde{x})$;
 \State $\delta_{l,k}^s = \text{argmin}_{\delta}\,\,h(z_{l,k}^s+\delta)+\langle\widetilde{\nabla}_{l,k}^s,\delta\rangle + \frac{{\color{red}\beta_s}}{2\eta}\norm{\delta}^2$;
 \State $z_{l,k+1}^s = z_{l,k}^s + \delta_{l,k}^s$;
 \State $x_{l,k+1}^s = \widetilde{x}+\beta_s(z_{l,k+1}^s-\widetilde{x})$;
 \EndFor
$\widetilde{x}_{l}^s = \frac{1}{m_s}\sum_{k=0}^{m_s-1} x_{l,k}^s$, $m_{s+1} = \gamma m_s$;
\State Send $\widetilde{x}_{l}^s$ to master node.
 \EndFor
 \EndFor
 \State{\bf Master Node:}
 \State\qquad\,\,\,\, Average $\widetilde{x}_l$ received from workers;
  \State\qquad\,\,\,\, Broadcast averaged $\widetilde{x}$ to local workers;
  \State\qquad\,\,\,\, Average $\nabla f(\widetilde{x}_l)$ received from workers;
  \State\qquad\,\,\,\, Broadcast averaged $\nabla f(\widetilde{x})$ to local workers;
 \State\Output $\widetilde{x}^S$
\end{algorithmic}
\end{algorithm}

\subsection{Asynchronous Accelerated SGD}

The  synchronous algorithm could be extended to the asynchronous case called Async-AcPSVRG  as shown in Algorithm \ref{AsyncVR-Algo}. We adopt asynchronous update in each inner loop and there is synchronization operation after each epoch. In Async-AcPSVRG, the  master node keeps a copy of the averaged $x$. We make workers do proximal mapping step, and server is responsible for element-wise addition operations, average $x$, aggregate full gradient and then broadcast it to workers at the end of each epoch. 

The key
idea for Async-AcPSVRG is that, once a local node solves the proximal update, it  sends  the update to the master. The master thread will use the update received by the local node to update $x$. Each processor repeatedly runs these procedures concurrently, without any synchronization. We use $k-\tau_k$ to denote the state of $x$ at the reading time by the local worker. For asynchronous algorithms, the partial gradient calculated by the local worker will be delayed, and at iteration $k$ the worker $l$ could only obtain $\nabla f_{i_{l,k}^s}(x_{k-\tau_k}^s)$ instead of $\nabla f_{i_{l,k}^s}(x_{k}^s)$. We assume that the delay in between the time of evaluation and updating is bounded by a non-negative integer $\tau$, i.e., $\tau_k\leq \tau$. The parameter $\tau$ captures the degree of delay. When there are more threads, the delay accumulates and results in larger $\tau$. Furthermore, we also assume that the system is synchronized after every epoch.

For the purpose of our analysis, we assume a consistent read model, i.e., when $x$ is read or updated in the central node, it will be locked. However, the proposed asynchronous algorithms can be easily implemented in a lock-free setting, leading to further speedups. We leave the analysis of inconsistent read (wild) model as future work.


% \begin{algorithm}\label{VR-Algo}
% \caption{AFSVRG for smooth component functions}\label{alg:euclid}
% \begin{algorithmic}[1]
% \State\Input The number of epochs $S$ and the step size $\eta$.
% \State\Initialize $\widetilde{x}^0$, $m_1$, $\theta_1$, and $\rho > 1$.
% \For{ $s=1,2,\ldots, S$ }
% \State $\widetilde{\mu} = \frac{1}{n}\sum_{i=1}^n\nabla f_i(\widetilde{x}^{s-1})$, $x_0^s = y_0^s = \widetilde{x}^{s-1}$;
%  \For{$k=0,1,2,\ldots,m_s-1$}
%  \State Pick $i_k^s$ uniformly at random from $\{1,\ldots,n\}$;
%  \State $\widetilde{\nabla} f_{i_k^s}(x_{k}^s) = \nabla f_{i_k^s}(x_{k}^s) - \nabla f_{i_k^s}(\widetilde{x}^{s-1}) + \widetilde{\mu}$;
%  \State $y_{k+1}^s = y_{k}^s - \eta\left[\widetilde{\nabla} f_{i_k^s}(x_{k}^s)+ \nabla g(x_{k}^s)\right]$;
%  \State $x_{k+1}^s = \widetilde{x}^{s-1}+\beta_s(y_{k+1}^s-\widetilde{x}^{s-1})$;
%  \EndFor
% $\widetilde{x}^s = \frac{1}{m_s}\sum_{k=0}^{m_s-1} x_{k}^s$, $m_{s+1} = \lceil{\rho^s\cdot m_1}\rceil$
%  \EndFor
%  \State\Output $\widetilde{x}^S$
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[H]
\caption{Async-AcPSVRG}\label{AsyncVR-Algo}
\begin{algorithmic}
\State\Input The number of epochs $S$ and the step size $\eta$.
\State\Initialize  $\widetilde{x}^0$, $m_1$, $\theta_1$, and $\rho > 1$.

\For{ $s=1$ {\bf to} $S$ }
\State $t=0$;
\State{\bf Worker $l$} 
\State Wait until it receives $\widetilde{x} = \widetilde{x}^{s-1}$ from master node;
\State  Send $\nabla_l f(\widetilde{x}) = \sum_{i\in\Omega_l}\nabla f_i(\widetilde{x})$ to master node;
\State  Wait until it receives $\nabla f(\widetilde{x})$ from master node;
 \State $x_{l,0}^s = z_{l,0}^s = \widetilde{x}$;
 \For{$k=0$ {\bf to} $m_s-1$}
 \State Receive $x_{l,k-\tau_k}^s:=x_{t}^s$ from master node.
 \State Pick $i_{l,k}^s$ uniformly at random from $\Omega_l$;
 \State $\widetilde{\nabla}_{l,k}^s = \nabla f_{i_{l,k}^s}(x_{l,k-\tau_k}^s) - \nabla f_{i_{l,k}^s}(\widetilde{x}) + \nabla f(\widetilde{x})$;
 \State $\delta_{l,k}^s = \text{argmin}_{\delta}\,\,h(z_{l,k}^s+\delta)+\langle\widetilde{\nabla}_{l,k}^s,\delta\rangle + \frac{{\color{red}\beta_s}}{2\eta}\norm{\delta}^2$;
 \State $z_{l,k+1}^s = z_{l,k}^s + \delta_{l,k}^s$;
 \State Send $z_{l,k+1}^s$ to master node;
\item[{\bf ~~~~~~Master Node:}]
 \State Receive $z_{l,k+1}^s$ from worker $l$; 
  \State Update $x^s_{t+1} = \widetilde{x}+\beta_s(z_{l,k+1}^s-\widetilde{x})$, $t=t+1$;
  \EndFor
   \State Calculate average $\widetilde{x}^s = \frac{1}{t}\sum_{i=1}^{t} x_{i}^s$ and broadcast averaged $\widetilde{x}^s$. 
  \State Receive $\nabla_l f(\widetilde{x}^s)$ from workers and calculate average $\nabla f(\widetilde{x}^s)$;
  \State Broadcast averaged $\nabla f(\widetilde{x}^s)$ to local workers;
  \State $m_{s+1} = \gamma m_s$;
  \EndFor
 \State\Output $\widetilde{x}^S$
\end{algorithmic}
\end{algorithm}

\section{Convergence Analysis}
In this section, we provide convergence analysis for distributed AcPSVRG. For further analysis, throughout this paper, we make the following assumptions, which are commonly used in previous works \cite{Reddi2015, Meng2016}.

\begin{Assumption}[Lipschitz Gradient]\label{Assump1}
The components $f_i(x), i=1,2,\ldots,n$ are differentiable and have Lipschitz continuous partial gradients, i.e., for $x,y\in \R^d$, we have,
\begin{equation}
    \norm{\nabla f_i(x) - \nabla f_i(y)} \leq L \norm{x-y}.
\end{equation}
\end{Assumption}
\begin{Assumption}[Convexity]\label{Assump2}
The components $f_i(x), i=1,2,\ldots,n$ and function $h(x)$ are convex. The objective function $F(x)$ could be strongly convex with parameter $\mu$, i.e., 
$\forall x,y \in \R^d$
\begin{equation}\label{Convex:Eq1}
F(y)\geq F(x) + \Iprod{\xi}{y-x} + \frac{\mu}{2}\norm{x-y}^2,\qquad \forall \xi\in\partial F(x),
\end{equation}
and for non-strongly convex functions $\mu=0$. 
\end{Assumption}

\begin{Assumption}[Bounded Delay]\label{Assump3}
The delays $\tau_1,\tau_2,\ldots$ are independent random variables, and $\tau_k\leq \tau$ for all $k$. As we mentioned earlier, we use $k-\tau_k$ to denote the read state at iteration $k$ in the asynchronous algorithm.
\end{Assumption}

Let $p(w)$ be a convex function over a convex set $X$. Let $\hat{w} = {argmin}_{w\in X}\{p(w)+\alpha\norm{w-\overline{y}}^2\}$ for some $\overline{y}\in X$ and $\alpha\geq 0$. Due to the fact that the sum of a convex and a strongly convex function is also strongly convex,  we have
\begin{equation}\label{threepoint-convex}
 p(w)+\alpha\norm{w-\overline{y}}^2 \geq p(\hat{w})+\alpha\norm{\hat{w}-\overline{y}}^2+\alpha\norm{w-\hat{w}}^2.
\end{equation}


\begin{lemma}\label{lemma0}\cite{Johnson12,Allen-Zhu17} If $f_i(x)$'s, $i=1,2,\ldots,n$ have $L$-Lipschitz continuous gradients, then with defining 
\[
\widetilde{\nabla} f(w) = \nabla f_k(w) - \nabla f_k(\widetilde{x})+ f(\widetilde{x}),
\]
we have
\begin{equation}
\E\left(\norm{\widetilde{\nabla} f(w)-\nabla f(w)^2}^2\right) \leq 2L (f(\widetilde{x})-f(w)+\Iprod{\nabla f(w)}{w-\widetilde{x}}).
\end{equation}
 
\end{lemma}


\begin{lemma}\label{lemma1}
Under Assumption \ref{Assump1}, if $x^*$ is the optimal solution of Problem \eqref{problem}, for the Algorithm \ref{AsyncVR-Algo} and $\eta < \frac{1}{2L}$, we have 
\begin{equation}
\begin{split}
\E\left[ F(\widetilde{x}^s) - F(x^*)\right] &\leq \lambda_s\E[F(\widetilde{x}^{s-1}) - F(x^*)]\\
&+{\color{red}\frac{\beta_s^2}{2\eta m_s(1-\alpha_s)}}\E[\norm{z_{0}^s-x^*}^2-\norm{z_{m_s}^s-x^*}^2 ],
\end{split}
\end{equation}
with $\lambda_s = \frac{(1-\beta_s+\alpha_s)}{1-\alpha_s}$, $\alpha_s=\frac{4(2+\theta_{\eta}^{-1})L^2\tau^2\eta^2}{1-2L^2 \tau^2\eta^2}$ and $\theta_{\eta} = \frac{1-\eta L}{\eta L}$.
\end{lemma}
\begin{proof}
By choosing $\eta < \frac{1}{2L}$ we have,
\begin{equation}\label{Eq1-Lemma1}
\begin{split}
f(x_{k+1}^s) & \leq f(x_{k}^s) + \Iprod{\nabla f(x_{k}^s)}{x_{k+1}^s-x_{k}^s}+\frac{L}{2}\norm{x_{k+1}^s-x_{k}^s}^2\\
&=f(x_{k}^s) + \Iprod{\nabla f(x_{k}^s)}{x_{k+1}^s-x_{k}^s}\\
&~~~+ {\color{red}\frac{1 }{2\eta}}\norm{x_{k+1}^s-x_{k}^s}^2-\frac{\theta_{\eta} L}{2}\norm{x_{k+1}^s-x_{k}^s}^2\\
&\stackrel{a}{=}f(x_{k}^s) + \Iprod{\VRG+\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}{x_{k+1}^s-x_{k}^s} + {\color{red} \frac{1}{2\eta}}\norm{x_{k+1}^s-x_{k}^s}^2\\
&~~~+\Iprod{\nabla f(x_{k-\tau_k}^s)-\VRG}{x_{k+1}^s-x_{k}^s}-\frac{\theta_{\eta} L}{2}\norm{x_{k+1}^s-x_{k}^s}.
\end{split}
\end{equation}
where the first inequality follows from Lipschitz continuous nature of the gradient of function $f$ and $\frac{1}{L\eta}-\theta_{\eta}=1$. In $\stackrel{a}{=}$, we add and subtract $\Iprod{\VRG-\nabla f(x_{k-\tau_k}^s)}{x_{k+1}^s-x_{k}^s}$. We also have,
\begin{equation}\label{Eq2-Lemma3}
\begin{split}
& \E\Iprod{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}{x_{k+1}^s-x_{k}^s}\\
&\leq \frac{1}{2 \theta_{\eta} L} \norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2+ \frac{L\theta_{\eta}}{2}\norm{x_{k+1}^s-x_{k}^s}^2,
\end{split}
\end{equation}
where it follows from Cauchy-Schwarz inequality. To bound the last term in equation \eqref{Eq1-Lemma1}, we obtain,
\begin{equation}\label{Eq2-Lemma1}
\begin{split}
&\Iprod{\nabla f(x_{k-\tau_{k}}^s)-\VRG}{x_{k+1}^s-x_{k}^s}\\
&\stackrel{a}{\leq} \E\left[\frac{1}{2\theta_{\eta} L}\norm{\nabla f(x_{k-\tau_{k}}^s)-\VRG}^2 + \frac{\theta_{\eta} L}{2}\norm{x_{k+1}^s-x_{k}^s}^2\right]\\
&\stackrel{b}{\leq} \frac{1}{\theta_{\eta}}\left[f(\widetilde{x}^{s-1})-f(x_{k-\tau_k}^s)+\Iprod{\nabla f(x_{k-\tau_k}^s)}{x_{k-\tau_k}^s-\widetilde{x}^s}\right]+ \frac{\theta_{\eta} L}{2}\norm{x_{k+1}^{s}-x_{k}^s}^2,
\end{split}
\end{equation}
where inequality $\stackrel{a}{\leq}$ follows from the Cauchy-Schwarz inequality, and inequality $\stackrel{b}{\leq}$ follows from Lemma \ref{lemma0}. 



Substituting the inequality \eqref{Eq2-Lemma1} into \eqref{Eq1-Lemma1}, and taking expectation over $i ^s_k$ , we obtain,
\begin{equation}\label{Eq3-Lemma1-1}
\begin{split}
&F(x_{k+1}^s) -f(x_{k}^s)\leq h(x_{k+1}^s)+ \Iprod{\VRG}{x_{k+1}^s-x_{k}^s}+{\color{red}\frac{1}{2\eta}}\norm{x_{k+1}^s-x_{k}^s}^2\\
&~~~+\frac{1}{\theta_{\eta}}\left[f(\widetilde{x}^{s-1})-f(x_{k-\tau_k}^s)+\Iprod{\nabla f(x_{k-\tau_k}^s)}{x_{k-\tau_k}^s-\widetilde{x}^s}\right]\\&
~~~+{\color{red}\frac{1}{2 \theta_{\eta} L} \norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2}\\
&\stackrel{a}{\leq}\E\left[\Iprod{\beta_s\VRG}{z_{k+1}^s-z_{k}^s}\right]+{\color{red}\frac{ \beta_s^2}{2\eta}}\norm{z_{k+1}^s-z_{k}^s}^2+\beta_s h(z_{k+1}^s)\\
&~~~+(1-\beta_s) h(\widetilde{x}_k^{s-1})+\frac{1}{\theta_{\eta}}\left[f(\widetilde{x}^{s-1})-f(x_{k-\tau_k}^s)+\Iprod{\nabla f(x_{k-\tau_k}^s)}{x_{k-\tau_k}^s-\widetilde{x}^{s-1}}\right]\\
&~~~+{\color{red}\frac{1}{2 \theta_{\eta} L} \norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2}
\end{split}
\end{equation}
where in inequality $\stackrel{a}{\leq}$ we use the update $x_k^s = z_k^s+(1-\beta_s)\widetilde{x}^{s-1}$ and convexity of $h$. Since   $p(\delta) = h(z_k^s+\delta)+\langle\widetilde{\nabla}_k^s,\delta\rangle$ is a convex function, using inequality \eqref{threepoint-convex} with $w=x^*-z_{k}^s$, $\overline{y} = 0$, it follows
\begin{equation}
\begin{split}
h(z_{k+1}^s) &+ \Iprod{\VRG}{z_{k+1}^s-z_{k}^s}+{\color{red}\frac{\beta_s}{2\eta}}\norm{z_{k+1}^s-z_{k}^s}^2  \\
&\leq h(x^*)+\Iprod{\VRG}{x^* - z_{k}^s}+{\color{red}\frac{\beta_s}{2\eta}}(\norm{z_{k}^s-x^*}^2-\norm{z_{k+1}^s-x^*}^2). \\ 
\end{split}
\end{equation}
By replacing the above inequality in \eqref{Eq3-Lemma1-1} we have,
\begin{equation}\label{Eq3-Lemma1I}
\begin{split}
&\E\left[F(x_{k+1}^s) -f(x_{k}^s)\right]\\
&{\leq}\E\left[\Iprod{\beta_s\VRG}{x^* - z_{k}^s}+{\color{red}\frac{\beta_s^2}{2\eta}}(\norm{z_{k}^s-x^*}^2-\norm{z_{k+1}^s-x^*}^2)+\beta_s h(x^*)+(1-\beta_s) h(\widetilde{x}_k^{s-1})\right]\\
&~~~~+\frac{1}{\theta_{\eta}}\E\left[f(\widetilde{x}^{s-1})-f(x_{k-\tau_k}^s)+\Iprod{\nabla f(x_{k-\tau_k}^s)}{x_{k-\tau_k}^s-\widetilde{x}^{s-1}}\right]\\
&~~~~+{\color{red}\frac{1}{2 \theta_{\eta} L} \E\norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2}\\
& \stackrel{a}{=} \E\left[{\color{red}\frac{ \beta_s^2}{2\eta}}(\norm{z_{k}^s-x^*}^2-\norm{z_{k+1}^s-x^*}^2)+\beta_sh(x^*)\right]+(1-\beta_s) \E[h(\widetilde{x}_k^{s-1})]\\
&~~~~+\E\left[\Iprod{\nabla f(x_{k-\tau_k}^s)}{\beta_sx^*+(1-\beta_s)\widetilde{x}^{s-1}-{x}^{s}_{k}+\theta_{\eta}^{-1}(x_{k-\tau_k}^s-\widetilde{x}^{s-1})}+\frac{1}{\theta_{\eta}}f(\widetilde{x}^{s-1})\right]\\
&~~~~+\E\left[\Iprod{-\nabla f_{i_k^s}(\widetilde{x}^{s-1})+\nabla f(\widetilde{x}^{s-1})}{\beta_sx^*+(1-\beta_s)\widetilde{x}^{s-1}-x_{k}^{s}}-\frac{1}{\theta_{\eta}}f(\widetilde{x}^{s}_{k-\tau_k})\right]\\
&~~~~+{\color{red}\frac{1}{2 \theta_{\eta} L} \E\norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2}
\end{split}
\end{equation}
The equality $\stackrel{a}{=}$ is obtained by definition of $\VRG$ and rearranging terms. By using    
\[
\E\Iprod{-\nabla f_{i_k^s}(\widetilde{x}^{s-1})+\nabla f(\widetilde{x}^{s-1})}{\beta_sx^*+(1-\beta_s)\widetilde{x}^{s-1}-x_{k}^{s}}=0,
\]
in \eqref{Eq3-Lemma1I}, we have
\begin{equation}\label{Eq3-Lemma1}
\begin{split}
&\E\left[F(x_{k+1}^s) -f(x_{k}^s)\right]\\
&\leq\E\left[{\color{red}\frac{ \beta_s^2}{2\eta}}(\norm{z_{k}^s-x^*}^2-\norm{z_{k+1}^s-x^*}^2)+\beta_sh(x^*)+(1-\beta_s) h(\widetilde{x}_k^{s-1})\right]\\
&~~~+\E\left[\Iprod{\nabla f(x_{k-\tau_k}^s)}{\beta_sx^*+(1-\beta_s)\widetilde{x}^{s-1}-{x}^{s}_{k}+\theta_{\eta}^{-1}(x_{k-\tau_k}^s-\widetilde{x}^{s-1})}\right]\\
&~~~+\frac{1}{\theta_{\eta}}\E\left[f(\widetilde{x}^{s-1})-f({x}_{k-\tau_k}^{s})\right]+{\color{red}\frac{1}{2 \theta_{\eta} L} \E\norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2}.
\end{split}
\end{equation}
Additionally, we have the following,
\begin{equation}\label{Eq4-Lemma1}
\begin{split}
&\Iprod{\nabla f(x_{k-\tau_k}^s)}{\beta_sx^*+(1-\beta_s)\widetilde{x}^{s-1}-{x}^{s}_{k}+\theta_{\eta}^{-1}(x_{k-\tau_k}^s-\widetilde{x}^{s-1})}\\
&=\Iprod{\nabla f(x_{k-\tau_k}^s)}{\beta_sx^*+(1-\beta_s-\theta_{\eta}^{-1})\widetilde{x}^{s-1}+\theta_{\eta}^{-1}x_{k-\tau_k}^s-x_{k-\tau_k}^s+x_{k-\tau_k}^s-{x}^{s}_{k}}\\
&\stackrel{a}{\leq} \beta_sf(x^*)+(1-\beta_s-\theta_{\eta}^{-1})f(\widetilde{x}^{s-1})+\theta_{\eta}^{-1}f(x_{k-\tau_k}^s)-f(x_{k-\tau_k}^s)\\
&~~~~~+\Iprod{\nabla f(x_{k-\tau_k}^s)-\nabla f(x_{k}^s)}{x_{k-\tau_k}^s-{x}^{s}_{k}}+\Iprod{\nabla f(x_{k}^s)}{x_{k-\tau_k}^s-{x}^{s}_{k}}\\
&\stackrel{b}{\leq} \beta_sf(x^*)+(1-\beta_s-\theta_{\eta}^{-1})f(\widetilde{x}^{s-1})+\theta_{\eta}^{-1}f(x_{k-\tau_k}^s)\\
&~~~~~+\Iprod{\nabla f(x_{k-\tau_k}^s)-\nabla f(x_{k}^s)}{x_{k-\tau_k}^s-{x}^{s}_{k}}- f(x_{k}^s)
\end{split}
\end{equation}
where in inequalities $\stackrel{a}{\leq}$ and $\stackrel{b}{\leq}$ we used the convexity of $f$. 

Since $z_{k+1}^s$ is the optimal solution of proximal subproblem in the Algorithm \ref{AsyncVR-Algo}, there exists ${\xi}_{k+1}^s\in\partial h(z_{k+1}^s)$ satisfying
\begin{equation}\label{prox}
{\color{red}\beta_s}(z_{k+1}^s - z_{k}^s) + \eta\VRG + \eta{\xi}_{k+1}^s = 0.
\end{equation}
We next bound the term $\E[\norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2]$ using the fact that gradient of the function $f$ is Lipschitz continuous in the following manner,
\begin{equation}\label{Eq5-Lemma1}
\begin{split}
\E[\norm{\nabla f(x_{k}^s)-\nabla f(x_{k-\tau_k}^s)}^2]&\leq L^2\tau \sum_{i = k-\tau}^{k-1}\E[\norm{x_{i+1}^s-x_{i}^s}^2]\\
&\stackrel{a}{=} L^2\tau\beta_s^2\sum_{i = k-\tau}^{k-1}\E[\norm{z_{i+1}^s-z_{i}^s}^2] \\
&\stackrel{b}{=} {\color{red} L^2\tau\eta^2\sum_{i = k-\tau}^{k-1}\E[\norm{\widetilde{\nabla}_{i}^s+{\xi}_{i+1}^s}^2]}.
\end{split}
\end{equation}
Similarly,
\begin{equation}\label{Eq5p-Lemma1}
\begin{split}
\Iprod{\nabla f(x_{k-\tau_k}^s)-\nabla f(x_{k}^s)}{x_{k-\tau_k}^s-{x}^{s}_{k}}&\leq L\tau \sum_{i = k-\tau}^{k-1}\E[\norm{x_{i+1}^s-x_{i}^s}^2]\\
&\stackrel{c}{=} L\tau\beta_s^2\sum_{i = k-\tau}^{k-1}\E[\norm{z_{i+1}^s-z_{i}^s}^2] \\
&\stackrel{d}{=}  L\tau\eta^2\sum_{i = k-\tau}^{k-1}\E[\norm{\widetilde{\nabla}_{i}^s+{\xi}_{i+1}^s}^2]
\end{split}
\end{equation}
where in equalities $\stackrel{a}{=}$ and  $\stackrel{c}{=}$, we use the definition for the updates of $x_{i+1}^s$ in the algorithm Async-AcPSVRG and equalities $\stackrel{b}{=}$ and $\stackrel{d}{=}$ follow from \eqref{prox}.
We define the following quantities,
\begin{equation}
\begin{split}
u_k^s &= \nabla f_{i_k^s}(x_{k-\tau_k}^s) - \nabla f_{i_k^s}(\widetilde{x}^{s-1}) + \nabla f(\widetilde{x}^{s-1})+{\xi}_{k+1}^s = {\widetilde{\nabla}_{k}^s}+{\xi}_{k+1}^s,\\
v_k^s &= \nabla f_{i_k^s}(x_{k}^s) - \nabla f_{i_k^s}(\widetilde{x}^{s-1}) + \nabla f(\widetilde{x}^{s-1})+{\xi}_{k+1}^s,
\end{split}
\end{equation}
where ${\xi}_{k+1}^s\in\partial h(x_{k+1}^s)$.

By combining the bounds in equations \eqref{Eq4-Lemma1} and \eqref{Eq5p-Lemma1} and substituting the result and inequality \eqref{Eq5-Lemma1} in equation \eqref{Eq3-Lemma1}, we get
\begin{equation}
\begin{split}
\E[F(x_k^s)] \leq &\beta_s F(x^*)+(1-\beta_s)F(\widetilde{x}^{s-1}) + {\color{red}\frac{\beta_s^2}{2\eta}}\E[\norm{z_{k}^s-x^*}^2-\norm{z_{k+1}^s-x^*}^2]\\
&+(1+\frac{1}{2\theta_{\eta}})L\tau\eta^2\sum_{i = k-\tau}^{k-1}\E[\norm{u_{i}^s}^2]. 
\end{split}
\end{equation}
Equivalently, We have the following
\begin{equation}\label{main-ine-Lemma1}
\begin{split}
\E[F(x_k^s)-F(x^*)] \leq &(1-\beta_s)[F(\widetilde{x}^{s-1})-F(x^*)] + {\color{red}\frac{ \beta_s^2}{2\eta}}\E[\norm{z_{k}^s-x^*}^2-\norm{z_{k+1}^s-x^*}^2]\\
&+(1+\frac{1}{2\theta_{\eta}})L\tau\eta^2\sum_{i = k-\tau}^{k-1}\E[\norm{u_{i}^s}^2]. 
\end{split}
\end{equation}

We next bound the term $\E[\norm{u^t}^2]$ in terms of $\E[\norm{v^t}^2]$ in the following way,
\begin{equation}
\begin{split}
\E[\norm{u^s_i}^2] &\leq 2\E[\norm{u^s_i-v^s_i}^2+\norm{v^s_i}^2]\\
&\leq 2\E\left[\norm{ \nabla f_{i_k^s}(x_{k-\tau_k}^s)+{\xi}_{k}^s - \nabla f_{i_k^s}(x_{k}^s)-{\xi}_{k}^s}^2\right] + 2\E\norm{v^s_i}^2\\
&\stackrel{a}{\leq} 2 {L}^2\tau \sum_{i=k-\tau+1}^{k-1}\E\norm{x_{i+1}^s-x_{i}^s}^2+ 2\E\norm{v^s_i}^2\\
&= 2 {L}^2\beta_s^2\tau \sum_{i=k-\tau+1}^{k-1}\E\norm{z_{i+1}^s-z_{i}^s}^2+ 2\E\norm{v^s_i}^2\\
&= 2 {L}^2\tau \eta^2\sum_{i=k-\tau+1}^{k-1} \E\norm{\widetilde{\nabla}_{i}^s+\xi_{i}^s}^2+ 2\E\norm{v^s_i}^2\\
&= 2 {L}^2\tau \eta^2\sum_{i=k-\tau+1}^{k-1} \E\norm{u^s_i}^2+ 2\E\norm{v^s_i}^2,
\end{split}
\end{equation}
where in $\stackrel{a}{\leq}$ we use the Lipschitz
continuity of the gradient. 
Adding the above inequalities
from $k=0$ to $m_s-1$, we get
\begin{equation}
\begin{split}
\sum_{k=0}^{m_s-1}\E[\norm{u^s_k}]^2&\leq \sum_{k=0}^{m_s-1}\left[2L^2 \tau\eta^2\sum_{i=k-\tau+1}^{k-1} \E[\norm{u^s_i}^2]+2\E[\norm{v^s_k}^2]\right]\\
&\leq 2L^2 \tau^2\eta^2\sum_{k=0}^{m_s-1}\E[\norm{u^s_k}^2]+2\sum_{k=0}^{m_s-1}\E[\norm{v^s_k}^2].
\end{split}
\end{equation}
The last inequality follows from that fact that the delay is at most $\tau$ and in particular for each  $\E[\norm{u^s_i}^2]$, there are at most $\tau$ terms. From the above inequality, we get 
\begin{equation}\label{Eq6-Lemma1}
\begin{split}
\sum_{k=0}^{m_s-1} \E[\norm{u^s_k}]^2&\leq \frac{2}{1-2L^2 \tau^2\eta^2}\sum_{k=0}^{m_s-1}\E[\norm{v^s_k}^2].
\end{split}
\end{equation}


Adding the inequality \eqref{main-ine-Lemma1} from $k=0$ to $k=m_s-1$, we get
\begin{equation}\label{lemma-finaleq1-1}
\begin{split}
\sum_{k=0}^{m_s-1}\E[F(x_k^s)-F(x^*)] &\leq m_s(1-\beta_s)[F(\widetilde{x}^{s-1})-F(x^*)] + {\color{red}\frac{ \beta_s^2}{2\eta}}\E[\norm{z_{0}^s-x^*}^2-\norm{z_{m_s}^s-x^*}^2]\\
&~~+(1+\frac{1}{2\theta_{\eta}})L\tau^2\eta^2\sum_{k=0}^{m_s-1}\E[\norm{u_{i}^s}^2]\\
&\stackrel{a}{\leq} m_s(1-\beta_s)[F(\widetilde{x}^{s-1})-F(x^*)] + {\color{red}\frac{ \beta_s^2}{2\eta}}\E[\norm{z_{0}^s-x^*}^2-\norm{z_{m_s}^s-x^*}^2]\\
&~~+\frac{(2+\theta_{\eta}^{-1})L\tau^2\eta^2}{1-2L^2 \tau^2\eta^2}\sum_{k=0}^{m_s-1}\E[\norm{v^s_k}^2]
\end{split}
\end{equation}
where in inequality $\stackrel{a}{\leq}$ we used \eqref{Eq6-Lemma1}.


From Lemma 3 of \cite{Johnson12} (see also \cite{Reddi2015}), we have 
\[
\E[\norm{v^s_k}^2] \leq 4L \E[F(x_k^s)-F(x^*)+F(\widetilde{x}^{s-1})-F(x^*)].
\]
Substituting the above bound on $\E[\norm{v^s_k}^2]$ in equation \eqref{lemma-finaleq1-1}, we get the following
\begin{equation}\label{lemma-finaleq1}
\begin{split}
(1-&\frac{4(2+\theta_{\eta}^{-1})L^2\tau^2\eta^2}{1-2L^2 \tau^2\eta^2})\sum_{k=0}^{m_s-1}\E[F(x_k^s)-F(x^*)]\\ 
\leq &m_s\left((1-\beta_s)+\frac{4(2+\theta_{\eta}^{-1})L^2\tau^2\eta^2}{1-2L^2 \tau^2\eta^2}\right)[F(\widetilde{x}^{s-1})-F(x^*)] \\
&+ {\color{red}\frac{ \beta_s^2}{2\eta}}\E[\norm{z_{0}^s-x_*}^2-\norm{z_{m_s}^s-x_*}^2].
\end{split}
\end{equation}

From convexity of function $F$ and the definition $\widetilde{x}^s = \frac{1}{m_s}\sum_{k=0}^{m_s-1}x_k^s$, we get 
\[
F(\widetilde{x}^s) = F(\frac{1}{m_s}\sum_{k=0}^{m_s-1} x_k^s)\leq \frac{1}{m_s}\sum_{k=0}^{m_s-1}F(x_k^s).
\]
From the above inequality and \eqref{lemma-finaleq1}, we get the following
\begin{equation}
\begin{split}
(1-&\frac{4(2+\theta_{\eta}^{-1})L^2\tau^2\eta^2}{1-2L^2 \tau^2\eta^2})\E[F(\widetilde{x}^s)-F(x^*)]\\ 
\leq &((1-\beta_s)+\frac{4(2+\theta_{\eta}^{-1})L^2\tau^2\eta^2}{1-2L^2 \tau^2\eta^2})[F(\widetilde{x}^{s-1})-F(x^*)] \\
&+ {\color{red}\frac{\beta_s^2}{2\eta m_s}}\E[\norm{z_{0}^s-x^*}^2-\norm{z_{m_s}^s-x^*}^2].
\end{split}
\end{equation}
\end{proof}
\begin{theorem}
Suppose $\beta_s$ is chosen such that $\beta_s\leq \frac{1}{s+2}$,  $\lambda_s/\beta_s^2 \leq 1/\theta_{s-1}^2$ and $\eta$ is chosen such that $\alpha_s\leq \frac{1}{2}$.
Then, we have
\begin{equation}
\begin{split}
{\E[F(\widetilde{x}^S)-F(x^*)]}&\leq \frac{1}{\theta_{0}^2(S+2)^2}[F(\widetilde{x}^{0})-F(x^*)] \\ 
& + {\color{red}\frac{1}{\eta m_{S}(S+2)^2}}\E[\norm{z_{0}^1-x_*}^2].
\end{split}
\end{equation}
If function $F(x)$ is strongly convex with parameter $\mu$, and $\beta_s$ and $m_s$ are chosen such that
\[
\lambda_s' := \lambda_s+{\color{red}\frac{2 \beta_s^2}{\mu\eta m_s}} < 1,
\]
we have, 
\begin{equation}
\E\left[F(\widetilde{x}^S)-F(x^*)\right]\leq \lambda_{max}^S\left[F(\widetilde{x}^0)-F(x^*)\right],
\end{equation}
where $\lambda_{max} = \max_{s}{\lambda_s'}$.
\end{theorem}
\begin{proof}
From Lemma \ref{lemma1} we have,
\begin{equation}
\begin{split}
\E[F(\widetilde{x}^s)-F(x^*)]\leq & \lambda_s[F({\widetilde x}^{s-1})-F(x^*)]\\ 
&+ {\color{red}\frac{\beta_s^2}{2\eta m_s(1-\alpha_s)}}\E[\norm{z_{0}^s-x_*}^2-\norm{z_{m_s}^s-x_*}^2]
\end{split}
\end{equation}
Dividing both sides of the above inequality by $\beta_s^2$, and using the fact $\lambda_s/\beta_s^2 \leq 1/\theta_{s-1}^2$ we have
\begin{equation*}
\begin{split}
\frac{\E[F(\widetilde{x}^s)-F(x^*)]}{\beta_s^2}\leq & \frac{\lambda_s}{\beta_s^2}[F(\widetilde{x}^{s-1})-F(x^*)]\\ 
& + {\color{red}\frac{1 }{2\eta m_s(1-\alpha_s)}}\E[\norm{z_{0}^s-x^*}^2-\norm{z_{m_s}^s-x^*}^2]\\
\stackrel{a}{\leq} & \frac{1}{\theta_{s-1}^2}[F(\widetilde{x}^{s-1})-F(x^*)] \\
&+ {\color{red}\frac{1}{2\eta m_s(1-\alpha_s)}}\E[\norm{z_{0}^s-x^*}^2-\norm{z_{m_s}^s-x^*}^2]. 
\end{split}
\end{equation*}
where in inequality $\stackrel{a}{\leq}$ we used $\frac{\lambda_s}{\beta_s^2}\leq \frac{1}{\theta_{s-1}^2}$. By using $z_0^{s+1} = z_{m_s}^s$, and adding the above inequality from $s=1$ to $S$, we have
\begin{equation}
\begin{split}
\frac{\E[F(\widetilde{x}^S)-F(x^*)]}{\beta_s^2}\leq & \frac{1}{\theta_{0}^2}[F(\widetilde{x}^{0})-F(x^*)] + {\color{red}\frac{1}{\eta m}}\E\left[\norm{z_{0}^1-x_*}^2\right], \\
\end{split}
\end{equation}
where we used $m\leq m_s$ and $\alpha_s\leq \frac{1}{2}$.
Then we have
\begin{equation}
\begin{split}
{\E[F(\widetilde{x}^S)-F(x^*)]}\leq & \frac{\beta_s^2}{\theta_{0}^2}[F(\widetilde{x}^{0})-F(x^*)]+ {\color{red}\frac{ \beta_s^2}{\eta m}}\E[\norm{z_{0}^1-x_*}^2\\
\leq & \frac{1}{\theta_{0}^2(S+2)^2}[F(\widetilde{x}^{0})-F(x^*)] + {\color{red}\frac{1 }{\eta m(S+2)^2}}\E[\norm{z_{0}^1-x_*}^2]
\end{split}
\end{equation}

Now suppose $F(x)$ is $\mu$-strongly convex. Since $x^*$ is the optimal solution, by inequality \eqref{Convex:Eq1} we have 
\begin{equation}\label{Eq1-Theo1}
\nabla F(x^*) = 0,\qquad F(x)-F(x^*) \geq \frac{\mu}{2} \norm{x-x^*}^2.
\end{equation}
Using the inequality in \eqref{Eq1-Theo1} and $z_0^s = \widetilde{x}^{s-1}$, we have 

\begin{equation}
\begin{split}
&\qquad\E [F(\widetilde{x}^s)-F(x^*)]\\
&\stackrel{a}{\leq}\lambda_s\E[F(\widetilde{x}^{s-1})-F(x^*)]+ {\color{red}\frac{ \beta_s^2}{2 m_s\eta(1-\alpha_s)}}\E[\norm{z_{0}^s-x_*}^2-\norm{z_{m}^s-x_*}^2]\\
&\stackrel{b}{\leq}\lambda_s\E[F(\widetilde{x}^{s-1})-F(x^*)]+ {\color{red}\frac{2  \beta_s^2}{\mu\eta m_s}}\E[F(\widetilde{x}^{s-1})-F(x^*)]\\
&=(\lambda_s+{\color{red}\frac{2  \beta_s^2}{\mu\eta m_s}})\E[F(\widetilde{x}^{s-1})-F(x^*)]
\end{split}
\end{equation}\\
where the inequality $\stackrel{a}{\leq}$ follows from Lemma \ref{lemma1}, and the inequality $\stackrel{b}{\leq}$ is due to the inequality \eqref{Eq1-Theo1} and the inequality $\alpha_s\leq \frac{1}{2}$. 
\end{proof}
As a corollary, we immediately obtain an expected linear rate of convergence for the synchronous algorithm Sync-AcPSVRG for the strongly convex case.
\begin{corollary}
Suppose function $F$ is strongly convex and $\beta_s$, $m_s$ and step size $\eta$ are chosen
such that the following condition is satisfied
\[
\lambda_s' := 1-\beta_s+{\color{red}\frac{2\beta_s^2}{\eta\mu m_s}} < 1,
\]
Then, for iterates of algorithm Sync-AcPSVRG, we have
\begin{equation}
\E\left[F(\widetilde{x}^S)-F(x^*)\right]\leq \lambda_{max}^S\left[F(\widetilde{x}^0)-F(x^*)\right], 
\end{equation}
with $\lambda_{max} = \max_{s}{\lambda_s'}$. The optimal value of $\lambda$ is $\lambda_{max} = 1-\frac{\mu m_s\eta}{8}$ which is obtained by choosing ${\color{red}\beta_s=\frac{\mu m_s\eta}{4}}$.
\end{corollary}
\begin{proof}
For the synchronous algorithm, $\tau=0$, and consequently $\alpha_s=0$. Then, we have $\lambda_s = 1-\beta_s$ and 
\[
\lambda_s' = 1-\beta_s+{\color{red}\frac{2  \beta_s^2}{\mu\eta m_s}}.\]
The minimal value of $\lambda_{max}$ is obtained by minimizing $\lambda_s'$ with respect to $\beta_s$.
\end{proof}








\section{Experiments}
%\input{experiment.tex}

\input{experimentE.tex}

\begin{thebibliography}{99}

\bibitem{Agarwal2014} Agarwal, Alekh, and John C. Duchi. "Distributed delayed stochastic optimization." In Advances in Neural Information Processing Systems, pp. 873-881. 2011.

\bibitem{Allen-Zhu2016}  Allen-Zhu, Zeyuan, and Elad Hazan. "Variance reduction for faster non-convex optimization." In International Conference on Machine Learning, pp. 699-707. 2016.

\bibitem{Allen-Zhu2016I} Allen-Zhu, Zeyuan, and Yang Yuan. "Improved SVRG for non-strongly-convex or sum-of-non-convex objectives." In International conference on machine learning, pp. 1080-1089. 2016.

\bibitem{Allen-Zhu17}Z. Allen-Zhu,  "Katyusha: The first truly accelerated stochastic gradient method", in {\it Annual Symposium on the Theory of Computing}, 2017, pp. 1200-1205.

\bibitem{Defazio2014} Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives." In Advances in neural information processing systems, pp. 1646-1654. 2014.

\bibitem{Fercoq2015} Fercoq, Olivier, and Peter RichtÃ¡rik. "Accelerated, parallel, and proximal coordinate descent." SIAM Journal on Optimization 25, no. 4 (2015): 1997-2023.

\bibitem{Hazan2011}E. Hazan and S. Kale.  Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. {\it Journal of Machine Learning Research - Proceedings Track}, 19:421-436, 2011.

\bibitem{Hu2009} Hu, Chonghai, Weike Pan, and James T. Kwok. "Accelerated gradient methods for stochastic optimization and online learning." In Advances in Neural Information Processing Systems, pp. 781-789. 2009.

\bibitem{Johnson12} R. Johnson and T. Zhang, "Accelerating stochastic gradient descent using predictive variance reduction",
in {\it Advances in Neural Information Processing Systems}, 2013, pp. 315-323.

\bibitem{Konecny2016} Kone{\v{c}}n{\`y}, J., Liu, J., Richt{\'a}rik, P. and Tak{\'a}{\v{c}}, M., 2016. Mini-batch semi-stochastic gradient descent in the proximal setting. IEEE Journal of Selected Topics in Signal Processing, 10(2), pp.242-255.

\bibitem{Langford2009} Langford, John, Lihong Li, and Tong Zhang. "Sparse online learning via truncated gradient." Journal of Machine Learning Research 10, no. Mar (2009): 777-801.

\bibitem{Li2014} Li, M., Andersen, D.G., Park, J.W., Smola, A.J., Ahmed, A., Josifovski, V., Long, J., Shekita, E.J. and Su, B.Y., 2014, October. Scaling Distributed Machine Learning with the Parameter Server. In OSDI (Vol. 14, pp. 583-598).

\bibitem{Lin2015} Lin, Hongzhou, Julien Mairal, and Zaid Harchaoui. "A universal catalyst for first-order optimization." In Advances in Neural Information Processing Systems, pp. 3384-3392. 2015.

\bibitem{LiP2016} Li, Yitan, Linli Xu, Xiaowei Zhong, and Qing Ling. "Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic Gradient Descent." arXiv preprint arXiv:1605.06619 (2016).

\bibitem{Li2016} Li, Xingguo, Tuo Zhao, Raman Arora, Han Liu, and Jarvis Haupt. "Stochastic variance reduced optimization for nonconvex sparse learning." In International Conference on Machine Learning, pp. 917-925. 2016.

\bibitem{Mania2017} Mania, Horia, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I. Jordan. "Perturbed iterate analysis for asynchronous stochastic optimization." SIAM Journal on Optimization 27, no. 4 (2017): 2202-2229.

\bibitem{Mania2017} Mania, Horia, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael I. Jordan. "Perturbed iterate analysis for asynchronous stochastic optimization." SIAM Journal on Optimization 27, no. 4 (2017): 2202-2229.

\bibitem{Meng2016}Meng, X., Bradley, J., Yavuz, B., Sparks, E., Venkataraman, S., Liu, D., Freeman, J., Tsai, D.B., Amde, M., Owen, S. and Xin, D., 2016. Mllib: Machine learning in apache spark. {\it The Journal of Machine Learning Research}, 17(1), pp.1235-1241.

\bibitem{Meng2016} Meng, Qi, Wei Chen, Jingcheng Yu, Taifeng Wang, Zhiming Ma, and Tie-Yan Liu. "Asynchronous Accelerated Stochastic Gradient Descent." In IJCAI, pp. 1853-1859. 2016.

\bibitem{Meng2017} Meng, Qi, Wei Chen, Jingcheng Yu, Taifeng Wang, Zhiming Ma, and Tie-Yan Liu. "Asynchronous Stochastic Proximal Optimization Algorithms with Variance Reduction." In AAAI, pp. 2329-2335. 2017. 

\bibitem{Nemirovski2009} Nemirovski, Arkadi, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. "Robust stochastic approximation approach to stochastic programming." SIAM Journal on optimization 19, no. 4 (2009): 1574-1609.

\bibitem{Nesterov2004} Nesterov, Yurii. Introductory lectures on convex optimization: A basic course. Vol. 87. Springer Science $\&$ Business Media, 2004.

\bibitem{Nitanda2014}Nitanda, Atsushi. "Stochastic proximal gradient descent with acceleration techniques." In Advances in Neural Information Processing Systems, pp. 1574-1582. 2014.

\bibitem{Rakhlin2012} Rakhlin, Alexander, Ohad Shamir, and Karthik Sridharan. "Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization." In ICML. 2012.

\bibitem{Recht2011} Recht, Benjamin, Christopher Re, Stephen Wright, and Feng Niu. "Hogwild: A lock-free approach to parallelizing stochastic gradient descent." In Advances in neural information processing systems, pp. 693-701. 2011.

\bibitem{Reddi2015} Reddi, Sashank J., Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alexander J. Smola. "On variance reduction in stochastic gradient descent and its asynchronous variants." In Advances in Neural Information Processing Systems, pp. 2647-2655. 2015. 

\bibitem{Shalev-Shwartz2014} Shalev-Shwartz, Shai, and Tong Zhang. "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization." In International Conference on Machine Learning, pp. 64-72. 2014.

\bibitem{Shen2016} Shen, Zebang, Hui Qian, Tengfei Zhou, and Tongzhou Mu. "Adaptive Variance Reducing for Stochastic Gradient Descent." In IJCAI, pp. 1990-1996. 2016.

\bibitem{Xiao2014} Xiao, Lin, and Tong Zhang. "A proximal stochastic gradient method with progressive variance reduction." SIAM Journal on Optimization 24, no. 4 (2014): 2057-2075.

\bibitem{Zhao2014} Zhao, T., Yu, M., Wang, Y., Arora, R. and Liu, H., 2014. Accelerated mini-batch randomized block coordinate descent method. In Advances in neural information processing systems (pp. 3329-3337).

\bibitem{Zhao2015} Zhao, Peilin, and Tong Zhang. "Stochastic optimization with importance sampling for regularized loss minimization." In international conference on machine learning, pp. 1-9. 2015.


\end{thebibliography}
\end{document}
